{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "797be409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Executable: r:\\Files Ruben\\GitRepos\\DeepDiveV2AI\\.venv\\Scripts\\python.exe\n",
      "Working Directory: r:\\Files Ruben\\GitRepos\\DeepDiveV2AI\n",
      "CUDA Available: True\n",
      "Tools Directory set to: C:\\Users\\ruben\\Documents\\TrainingAI\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# This should print the path to your C: drive venv\n",
    "print(f\"Python Executable: {sys.executable}\")\n",
    "print(f\"Working Directory: {os.getcwd()}\") \n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "\n",
    "# Path configuration\n",
    "TOOLS_DIR = r\"C:\\Users\\ruben\\Documents\\TrainingAI\"\n",
    "print(f\"Tools Directory set to: {TOOLS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb29bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Output Folder Created: R:\\Files Ruben\\GitRepos\\DeepDiveV2AI\\TrainedAndMerged\\Version9\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "import os\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainerCallback\n",
    "from huggingface_hub import login\n",
    "\n",
    "# --- PATH CONFIGURATION ---\n",
    "# 1. Where are the tools? (llama.cpp and quantize.exe)\n",
    "TOOLS_DIR = r\"C:\\Users\\ruben\\Documents\\TrainingAI\"\n",
    "\n",
    "# 2. Where should the final result go?\n",
    "BASE_OUTPUT_DIR = r\"R:\\Files Ruben\\GitRepos\\DeepDiveV2AI\\TrainedAndMerged\"\n",
    "\n",
    "# --- VERSIONING LOGIC ---\n",
    "if not os.path.exists(BASE_OUTPUT_DIR):\n",
    "    os.makedirs(BASE_OUTPUT_DIR)\n",
    "\n",
    "version_num = 1\n",
    "while os.path.exists(os.path.join(BASE_OUTPUT_DIR, f\"Version{version_num}\")):\n",
    "    version_num += 1\n",
    "\n",
    "OUTPUT_VERSION_DIR = os.path.join(BASE_OUTPUT_DIR, f\"Version{version_num}\")\n",
    "os.makedirs(OUTPUT_VERSION_DIR)\n",
    "print(f\"üìÇ Output Folder Created: {OUTPUT_VERSION_DIR}\")\n",
    "\n",
    "# --- SETTINGS ---\n",
    "DATA_FILE = \"lore_training_data_v2.json\" # <--- Using the NEW v2 file\n",
    "MODEL_NAME = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "NEW_MODEL_NAME = \"Llama-3-8B-Gloom-Lore\"\n",
    "HF_TOKEN = \"\" \n",
    "TARGET_LOSS = 0.5 # Slightly lower for chat format\n",
    "\n",
    "# --- STOPPING LOGIC ---\n",
    "class StopAtLossCallback(TrainerCallback):\n",
    "    def __init__(self, threshold):\n",
    "        self.threshold = threshold\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs and \"loss\" in logs:\n",
    "            if logs[\"loss\"] <= self.threshold:\n",
    "                print(f\"\\nüõë STOPPING EARLY! Loss ({logs['loss']}) hit target.\")\n",
    "                control.should_training_stop = True\n",
    "\n",
    "login(token=HF_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3c59095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 90 conversations from r:\\Files Ruben\\GitRepos\\DeepDiveV2AI\\lore_training_data_v2.json\n",
      "Loading Tokenizer for formatting...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4f6f28b44ef480ab83096d218dbf919",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/90 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- üîç DATA VISUALIZATION ---\n",
      "Raw Input (JSON):\n",
      "[\n",
      "  {\n",
      "    \"role\": \"system\",\n",
      "    \"content\": \"You are a survivor on the Ark submarine. You are gritty and superstitious.\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": \"What is the Gloom?\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": \"[Mood: Dread] *Stares out the reinforced porthole, voice dropping to a whisper.* The Gloom... it's not just energy. It's the curse of these depths. It gives us the crystals, sure, but it births the horrors that hunt us. Don't stare into it too long, diver. It stares back.\"\n",
      "  }\n",
      "]\n",
      "\n",
      "Formatted Output (What the model actually sees):\n",
      "----------------------------------------\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a survivor on the Ark submarine. You are gritty and superstitious.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What is the Gloom?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "[Mood: Dread] *Stares out the reinforced porthole, voice dropping to a whisper.* The Gloom... it's not just energy. It's the curse of these depths. It gives us the crystals, sure, but it births the horrors that hunt us. Don't stare into it too long, diver. It stares back.<|eot_id|>\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 1. Load Data\n",
    "try:\n",
    "    with open(DATA_FILE, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    print(f\"Loaded {len(data)} conversations from {os.path.abspath(DATA_FILE)}\")\n",
    "except FileNotFoundError:\n",
    "    raise FileNotFoundError(f\"Could not find {DATA_FILE} in {os.getcwd()}\")\n",
    "\n",
    "# 2. Convert to Hugging Face Dataset\n",
    "dataset = Dataset.from_list(data)\n",
    "\n",
    "# 3. Load Tokenizer (Needed for the Chat Template)\n",
    "print(\"Loading Tokenizer for formatting...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, token=HF_TOKEN)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# 4. Apply Llama 3 Chat Template\n",
    "def format_chat_template(example):\n",
    "    # This automatically adds <|start_header_id|>system... etc\n",
    "    return {\n",
    "        \"text\": tokenizer.apply_chat_template(\n",
    "            example[\"messages\"], \n",
    "            tokenize=False, \n",
    "            add_generation_prompt=False\n",
    "        )\n",
    "    }\n",
    "\n",
    "dataset = dataset.map(format_chat_template, batched=False)\n",
    "\n",
    "# 5. VISUALIZATION (Check your data!)\n",
    "print(\"\\n--- üîç DATA VISUALIZATION ---\")\n",
    "print(\"Raw Input (JSON):\")\n",
    "print(json.dumps(data[0]['messages'], indent=2))\n",
    "print(\"\\nFormatted Output (What the model actually sees):\")\n",
    "print(\"-\" * 40)\n",
    "print(dataset[0]['text'])\n",
    "print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fbc1a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Base Model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f3a993c7f884c7d8eec23ad8e24d028",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")\n",
    "\n",
    "print(\"Loading Base Model...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME, quantization_config=bnb_config, device_map=\"auto\", token=HF_TOKEN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4717457b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48eefd7e8591434eaef657d4b6194c06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/90 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r:\\Files Ruben\\GitRepos\\DeepDiveV2AI\\.venv\\Lib\\site-packages\\trl\\trainer\\sft_trainer.py:323: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 128009}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='400' max='1350' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 400/1350 10:49 < 25:49, 0.61 it/s, Epoch 4/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>3.337400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.770500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>1.656100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.500400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>1.383800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.422200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>1.385200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.123800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>1.097400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.089500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>0.920000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.697100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>0.672400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.701900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>0.510700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.448600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üõë STOPPING EARLY! Loss (0.4486) hit target.\n",
      "Training finished.\n",
      "LoRA adapters saved to: R:\\Files Ruben\\GitRepos\\DeepDiveV2AI\\TrainedAndMerged\\Version9\n"
     ]
    }
   ],
   "source": [
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16, lora_dropout=0.1, r=64, bias=\"none\", task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=15,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=1,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    save_steps=0,\n",
    "    logging_steps=25,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.001,\n",
    "    fp16=True,\n",
    "    bf16=False,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=1024, # Increased for chat history\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing=False,\n",
    "    callbacks=[StopAtLossCallback(TARGET_LOSS)]\n",
    ")\n",
    "\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "print(\"Training finished.\")\n",
    "\n",
    "# Save adapters to the Version Folder\n",
    "trainer.model.save_pretrained(os.path.join(OUTPUT_VERSION_DIR, NEW_MODEL_NAME))\n",
    "tokenizer.save_pretrained(os.path.join(OUTPUT_VERSION_DIR, NEW_MODEL_NAME))\n",
    "print(f\"LoRA adapters saved to: {OUTPUT_VERSION_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4a15f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted 'model' variable.\n",
      "Deleted 'trainer' variable.\n",
      "‚úÖ VRAM & RAM cleared successfully.\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "# Try to delete variables if they exist\n",
    "try:\n",
    "    del model\n",
    "    print(\"Deleted 'model' variable.\")\n",
    "except NameError:\n",
    "    print(\"'model' variable was already gone.\")\n",
    "\n",
    "try:\n",
    "    del trainer\n",
    "    print(\"Deleted 'trainer' variable.\")\n",
    "except NameError:\n",
    "    print(\"'trainer' variable was already gone.\")\n",
    "\n",
    "# Force Garbage Collection regardless\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "print(\"‚úÖ VRAM & RAM cleared successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c183a5bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model with Disk Offloading (Prevents Crash)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c322b4baa990489c9116598387b6b21e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying LoRA adapter...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r:\\Files Ruben\\GitRepos\\DeepDiveV2AI\\.venv\\Lib\\site-packages\\accelerate\\utils\\modeling.py:1566: UserWarning: Current model requires 256 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging weights...\n",
      "Saving merged model to: R:\\Files Ruben\\GitRepos\\DeepDiveV2AI\\TrainedAndMerged\\Version9\\merged_model_temp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r:\\Files Ruben\\GitRepos\\DeepDiveV2AI\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py:3970: UserWarning: Attempting to save a model with offloaded modules. Ensure that unallocated cpu memory exceeds the `shard_size` (5GB default)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92c3ab68d9364fa09bc59e6ff2859974",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge & Save complete.\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "import gc\n",
    "import shutil\n",
    "\n",
    "# 1. Define Overflow Folder\n",
    "offload_dir = os.path.join(OUTPUT_VERSION_DIR, \"offload_temp\")\n",
    "\n",
    "print(\"Loading base model with Disk Offloading (Prevents Crash)...\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",          # Use GPU first, then RAM, then Disk\n",
    "    offload_folder=offload_dir, # <--- The Safety Net\n",
    "    low_cpu_mem_usage=True,\n",
    "    token=HF_TOKEN\n",
    ")\n",
    "\n",
    "print(\"Applying LoRA adapter...\")\n",
    "# Load the adapter we just saved in the version folder\n",
    "adapter_path = os.path.join(OUTPUT_VERSION_DIR, NEW_MODEL_NAME)\n",
    "model_to_merge = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "\n",
    "print(\"Merging weights...\")\n",
    "merged_model = model_to_merge.merge_and_unload()\n",
    "\n",
    "# 3. Save with Sharding \n",
    "# Saves in 2GB chunks to prevent a RAM spike while writing to disk\n",
    "output_merged_dir = os.path.join(OUTPUT_VERSION_DIR, \"merged_model_temp\")\n",
    "print(f\"Saving merged model to: {output_merged_dir}\")\n",
    "\n",
    "merged_model.save_pretrained(\n",
    "    output_merged_dir, \n",
    "    safe_serialization=True, \n",
    "    max_shard_size=\"2GB\" \n",
    ")\n",
    "tokenizer.save_pretrained(output_merged_dir)\n",
    "\n",
    "# 4. Clean up the offload folder immediately\n",
    "if os.path.exists(offload_dir):\n",
    "    shutil.rmtree(offload_dir)\n",
    "\n",
    "# 5. Clean up the model from RAM to free up space for the next step\n",
    "del base_model\n",
    "del model_to_merge\n",
    "del merged_model\n",
    "gc.collect()\n",
    "\n",
    "print(\"Merge & Save complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2241ed04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using script: C:\\Users\\ruben\\Documents\\TrainingAI\\llama.cpp\\convert_hf_to_gguf.py\n",
      "Converting: R:\\Files Ruben\\GitRepos\\DeepDiveV2AI\\TrainedAndMerged\\Version9\\merged_model_temp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hf-to-gguf:Loading model: merged_model_temp\n",
      "INFO:hf-to-gguf:Model architecture: LlamaForCausalLM\n",
      "INFO:hf-to-gguf:gguf: loading model weight map from 'model.safetensors.index.json'\n",
      "INFO:hf-to-gguf:gguf: indexing model part 'model-00004-of-00009.safetensors'\n",
      "INFO:hf-to-gguf:gguf: indexing model part 'model-00003-of-00009.safetensors'\n",
      "INFO:hf-to-gguf:gguf: indexing model part 'model-00001-of-00009.safetensors'\n",
      "INFO:hf-to-gguf:gguf: indexing model part 'model-00007-of-00009.safetensors'\n",
      "INFO:hf-to-gguf:gguf: indexing model part 'model-00008-of-00009.safetensors'\n",
      "INFO:hf-to-gguf:gguf: indexing model part 'model-00009-of-00009.safetensors'\n",
      "INFO:hf-to-gguf:gguf: indexing model part 'model-00006-of-00009.safetensors'\n",
      "INFO:hf-to-gguf:gguf: indexing model part 'model-00005-of-00009.safetensors'\n",
      "INFO:hf-to-gguf:gguf: indexing model part 'model-00002-of-00009.safetensors'\n",
      "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
      "INFO:hf-to-gguf:Exporting model...\n",
      "INFO:hf-to-gguf:blk.11.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.11.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.11.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.11.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.11.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.11.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.11.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.11.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.11.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.12.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.12.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.12.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.12.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.12.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.12.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.12.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.12.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.12.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.13.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.13.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.13.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.13.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.13.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.13.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.13.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.13.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.13.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.14.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.14.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.14.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.14.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.14.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.14.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.14.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.14.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.14.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.15.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.15.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.15.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.15.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.15.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.10.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.10.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.10.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.10.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.10.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.10.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.10.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.10.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.10.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.6.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.6.ffn_down.weight,       torch.float16 --> F16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.6.ffn_up.weight,         torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.6.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.7.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.7.ffn_down.weight,       torch.float16 --> F16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.7.ffn_gate.weight,       torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.7.ffn_up.weight,         torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.7.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.7.attn_k.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.7.attn_output.weight,    torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.7.attn_q.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.7.attn_v.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.8.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.8.ffn_down.weight,       torch.float16 --> F16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.8.ffn_gate.weight,       torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.8.ffn_up.weight,         torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.8.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.8.attn_k.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.8.attn_output.weight,    torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.8.attn_q.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.8.attn_v.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.9.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.9.ffn_down.weight,       torch.float16 --> F16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.9.ffn_gate.weight,       torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.9.ffn_up.weight,         torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.9.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.9.attn_k.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.9.attn_output.weight,    torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.9.attn_q.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.9.attn_v.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:token_embd.weight,           torch.float16 --> F16, shape = {4096, 128256}\n",
      "INFO:hf-to-gguf:blk.0.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.0.ffn_down.weight,       torch.float16 --> F16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.0.ffn_gate.weight,       torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.0.ffn_up.weight,         torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.0.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.0.attn_k.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.0.attn_output.weight,    torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.0.attn_q.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.0.attn_v.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.1.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.1.ffn_down.weight,       torch.float16 --> F16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.1.ffn_gate.weight,       torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.1.ffn_up.weight,         torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.1.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.1.attn_k.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.1.attn_output.weight,    torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.1.attn_q.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.1.attn_v.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.2.attn_k.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.2.attn_q.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.2.attn_v.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.24.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.24.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.24.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.24.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.25.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.25.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.25.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.25.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.25.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.25.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.25.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.25.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.25.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.26.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.26.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.26.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.26.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.26.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.26.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.26.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.26.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.26.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.27.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.27.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.27.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.27.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.27.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.27.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.27.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.27.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.27.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.28.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.28.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.28.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.28.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.28.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.28.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.28.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.28.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.28.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.29.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.29.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.29.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.29.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.29.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.29.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.29.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.29.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.29.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.30.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.30.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.30.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.30.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.30.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.30.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.30.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.30.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.30.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.31.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.31.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.31.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.31.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.31.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.31.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.31.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.31.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.31.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:output_norm.weight,          torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:output.weight,               torch.float16 --> F16, shape = {4096, 128256}\n",
      "INFO:hf-to-gguf:blk.20.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.20.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.20.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.20.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.20.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.20.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.20.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.20.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.20.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.21.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.21.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.21.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.21.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.21.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.21.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.21.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.21.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.21.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.22.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.22.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.22.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.22.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.22.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.22.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.22.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.22.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.22.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.23.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.23.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.23.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.23.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.23.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.23.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.23.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.23.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.23.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.24.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.24.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.24.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.24.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.24.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.15.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.15.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.15.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.15.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.16.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.16.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.16.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.16.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.16.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.16.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.16.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.16.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.16.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.17.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.17.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.17.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.17.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.17.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.17.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.17.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.17.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.17.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.18.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.18.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.18.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.18.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.18.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.18.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.18.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.18.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.18.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.19.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.19.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.19.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.19.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.19.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.19.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.19.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.19.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.19.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.2.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.2.ffn_down.weight,       torch.float16 --> F16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.2.ffn_gate.weight,       torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.2.ffn_up.weight,         torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.2.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.2.attn_output.weight,    torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.3.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.3.ffn_down.weight,       torch.float16 --> F16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.3.ffn_gate.weight,       torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.3.ffn_up.weight,         torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.3.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.3.attn_k.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.3.attn_output.weight,    torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.3.attn_q.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.3.attn_v.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.4.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.4.ffn_down.weight,       torch.float16 --> F16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.4.ffn_gate.weight,       torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.4.ffn_up.weight,         torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.4.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.4.attn_k.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.4.attn_output.weight,    torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.4.attn_q.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.4.attn_v.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.5.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.5.ffn_down.weight,       torch.float16 --> F16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.5.ffn_gate.weight,       torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.5.ffn_up.weight,         torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.5.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.5.attn_k.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.5.attn_output.weight,    torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.5.attn_q.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.5.attn_v.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.6.ffn_gate.weight,       torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.6.attn_k.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.6.attn_output.weight,    torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.6.attn_q.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.6.attn_v.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:Set meta model\n",
      "INFO:hf-to-gguf:Set model parameters\n",
      "INFO:hf-to-gguf:gguf: context length = 8192\n",
      "INFO:hf-to-gguf:gguf: embedding length = 4096\n",
      "INFO:hf-to-gguf:gguf: feed forward length = 14336\n",
      "INFO:hf-to-gguf:gguf: head count = 32\n",
      "INFO:hf-to-gguf:gguf: key-value head count = 8\n",
      "INFO:hf-to-gguf:gguf: rope theta = 500000.0\n",
      "INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-05\n",
      "INFO:hf-to-gguf:gguf: file type = 1\n",
      "INFO:hf-to-gguf:Set model quantization version\n",
      "INFO:hf-to-gguf:Set model tokenizer\n",
      "The tokenizer you are loading from 'R:\\Files Ruben\\GitRepos\\DeepDiveV2AI\\TrainedAndMerged\\Version9\\merged_model_temp' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n",
      "WARNING:gguf.vocab:Unknown separator token '<|begin_of_text|>' in TemplateProcessing<pair>\n",
      "INFO:gguf.vocab:Adding 280147 merge(s).\n",
      "INFO:gguf.vocab:Setting special token type bos to 128000\n",
      "INFO:gguf.vocab:Setting special token type eos to 128009\n",
      "INFO:gguf.vocab:Setting special token type pad to 128009\n",
      "INFO:gguf.vocab:Setting add_bos_token to True\n",
      "INFO:gguf.vocab:Setting add_sep_token to False\n",
      "INFO:gguf.vocab:Setting chat_template to {% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n",
      "\n",
      "'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "' }}{% endif %}\n",
      "INFO:gguf.gguf_writer:Writing the following files:\n",
      "INFO:gguf.gguf_writer:R:\\Files Ruben\\GitRepos\\DeepDiveV2AI\\TrainedAndMerged\\Version9\\Llama-3-8B-Gloom-Lore.fp16.gguf: n_tensors = 291, total_size = 16.1G\n",
      "\n",
      "Writing:   0%|          | 0.00/16.1G [00:00<?, ?byte/s]\n",
      "Writing:   1%|‚ñè         | 235M/16.1G [00:00<00:12, 1.29Gbyte/s]\n",
      "Writing:   2%|‚ñè         | 394M/16.1G [00:00<00:12, 1.25Gbyte/s]\n",
      "Writing:   3%|‚ñé         | 554M/16.1G [00:00<00:11, 1.31Gbyte/s]\n",
      "Writing:   5%|‚ñç         | 789M/16.1G [00:00<00:12, 1.27Gbyte/s]\n",
      "Writing:   6%|‚ñå         | 990M/16.1G [00:00<00:12, 1.19Gbyte/s]\n",
      "Writing:   8%|‚ñä         | 1.22G/16.1G [00:01<00:12, 1.17Gbyte/s]\n",
      "Writing:   9%|‚ñâ         | 1.43G/16.1G [00:01<00:12, 1.20Gbyte/s]\n",
      "Writing:  10%|‚ñà         | 1.66G/16.1G [00:01<00:11, 1.21Gbyte/s]\n",
      "Writing:  12%|‚ñà‚ñè        | 1.86G/16.1G [00:01<00:11, 1.24Gbyte/s]\n",
      "Writing:  13%|‚ñà‚ñé        | 2.06G/16.1G [00:01<00:10, 1.30Gbyte/s]\n",
      "Writing:  14%|‚ñà‚ñç        | 2.30G/16.1G [00:01<00:11, 1.23Gbyte/s]\n",
      "Writing:  16%|‚ñà‚ñå        | 2.50G/16.1G [00:02<00:10, 1.27Gbyte/s]\n",
      "Writing:  17%|‚ñà‚ñã        | 2.73G/16.1G [00:02<00:10, 1.26Gbyte/s]\n",
      "Writing:  18%|‚ñà‚ñä        | 2.97G/16.1G [00:02<00:10, 1.27Gbyte/s]\n",
      "Writing:  20%|‚ñà‚ñâ        | 3.17G/16.1G [00:02<00:09, 1.29Gbyte/s]\n",
      "Writing:  21%|‚ñà‚ñà        | 3.41G/16.1G [00:02<00:09, 1.30Gbyte/s]\n",
      "Writing:  22%|‚ñà‚ñà‚ñè       | 3.61G/16.1G [00:02<00:09, 1.33Gbyte/s]\n",
      "Writing:  24%|‚ñà‚ñà‚ñç       | 3.84G/16.1G [00:03<00:09, 1.31Gbyte/s]\n",
      "Writing:  31%|‚ñà‚ñà‚ñà       | 4.98G/16.1G [00:03<00:08, 1.26Gbyte/s]\n",
      "Writing:  32%|‚ñà‚ñà‚ñà‚ñè      | 5.21G/16.1G [00:04<00:09, 1.19Gbyte/s]\n",
      "Writing:  33%|‚ñà‚ñà‚ñà‚ñé      | 5.34G/16.1G [00:04<00:08, 1.20Gbyte/s]\n",
      "Writing:  34%|‚ñà‚ñà‚ñà‚ñç      | 5.53G/16.1G [00:04<00:08, 1.25Gbyte/s]\n",
      "Writing:  36%|‚ñà‚ñà‚ñà‚ñå      | 5.77G/16.1G [00:04<00:08, 1.17Gbyte/s]\n",
      "Writing:  37%|‚ñà‚ñà‚ñà‚ñã      | 6.02G/16.1G [00:04<00:08, 1.24Gbyte/s]\n",
      "Writing:  39%|‚ñà‚ñà‚ñà‚ñâ      | 6.25G/16.1G [00:05<00:07, 1.26Gbyte/s]\n",
      "Writing:  40%|‚ñà‚ñà‚ñà‚ñà      | 6.49G/16.1G [00:05<00:07, 1.27Gbyte/s]\n",
      "Writing:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 6.69G/16.1G [00:05<00:07, 1.30Gbyte/s]\n",
      "Writing:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 6.92G/16.1G [00:05<00:07, 1.30Gbyte/s]\n",
      "Writing:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 7.12G/16.1G [00:05<00:06, 1.32Gbyte/s]\n",
      "Writing:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 7.36G/16.1G [00:05<00:06, 1.31Gbyte/s]\n",
      "Writing:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 7.56G/16.1G [00:06<00:06, 1.23Gbyte/s]\n",
      "Writing:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 7.80G/16.1G [00:06<00:06, 1.26Gbyte/s]\n",
      "Writing:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 8.00G/16.1G [00:06<00:06, 1.28Gbyte/s]\n",
      "Writing:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 8.23G/16.1G [00:06<00:06, 1.27Gbyte/s]\n",
      "Writing:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 8.43G/16.1G [00:06<00:05, 1.31Gbyte/s]\n",
      "Writing:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 8.67G/16.1G [00:06<00:05, 1.31Gbyte/s]\n",
      "Writing:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 8.87G/16.1G [00:07<00:05, 1.29Gbyte/s]\n",
      "Writing:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 9.10G/16.1G [00:07<00:05, 1.27Gbyte/s]\n",
      "Writing:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 10.2G/16.1G [00:08<00:04, 1.25Gbyte/s]\n",
      "Writing:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 10.5G/16.1G [00:08<00:04, 1.18Gbyte/s]\n",
      "Writing:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 10.6G/16.1G [00:08<00:04, 1.18Gbyte/s]\n",
      "Writing:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 10.8G/16.1G [00:08<00:04, 1.23Gbyte/s]\n",
      "Writing:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 11.0G/16.1G [00:08<00:04, 1.24Gbyte/s]\n",
      "Writing:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 11.2G/16.1G [00:08<00:03, 1.27Gbyte/s]\n",
      "Writing:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 11.5G/16.1G [00:09<00:03, 1.27Gbyte/s]\n",
      "Writing:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 11.7G/16.1G [00:09<00:03, 1.26Gbyte/s]\n",
      "Writing:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 11.9G/16.1G [00:09<00:03, 1.25Gbyte/s]\n",
      "Writing:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 12.1G/16.1G [00:09<00:03, 1.27Gbyte/s]\n",
      "Writing:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 12.3G/16.1G [00:09<00:03, 1.14Gbyte/s]\n",
      "Writing:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 12.4G/16.1G [00:10<00:04, 868Mbyte/s] \n",
      "Writing:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 12.5G/16.1G [00:10<00:04, 743Mbyte/s]\n",
      "Writing:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 12.7G/16.1G [00:10<00:04, 715Mbyte/s]\n",
      "Writing:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 12.8G/16.1G [00:10<00:04, 725Mbyte/s]\n",
      "Writing:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 13.0G/16.1G [00:11<00:04, 732Mbyte/s]\n",
      "Writing:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 13.1G/16.1G [00:11<00:04, 704Mbyte/s]\n",
      "Writing:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 13.2G/16.1G [00:11<00:04, 613Mbyte/s]\n",
      "Writing:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 13.3G/16.1G [00:11<00:04, 645Mbyte/s]\n",
      "Writing:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 13.4G/16.1G [00:11<00:03, 670Mbyte/s]\n",
      "Writing:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 13.5G/16.1G [00:11<00:03, 691Mbyte/s]\n",
      "Writing:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 13.6G/16.1G [00:12<00:03, 699Mbyte/s]\n",
      "Writing:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 13.8G/16.1G [00:12<00:03, 738Mbyte/s]\n",
      "Writing:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 14.0G/16.1G [00:12<00:02, 708Mbyte/s]\n",
      "Writing:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 14.1G/16.1G [00:12<00:02, 703Mbyte/s]\n",
      "Writing:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 14.2G/16.1G [00:12<00:03, 595Mbyte/s]\n",
      "Writing:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 14.3G/16.1G [00:13<00:02, 601Mbyte/s]\n",
      "Writing:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 14.4G/16.1G [00:13<00:02, 578Mbyte/s]\n",
      "Writing:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 14.5G/16.1G [00:13<00:02, 565Mbyte/s]\n",
      "Writing:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 14.7G/16.1G [00:13<00:02, 568Mbyte/s]\n",
      "Writing:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 14.8G/16.1G [00:13<00:02, 567Mbyte/s]\n",
      "Writing:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 14.9G/16.1G [00:14<00:02, 548Mbyte/s]\n",
      "Writing:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 15.1G/16.1G [00:14<00:01, 598Mbyte/s]\n",
      "Writing:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 15.2G/16.1G [00:14<00:01, 626Mbyte/s]\n",
      "Writing:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 15.3G/16.1G [00:14<00:01, 604Mbyte/s]\n",
      "Writing:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 15.5G/16.1G [00:15<00:00, 623Mbyte/s]\n",
      "Writing:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 15.7G/16.1G [00:15<00:00, 623Mbyte/s]\n",
      "Writing:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 15.8G/16.1G [00:15<00:00, 590Mbyte/s]\n",
      "Writing:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 15.9G/16.1G [00:15<00:00, 629Mbyte/s]\n",
      "Writing:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 16.0G/16.1G [00:15<00:00, 601Mbyte/s]\n",
      "Writing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16.1G/16.1G [00:16<00:00, 1.00Gbyte/s]\n",
      "INFO:hf-to-gguf:Model successfully exported to R:\\Files Ruben\\GitRepos\\DeepDiveV2AI\\TrainedAndMerged\\Version9\\Llama-3-8B-Gloom-Lore.fp16.gguf\n"
     ]
    }
   ],
   "source": [
    "# Paths\n",
    "llama_cpp_folder = os.path.join(TOOLS_DIR, \"llama.cpp\")\n",
    "convert_script = os.path.join(llama_cpp_folder, \"convert_hf_to_gguf.py\")\n",
    "\n",
    "# Points to the temp folder created in the previous cell\n",
    "model_path = os.path.join(OUTPUT_VERSION_DIR, \"merged_model_temp\")\n",
    "# Saves the heavy FP16 GGUF into the version folder\n",
    "outfile_path = os.path.join(OUTPUT_VERSION_DIR, f\"{NEW_MODEL_NAME}.fp16.gguf\")   \n",
    "\n",
    "if not os.path.exists(convert_script):\n",
    "    print(f\"Error: Could not find conversion script at: {convert_script}\")\n",
    "else:\n",
    "    print(f\"Using script: {convert_script}\")\n",
    "    print(f\"Converting: {model_path}\")\n",
    "    !python \"{convert_script}\" \"{model_path}\" --outtype f16 --outfile \"{outfile_path}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a62fbd8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Detected latest build folder: R:\\Files Ruben\\GitRepos\\DeepDiveV2AI\\TrainedAndMerged\\Version9\n",
      "Quantizing (Fresh RAM Mode)...\n",
      "   Input: R:\\Files Ruben\\GitRepos\\DeepDiveV2AI\\TrainedAndMerged\\Version9\\Llama-3-8B-Gloom-Lore.fp16.gguf\n",
      "\n",
      "‚úÖ Quantization Complete.\n",
      "üßπ Cleaning up massive temporary files...\n",
      "   - Deleted: Llama-3-8B-Gloom-Lore.fp16.gguf\n",
      "   - Deleted: merged_model_temp folder\n",
      "\n",
      "üéâ SUCCESS! Final model saved to:\n",
      "R:\\Files Ruben\\GitRepos\\DeepDiveV2AI\\TrainedAndMerged\\Version9\\Llama-3-8B-Gloom-Lore.Q4_K_M.gguf\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import shutil\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# --- CONFIGURATION (Must match Cell 2) ---\n",
    "TOOLS_DIR = r\"C:\\Users\\ruben\\Documents\\TrainingAI\"\n",
    "BASE_OUTPUT_DIR = r\"R:\\Files Ruben\\GitRepos\\DeepDiveV2AI\\TrainedAndMerged\"\n",
    "NEW_MODEL_NAME = \"Llama-3-8B-Gloom-Lore\"\n",
    "\n",
    "# --- 1. Auto-Detect the Latest Version Folder ---\n",
    "# Since we restarted the kernel, we need to find where the file is.\n",
    "version_folders = glob.glob(os.path.join(BASE_OUTPUT_DIR, \"Version*\"))\n",
    "if not version_folders:\n",
    "    raise FileNotFoundError(\"‚ùå No Version folders found! Did you run the training step?\")\n",
    "\n",
    "# Sort to find the highest number (latest run)\n",
    "latest_version_dir = max(version_folders, key=os.path.getctime)\n",
    "print(f\"üìÇ Detected latest build folder: {latest_version_dir}\")\n",
    "\n",
    "# Define Paths\n",
    "input_gguf = os.path.join(latest_version_dir, f\"{NEW_MODEL_NAME}.fp16.gguf\")\n",
    "output_gguf = os.path.join(latest_version_dir, f\"{NEW_MODEL_NAME}.Q4_K_M.gguf\")\n",
    "quantize_exe = os.path.join(TOOLS_DIR, \"llama-quantize.exe\")\n",
    "temp_merged_folder = os.path.join(latest_version_dir, \"merged_model_temp\")\n",
    "\n",
    "# --- 2. Execution Logic ---\n",
    "if not os.path.exists(quantize_exe):\n",
    "    print(f\"‚ùå Error: Tool not found at: {quantize_exe}\")\n",
    "elif not os.path.exists(input_gguf):\n",
    "    print(f\"‚ùå Error: Input file not found: {input_gguf}\")\n",
    "    print(\"   üëâ Did the conversion step finish successfully?\")\n",
    "else:\n",
    "    print(f\"Quantizing (Fresh RAM Mode)...\")\n",
    "    print(f\"   Input: {input_gguf}\")\n",
    "    \n",
    "    try:\n",
    "        # Run Command\n",
    "        result = subprocess.run(\n",
    "            [quantize_exe, input_gguf, output_gguf, \"Q4_K_M\"],\n",
    "            cwd=TOOLS_DIR,\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            check=True\n",
    "        )\n",
    "        \n",
    "        print(\"\\n‚úÖ Quantization Complete.\")\n",
    "        \n",
    "        # --- 3. Cleanup Logic ---\n",
    "        print(\"üßπ Cleaning up massive temporary files...\")\n",
    "        \n",
    "        if os.path.exists(input_gguf):\n",
    "            os.remove(input_gguf)\n",
    "            print(f\"   - Deleted: {os.path.basename(input_gguf)}\")\n",
    "            \n",
    "        if os.path.exists(temp_merged_folder):\n",
    "            shutil.rmtree(temp_merged_folder)\n",
    "            print(f\"   - Deleted: merged_model_temp folder\")\n",
    "            \n",
    "        print(f\"\\nüéâ SUCCESS! Final model saved to:\\n{output_gguf}\")\n",
    "\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(\"\\n‚ùå Quantization failed!\")\n",
    "        print(\"--- Error Details ---\")\n",
    "        print(e.stderr)\n",
    "        print(\"---------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
