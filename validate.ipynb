{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a284023",
   "metadata": {},
   "source": [
    "## Setup and Configuration\n",
    "We define the paths and ensure we are targeting the correct folders. If this cell hangs, it means your computer is struggling to talk to the R: drive or initialize the llama-cpp library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9d68eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Environment Ready.\n",
      "Target Directory: R:\\Files Ruben\\GitRepos\\DeepDiveV2AI\\TrainedAndMerged\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "from llama_cpp import Llama\n",
    "\n",
    "# --- PATHS ---\n",
    "BASE_DIR = r\"R:\\Files Ruben\\GitRepos\\DeepDiveV2AI\"\n",
    "MODELS_DIR = os.path.join(BASE_DIR, \"TrainedAndMerged\")\n",
    "TRAIN_DATA_PATH = os.path.join(BASE_DIR, \"lore_training_data_v2.json\")\n",
    "\n",
    "print(f\"‚úÖ Environment Ready.\\nTarget Directory: {MODELS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8518ae",
   "metadata": {},
   "source": [
    "## Smart Model Scanner\n",
    "This cell is updated to handle your specific situation: Version1 is labeled as \"Base\", and it finds the .gguf file even if the name changes between folders. It skips empty folders automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f858a5cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Found Models:\n",
      "  > BASE_MODEL: Meta-Llama-3-8B-Instruct-Q4_K_M.gguf\n",
      "  > Version3: Llama-3-8B-Gloom-Lore.Q4_K_M.gguf\n",
      "  > Version5: Llama-3-8B-Gloom-Lore.Q4_K_M.gguf\n",
      "  > Version9: Llama-3-8B-Gloom-Lore.Q4_K_M.gguf\n",
      "  > Version10: Llama-3-8B-Gloom-Lore.Q4_K_M.gguf\n"
     ]
    }
   ],
   "source": [
    "model_mapping = {}\n",
    "\n",
    "# We want to sort Version1, Version2, Version10 correctly\n",
    "def natural_sort_key(s):\n",
    "    return [int(text) if text.isdigit() else text.lower() for text in re.split('([0-9]+)', s)]\n",
    "\n",
    "folders = sorted(os.listdir(MODELS_DIR), key=natural_sort_key)\n",
    "\n",
    "for folder in folders:\n",
    "    folder_path = os.path.join(MODELS_DIR, folder)\n",
    "    \n",
    "    if os.path.isdir(folder_path):\n",
    "        # Look for any GGUF file inside the version folder\n",
    "        ggufs = [f for f in os.listdir(folder_path) if f.endswith(\".gguf\")]\n",
    "        \n",
    "        if ggufs:\n",
    "            full_path = os.path.join(folder_path, ggufs[0])\n",
    "            # Special naming for Version 1\n",
    "            label = \"BASE_MODEL\" if folder.lower() == \"version1\" else folder\n",
    "            model_mapping[label] = full_path\n",
    "\n",
    "print(\"üìÇ Found Models:\")\n",
    "for label, path in model_mapping.items():\n",
    "    print(f\"  > {label}: {os.path.basename(path)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1dc3f9e",
   "metadata": {},
   "source": [
    "## Extract Test Questions and Keywords\n",
    "This cell parses your JSON file to create the \"exam\" for the AI. It extracts the mood and specific lore keywords to check against the AI's answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "671c67ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Prepared 15 test cases.\n"
     ]
    }
   ],
   "source": [
    "# Load Lore Data\n",
    "with open(TRAIN_DATA_PATH, 'r', encoding='utf-8') as f:\n",
    "    lore_data = json.load(f)\n",
    "\n",
    "test_cases = []\n",
    "# We will test a variety of questions from the file\n",
    "for entry in lore_data[:15]: \n",
    "    messages = entry['messages']\n",
    "    user_q = next(m['content'] for m in messages if m['role'] == 'user')\n",
    "    expected_a = next(m['content'] for m in messages if m['role'] == 'assistant')\n",
    "    \n",
    "    # Identify key lore words (words > 5 letters)\n",
    "    keywords = set(re.findall(r'\\w{5,}', expected_a.lower()))\n",
    "    \n",
    "    test_cases.append({\n",
    "        \"question\": user_q,\n",
    "        \"keywords\": keywords,\n",
    "        \"expected\": expected_a\n",
    "    })\n",
    "\n",
    "print(f\"üìù Prepared {len(test_cases)} test cases.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a210c8ad",
   "metadata": {},
   "source": [
    "## Cell: Run Validation (GPU Accelerated)\n",
    "This cell loops through each model. Because we are using the GPU, the n_gpu_layers=-1 argument will ensure the 4070 Ti handles the heavy lifting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ecbc0c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Loading BASE_MODEL onto GPU... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (8192) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready.\n",
      "üöÄ Loading Version3 onto GPU... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (8192) -- the full capacity of the model will not be utilized\n",
      "r:\\Files Ruben\\GitRepos\\DeepDiveV2AI\\.venv\\Lib\\site-packages\\llama_cpp\\llama.py:1242: RuntimeWarning: Detected duplicate leading \"<|begin_of_text|>\" in prompt, this will likely reduce response quality, consider removing it...\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready.\n",
      "üöÄ Loading Version5 onto GPU... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (8192) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready.\n",
      "üöÄ Loading Version9 onto GPU... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (8192) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready.\n",
      "üöÄ Loading Version10 onto GPU... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (8192) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready.\n",
      "\n",
      "‚ú® GPU Validation Complete.\n"
     ]
    }
   ],
   "source": [
    "results_list = []\n",
    "\n",
    "# System prompt to set the AI's persona\n",
    "system_msg = \"You are a survivor on the Ark submarine. You are gritty and superstitious.\"\n",
    "\n",
    "for model_label, model_path in model_mapping.items():\n",
    "    print(f\"üöÄ Loading {model_label} onto GPU...\", end=\" \")\n",
    "    \n",
    "    try:\n",
    "        # n_gpu_layers=-1 offloads all layers to your 4070 Ti\n",
    "        # n_ctx=2048 gives the model enough \"memory\" for context\n",
    "        llm = Llama(\n",
    "            model_path=model_path, \n",
    "            n_ctx=2048, \n",
    "            n_gpu_layers=-1, \n",
    "            verbose=False\n",
    "        )\n",
    "        print(\"Ready.\")\n",
    "        \n",
    "        for test in test_cases:\n",
    "            # Constructing the Llama 3 specific chat template\n",
    "            prompt = f\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n{system_msg}<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n{test['question']}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "            \n",
    "            # Generate response\n",
    "            output = llm(prompt, max_tokens=150, stop=[\"<|eot_id|>\", \"<|start_header_id|>\"])\n",
    "            response = output['choices'][0]['text'].strip()\n",
    "            \n",
    "            # Metrics Calculation\n",
    "            hit_count = sum(1 for word in test['keywords'] if word in response.lower())\n",
    "            has_mood = 1 if \"[Mood:\" in response else 0\n",
    "            \n",
    "            results_list.append({\n",
    "                \"Model\": model_label,\n",
    "                \"Question\": test['question'],\n",
    "                \"Response\": response,\n",
    "                \"Lore_Hits\": hit_count,\n",
    "                \"Format_Correct\": bool(has_mood)\n",
    "            })\n",
    "            \n",
    "        # Clean up GPU VRAM before loading the next model\n",
    "        del llm\n",
    "        import gc\n",
    "        gc.collect()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to load {model_label}: {e}\")\n",
    "\n",
    "print(\"\\n‚ú® GPU Validation Complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fe95fd",
   "metadata": {},
   "source": [
    "## Result Visualization\n",
    "This final cell displays the \"Winner\" by averaging the scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b788c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÜ Model Comparison Summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lore_Hits</th>\n",
       "      <th>Format_Correct</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Version5</th>\n",
       "      <td>3.533333</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Version9</th>\n",
       "      <td>2.066667</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Version3</th>\n",
       "      <td>1.600000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Version10</th>\n",
       "      <td>1.466667</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BASE_MODEL</th>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Lore_Hits  Format_Correct\n",
       "Model                                \n",
       "Version5     3.533333             1.0\n",
       "Version9     2.066667             1.0\n",
       "Version3     1.600000             0.0\n",
       "Version10    1.466667             1.0\n",
       "BASE_MODEL   0.800000             0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Sample Comparison for: 'Who is the Broker?'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Response</th>\n",
       "      <th>Lore_Hits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BASE_MODEL</td>\n",
       "      <td>(sharply) Ah, the Broker? You mean that slippe...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Version3</td>\n",
       "      <td>The Broker is the middleman. They control the ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Version5</td>\n",
       "      <td>[Mood: Warning] *Eyes you seriously.* The Brok...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Version9</td>\n",
       "      <td>[Mood: Warning] *Eyes narrowing.* The Broker? ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>Version10</td>\n",
       "      <td>[Mood: Suspicious] *Spits on the deck.* He's t...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Model                                           Response  Lore_Hits\n",
       "1   BASE_MODEL  (sharply) Ah, the Broker? You mean that slippe...          2\n",
       "16    Version3  The Broker is the middleman. They control the ...          2\n",
       "31    Version5  [Mood: Warning] *Eyes you seriously.* The Brok...          3\n",
       "46    Version9  [Mood: Warning] *Eyes narrowing.* The Broker? ...          4\n",
       "61   Version10  [Mood: Suspicious] *Spits on the deck.* He's t...          4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.DataFrame(results_list)\n",
    "\n",
    "# Calculate average hits per model\n",
    "summary = df.groupby(\"Model\").agg({\n",
    "    \"Lore_Hits\": \"mean\",\n",
    "    \"Format_Correct\": \"mean\"\n",
    "}).sort_values(by=\"Lore_Hits\", ascending=False)\n",
    "\n",
    "print(\"üèÜ Model Comparison Summary:\")\n",
    "display(summary)\n",
    "\n",
    "# Display a specific comparison\n",
    "print(\"\\nüîç Sample Comparison for: 'Who is the Broker?'\")\n",
    "display(df[df['Question'] == \"Who is the Broker?\"][['Model', 'Response', 'Lore_Hits']])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
