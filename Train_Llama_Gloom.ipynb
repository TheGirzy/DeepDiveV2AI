{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7838639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Executable: c:\\Users\\ruben\\Documents\\TrainingAI\\venv\\Scripts\\python.exe\n",
      "Torch Version: 2.5.1+cu121\n",
      "CUDA Available: True\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "print(f\"Python Executable: {sys.executable}\")\n",
    "print(f\"Torch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac550458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer\n",
    "import os\n",
    "\n",
    "# Configuration\n",
    "MODEL_NAME = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "NEW_MODEL_NAME = \"Llama-3-8B-Gloom-Lore\"\n",
    "HF_TOKEN = \"...\" # <--- PASTE YOUR NEW TOKEN HERE\n",
    "DATA_FILE = \"lore_training_data.json\"  # <--- This was missing\n",
    "\n",
    "# Set up torch to use GPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Login to Hugging Face programmatically\n",
    "from huggingface_hub import login\n",
    "login(token=HF_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "110ed695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 7 examples from lore_training_data.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b42d66dc81040889385eb85d43cb7b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample Prompt:\n",
      "\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a gritty shopkeeper on a submarine in a sci-fi horror setting.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What is the Gloom?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "The Gloom is the lifeblood and the curse of these depths. It's an energy, a presence... something ancient. It gives us the crystals we seek, but it also births the horrors that hunt us. Don't stare into it too long; it stares back.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "# 1. Load Data from JSON file\n",
    "try:\n",
    "    with open(DATA_FILE, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    print(f\"Successfully loaded {len(data)} examples from {DATA_FILE}\")\n",
    "except FileNotFoundError:\n",
    "    raise FileNotFoundError(f\"Could not find {DATA_FILE}. Make sure it is in the same folder as this notebook.\")\n",
    "\n",
    "# 2. Convert to Hugging Face Dataset object\n",
    "dataset = Dataset.from_list(data)\n",
    "\n",
    "# 3. Define Llama 3 Prompt Format\n",
    "def format_llama3_prompts(examples):\n",
    "    texts = []\n",
    "    for instruction, response in zip(examples['instruction'], examples['response']):\n",
    "        # Exact Llama 3 Instruct format\n",
    "        prompt = (\n",
    "            f\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n\"\n",
    "            f\"You are a gritty shopkeeper on a submarine in a sci-fi horror setting.<|eot_id|>\"\n",
    "            f\"<|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "            f\"{instruction}<|eot_id|>\"\n",
    "            f\"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "            f\"{response}<|eot_id|>\"\n",
    "        )\n",
    "        texts.append(prompt)\n",
    "    return {'text': texts}\n",
    "\n",
    "# 4. Map the formatting to the dataset\n",
    "dataset = dataset.map(format_llama3_prompts, batched=True)\n",
    "\n",
    "# Show an example to verify\n",
    "print(\"\\nSample Prompt:\\n\")\n",
    "print(dataset[0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95993a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e5caf13435a4d10b3de3c86ba48439a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 4-bit Quantization Config (Crucial for 12GB VRAM)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")\n",
    "\n",
    "# Load Base Model\n",
    "print(\"Loading base model...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    token=HF_TOKEN\n",
    ")\n",
    "\n",
    "# Load Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, token=HF_TOKEN)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98c845f9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'peft_config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Initialize Trainer\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Since we downgraded to trl==0.8.6, this original syntax works perfectly again.\u001b[39;00m\n\u001b[32m      3\u001b[39m trainer = SFTTrainer(\n\u001b[32m      4\u001b[39m     model=model,\n\u001b[32m      5\u001b[39m     train_dataset=dataset,\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     peft_config=\u001b[43mpeft_config\u001b[49m,\n\u001b[32m      7\u001b[39m     dataset_text_field=\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;66;03m# explicit column name\u001b[39;00m\n\u001b[32m      8\u001b[39m     max_seq_length=\u001b[32m512\u001b[39m,        \u001b[38;5;66;03m# explicit sequence length\u001b[39;00m\n\u001b[32m      9\u001b[39m     tokenizer=tokenizer,       \u001b[38;5;66;03m# we can use 'tokenizer' again!\u001b[39;00m\n\u001b[32m     10\u001b[39m     args=training_arguments,\n\u001b[32m     11\u001b[39m     packing=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     12\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'peft_config' is not defined"
     ]
    }
   ],
   "source": [
    "# Initialize Trainer\n",
    "# Since we downgraded to trl==0.8.6, this original syntax works perfectly again.\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\", # explicit column name\n",
    "    max_seq_length=512,        # explicit sequence length\n",
    "    tokenizer=tokenizer,       # we can use 'tokenizer' again!\n",
    "    args=training_arguments,\n",
    "    packing=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24654b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "print(\"Training finished.\")\n",
    "\n",
    "# Save the adapter locally\n",
    "trainer.model.save_pretrained(NEW_MODEL_NAME)\n",
    "tokenizer.save_pretrained(NEW_MODEL_NAME)\n",
    "print(f\"LoRA adapters saved to folder: {NEW_MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec8eac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "del model\n",
    "del trainer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"VRAM cleared.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9788f2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "print(\"Loading model on CPU for merging (this prevents VRAM errors)...\")\n",
    "# Load base model in FP16 on CPU\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"cpu\",\n",
    "    token=HF_TOKEN\n",
    ")\n",
    "\n",
    "print(\"Applying LoRA adapter...\")\n",
    "model_to_merge = PeftModel.from_pretrained(base_model, NEW_MODEL_NAME)\n",
    "\n",
    "print(\"Merging weights...\")\n",
    "merged_model = model_to_merge.merge_and_unload()\n",
    "\n",
    "# Save the merged model\n",
    "output_merged_dir = \"merged_model\"\n",
    "merged_model.save_pretrained(output_merged_dir, safe_serialization=True)\n",
    "tokenizer.save_pretrained(output_merged_dir)\n",
    "print(f\"Full merged model saved to: {output_merged_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c660351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "llama_cpp_path = os.path.abspath(\"./llama.cpp\") \n",
    "convert_script = os.path.join(llama_cpp_path, \"convert_hf_to_gguf.py\")\n",
    "model_path = os.path.abspath(\"./merged_model\")\n",
    "outfile_path = f\"{NEW_MODEL_NAME}.fp16.gguf\"\n",
    "\n",
    "# Check if script exists\n",
    "if not os.path.exists(convert_script):\n",
    "    print(\"Error: convert_hf_to_gguf.py not found.\")\n",
    "    print(\"Please ensure you have the llama.cpp SOURCE code folder named 'llama.cpp' in this directory.\")\n",
    "else:\n",
    "    # Run conversion\n",
    "    command = f'python \"{convert_script}\" \"{model_path}\" --outtype f16 --outfile \"{outfile_path}\"'\n",
    "    print(f\"Running conversion command...\")\n",
    "    exit_code = os.system(command)\n",
    "    \n",
    "    if exit_code == 0:\n",
    "        print(f\"Success! Created {outfile_path}\")\n",
    "    else:\n",
    "        print(\"Conversion failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26395eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "input_gguf = f\"{NEW_MODEL_NAME}.fp16.gguf\"\n",
    "output_gguf = f\"{NEW_MODEL_NAME}.Q4_K_M.gguf\"\n",
    "quantize_exe = \"llama-quantize.exe\" \n",
    "\n",
    "if os.path.exists(quantize_exe):\n",
    "    print(f\"Quantizing {input_gguf} to {output_gguf}...\")\n",
    "    command = f'{quantize_exe} \"{input_gguf}\" \"{output_gguf}\" Q4_K_M'\n",
    "    os.system(command)\n",
    "    print(\"Done! You can now use the Q4_K_M.gguf file in LM Studio or Ollama.\")\n",
    "else:\n",
    "    print(\"Error: llama-quantize.exe not found.\")\n",
    "    print(\"Please download the 'bin-win-cuda-x64' zip from llama.cpp releases and extract the exe here.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
