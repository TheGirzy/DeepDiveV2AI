{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7838639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Executable: r:\\Files Ruben\\GitRepos\\DeepDiveV2AI\\.venv\\Scripts\\python.exe\n",
      "Working Directory: r:\\Files Ruben\\GitRepos\\DeepDiveV2AI\n",
      "CUDA Available: True\n",
      "Tools Directory set to: C:\\Users\\ruben\\Documents\\TrainingAI\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# This should print the path to your C: drive venv\n",
    "print(f\"Python Executable: {sys.executable}\")\n",
    "print(f\"Working Directory: {os.getcwd()}\") \n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "\n",
    "# Path configuration\n",
    "TOOLS_DIR = r\"C:\\Users\\ruben\\Documents\\TrainingAI\" # <--- POINTS BACK TO OLD FOLDER\n",
    "print(f\"Tools Directory set to: {TOOLS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac550458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‚ Output Folder Created: R:\\Files Ruben\\GitRepos\\DeepDiveV2AI\\TrainedAndMerged\\Version5\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "import os\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainerCallback\n",
    "from huggingface_hub import login\n",
    "\n",
    "# --- PATH CONFIGURATION ---\n",
    "# 1. Where are the tools? (llama.cpp and quantize.exe)\n",
    "TOOLS_DIR = r\"C:\\Users\\ruben\\Documents\\TrainingAI\"\n",
    "\n",
    "# 2. Where should the final result go?\n",
    "BASE_OUTPUT_DIR = r\"R:\\Files Ruben\\GitRepos\\DeepDiveV2AI\\TrainedAndMerged\"\n",
    "\n",
    "# --- VERSIONING LOGIC ---\n",
    "if not os.path.exists(BASE_OUTPUT_DIR):\n",
    "    os.makedirs(BASE_OUTPUT_DIR)\n",
    "\n",
    "version_num = 1\n",
    "while os.path.exists(os.path.join(BASE_OUTPUT_DIR, f\"Version{version_num}\")):\n",
    "    version_num += 1\n",
    "\n",
    "OUTPUT_VERSION_DIR = os.path.join(BASE_OUTPUT_DIR, f\"Version{version_num}\")\n",
    "os.makedirs(OUTPUT_VERSION_DIR)\n",
    "print(f\"ðŸ“‚ Output Folder Created: {OUTPUT_VERSION_DIR}\")\n",
    "\n",
    "# --- SETTINGS ---\n",
    "DATA_FILE = \"lore_training_data.json\"\n",
    "MODEL_NAME = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "NEW_MODEL_NAME = \"Llama-3-8B-Gloom-Lore\"\n",
    "HF_TOKEN = \"token\" \n",
    "TARGET_LOSS = 0.6 \n",
    "\n",
    "# --- STOPPING LOGIC ---\n",
    "class StopAtLossCallback(TrainerCallback):\n",
    "    def __init__(self, threshold):\n",
    "        self.threshold = threshold\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs and \"loss\" in logs:\n",
    "            if logs[\"loss\"] <= self.threshold:\n",
    "                print(f\"\\nðŸ›‘ STOPPING EARLY! Loss ({logs['loss']}) hit target.\")\n",
    "                control.should_training_stop = True\n",
    "\n",
    "login(token=HF_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "110ed695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 161 examples from r:\\Files Ruben\\GitRepos\\DeepDiveV2AI\\lore_training_data.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "920ec1af52c3441bb3dc08adce634857",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/161 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data formatted.\n"
     ]
    }
   ],
   "source": [
    "# 1. Load Data\n",
    "try:\n",
    "    with open(DATA_FILE, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    print(f\"Loaded {len(data)} examples from {os.path.abspath(DATA_FILE)}\")\n",
    "except FileNotFoundError:\n",
    "    raise FileNotFoundError(f\"Could not find {DATA_FILE} in {os.getcwd()}\")\n",
    "\n",
    "# 2. Format Data\n",
    "dataset = Dataset.from_list(data)\n",
    "def format_llama3_prompts(examples):\n",
    "    texts = []\n",
    "    for instruction, response in zip(examples['instruction'], examples['response']):\n",
    "        prompt = (\n",
    "            f\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n\"\n",
    "            f\"You are a gritty shopkeeper on a submarine in a sci-fi horror setting.<|eot_id|>\"\n",
    "            f\"<|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "            f\"{instruction}<|eot_id|>\"\n",
    "            f\"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "            f\"{response}<|eot_id|>\"\n",
    "        )\n",
    "        texts.append(prompt)\n",
    "    return {'text': texts}\n",
    "\n",
    "dataset = dataset.map(format_llama3_prompts, batched=True)\n",
    "print(\"Data formatted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95993a86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bf3ec31beb648638bce84d85099f89e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME, quantization_config=bnb_config, device_map=\"auto\", token=HF_TOKEN\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, token=HF_TOKEN)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41ec75bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d86fece13b51481b8ef7706282c476b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/161 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r:\\Files Ruben\\GitRepos\\DeepDiveV2AI\\.venv\\Lib\\site-packages\\trl\\trainer\\sft_trainer.py:323: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 128009}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='550' max='2415' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 550/2415 02:46 < 09:27, 3.29 it/s, Epoch 3/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.550800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.810700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.721000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.508100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.347300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.369000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.149400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.955200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.974700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.884000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.580400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ›‘ STOPPING EARLY! Loss (0.5804) hit target.\n",
      "Training finished.\n",
      "LoRA adapters saved to: R:\\Files Ruben\\GitRepos\\DeepDiveV2AI\\TrainedAndMerged\\Version5\n"
     ]
    }
   ],
   "source": [
    "TARGET_LOSS = 0.6\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16, lora_dropout=0.1, r=64, bias=\"none\", task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=15,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=1,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    save_steps=0,\n",
    "    logging_steps=50,   # <--- CHANGED FROM 5 TO 50\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.001,\n",
    "    fp16=True,\n",
    "    bf16=False,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=512,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing=False,\n",
    "    callbacks=[StopAtLossCallback(TARGET_LOSS)]\n",
    ")\n",
    "\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "print(\"Training finished.\")\n",
    "\n",
    "# Save adapters to the Version Folder\n",
    "trainer.model.save_pretrained(os.path.join(OUTPUT_VERSION_DIR, NEW_MODEL_NAME))\n",
    "tokenizer.save_pretrained(os.path.join(OUTPUT_VERSION_DIR, NEW_MODEL_NAME))\n",
    "print(f\"LoRA adapters saved to: {OUTPUT_VERSION_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24654b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model with Disk Offloading (Prevents Crash)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee9ce10b0b794aae9ec238190a03008b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying LoRA adapter...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r:\\Files Ruben\\GitRepos\\DeepDiveV2AI\\.venv\\Lib\\site-packages\\accelerate\\utils\\modeling.py:1566: UserWarning: Current model requires 256 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging weights...\n",
      "Saving merged model to: R:\\Files Ruben\\GitRepos\\DeepDiveV2AI\\TrainedAndMerged\\Version5\\merged_model_temp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "r:\\Files Ruben\\GitRepos\\DeepDiveV2AI\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py:3970: UserWarning: Attempting to save a model with offloaded modules. Ensure that unallocated cpu memory exceeds the `shard_size` (5GB default)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "831350496988487b871ae8ce4cbf2b1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge & Save complete.\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "import gc\n",
    "import shutil\n",
    "\n",
    "# 1. AGGRESSIVE CLEANUP\n",
    "# We delete the training model from RAM to make space for the merge\n",
    "try:\n",
    "    del model\n",
    "    del trainer\n",
    "except NameError:\n",
    "    pass \n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# 2. Define Overflow Folder\n",
    "# If RAM fills up, Python will write temporary data here instead of crashing\n",
    "offload_dir = os.path.join(OUTPUT_VERSION_DIR, \"offload_temp\")\n",
    "\n",
    "print(\"Loading base model with Disk Offloading (Prevents Crash)...\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",          # Use GPU first, then RAM, then Disk\n",
    "    offload_folder=offload_dir, # <--- The Safety Net\n",
    "    low_cpu_mem_usage=True,\n",
    "    token=HF_TOKEN\n",
    ")\n",
    "\n",
    "print(\"Applying LoRA adapter...\")\n",
    "# Load the adapter we just saved in the version folder\n",
    "adapter_path = os.path.join(OUTPUT_VERSION_DIR, NEW_MODEL_NAME)\n",
    "model_to_merge = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "\n",
    "print(\"Merging weights...\")\n",
    "merged_model = model_to_merge.merge_and_unload()\n",
    "\n",
    "# 3. Save with Sharding \n",
    "# Saves in 2GB chunks to prevent a RAM spike while writing to disk\n",
    "output_merged_dir = os.path.join(OUTPUT_VERSION_DIR, \"merged_model_temp\")\n",
    "print(f\"Saving merged model to: {output_merged_dir}\")\n",
    "\n",
    "merged_model.save_pretrained(\n",
    "    output_merged_dir, \n",
    "    safe_serialization=True, \n",
    "    max_shard_size=\"2GB\" \n",
    ")\n",
    "tokenizer.save_pretrained(output_merged_dir)\n",
    "\n",
    "# 4. Clean up the offload folder immediately\n",
    "if os.path.exists(offload_dir):\n",
    "    shutil.rmtree(offload_dir)\n",
    "\n",
    "# 5. Clean up the model from RAM to free up space for the next step\n",
    "del base_model\n",
    "del model_to_merge\n",
    "del merged_model\n",
    "gc.collect()\n",
    "\n",
    "print(\"Merge & Save complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ec8eac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using script: C:\\Users\\ruben\\Documents\\TrainingAI\\llama.cpp\\convert_hf_to_gguf.py\n",
      "Converting: R:\\Files Ruben\\GitRepos\\DeepDiveV2AI\\TrainedAndMerged\\Version5\\merged_model_temp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hf-to-gguf:Loading model: merged_model_temp\n",
      "INFO:hf-to-gguf:Model architecture: LlamaForCausalLM\n",
      "INFO:hf-to-gguf:gguf: loading model weight map from 'model.safetensors.index.json'\n",
      "INFO:hf-to-gguf:gguf: indexing model part 'model-00001-of-00009.safetensors'\n",
      "INFO:hf-to-gguf:gguf: indexing model part 'model-00003-of-00009.safetensors'\n",
      "INFO:hf-to-gguf:gguf: indexing model part 'model-00004-of-00009.safetensors'\n",
      "INFO:hf-to-gguf:gguf: indexing model part 'model-00005-of-00009.safetensors'\n",
      "INFO:hf-to-gguf:gguf: indexing model part 'model-00002-of-00009.safetensors'\n",
      "INFO:hf-to-gguf:gguf: indexing model part 'model-00007-of-00009.safetensors'\n",
      "INFO:hf-to-gguf:gguf: indexing model part 'model-00008-of-00009.safetensors'\n",
      "INFO:hf-to-gguf:gguf: indexing model part 'model-00006-of-00009.safetensors'\n",
      "INFO:hf-to-gguf:gguf: indexing model part 'model-00009-of-00009.safetensors'\n",
      "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
      "INFO:hf-to-gguf:Exporting model...\n",
      "INFO:hf-to-gguf:token_embd.weight,           torch.float16 --> F16, shape = {4096, 128256}\n",
      "INFO:hf-to-gguf:blk.0.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.0.ffn_down.weight,       torch.float16 --> F16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.0.ffn_gate.weight,       torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.0.ffn_up.weight,         torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.0.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.0.attn_k.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.0.attn_output.weight,    torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.0.attn_q.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.0.attn_v.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.1.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.1.ffn_down.weight,       torch.float16 --> F16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.1.ffn_gate.weight,       torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.1.ffn_up.weight,         torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.1.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.1.attn_k.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.1.attn_output.weight,    torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.1.attn_q.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.1.attn_v.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.2.attn_k.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.2.attn_q.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.2.attn_v.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.10.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.10.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.10.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.10.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.10.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.10.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.10.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.10.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.10.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.6.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.6.ffn_down.weight,       torch.float16 --> F16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.6.ffn_up.weight,         torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.6.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.7.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.7.ffn_down.weight,       torch.float16 --> F16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.7.ffn_gate.weight,       torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.7.ffn_up.weight,         torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.7.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.7.attn_k.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.7.attn_output.weight,    torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.7.attn_q.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.7.attn_v.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.8.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.8.ffn_down.weight,       torch.float16 --> F16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.8.ffn_gate.weight,       torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.8.ffn_up.weight,         torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.8.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.8.attn_k.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.8.attn_output.weight,    torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.8.attn_q.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.8.attn_v.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.9.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.9.ffn_down.weight,       torch.float16 --> F16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.9.ffn_gate.weight,       torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.9.ffn_up.weight,         torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.9.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.9.attn_k.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.9.attn_output.weight,    torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.9.attn_q.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.9.attn_v.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.11.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.11.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.11.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.11.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.11.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.11.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.11.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.11.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.11.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.12.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.12.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.12.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.12.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.12.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.12.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.12.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.12.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.12.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.13.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.13.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.13.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.13.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.13.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.13.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.13.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.13.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.13.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.14.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.14.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.14.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.14.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.14.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.14.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.14.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.14.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.14.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.15.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.15.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.15.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.15.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.15.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.15.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.15.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.15.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.15.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.16.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.16.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.16.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.16.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.16.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.16.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.16.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.16.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.16.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.17.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.17.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.17.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.17.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.17.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.17.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.17.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.17.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.17.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.18.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.18.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.18.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.18.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.18.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.18.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.18.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.18.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.18.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.19.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.19.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.19.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.19.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.19.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.19.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.19.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.19.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.19.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.2.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.2.ffn_down.weight,       torch.float16 --> F16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.2.ffn_gate.weight,       torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.2.ffn_up.weight,         torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.2.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.2.attn_output.weight,    torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.3.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.3.ffn_down.weight,       torch.float16 --> F16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.3.ffn_gate.weight,       torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.3.ffn_up.weight,         torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.3.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.3.attn_k.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.3.attn_output.weight,    torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.3.attn_q.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.3.attn_v.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.4.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.4.ffn_down.weight,       torch.float16 --> F16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.4.ffn_gate.weight,       torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.4.ffn_up.weight,         torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.4.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.4.attn_k.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.4.attn_output.weight,    torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.4.attn_q.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.4.attn_v.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.5.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.5.ffn_down.weight,       torch.float16 --> F16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.5.ffn_gate.weight,       torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.5.ffn_up.weight,         torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.5.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.5.attn_k.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.5.attn_output.weight,    torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.5.attn_q.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.5.attn_v.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.6.ffn_gate.weight,       torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.6.attn_k.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.6.attn_output.weight,    torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.6.attn_q.weight,         torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.6.attn_v.weight,         torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.24.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.24.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.24.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.24.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.25.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.25.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.25.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.25.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.25.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.25.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.25.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.25.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.25.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.26.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.26.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.26.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.26.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.26.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.26.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.26.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.26.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.26.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.27.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.27.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.27.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.27.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.27.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.27.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.27.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.27.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.27.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.28.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.28.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.28.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.28.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.28.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.28.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.28.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.28.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.28.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.29.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.29.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.29.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.29.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.29.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.29.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.29.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.29.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.29.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.30.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.30.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.30.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.30.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.30.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.30.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.30.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.30.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.30.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.31.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.31.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.31.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.31.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.31.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.31.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.31.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.31.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.31.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:output_norm.weight,          torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.20.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.20.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.20.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.20.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.20.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.20.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.20.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.20.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.20.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.21.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.21.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.21.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.21.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.21.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.21.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.21.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.21.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.21.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.22.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.22.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.22.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.22.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.22.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.22.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.22.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.22.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.22.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.23.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.23.ffn_down.weight,      torch.float16 --> F16, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.23.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.23.ffn_up.weight,        torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.23.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.23.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.23.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.23.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.23.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.24.ffn_gate.weight,      torch.float16 --> F16, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.24.attn_k.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.24.attn_output.weight,   torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.24.attn_q.weight,        torch.float16 --> F16, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.24.attn_v.weight,        torch.float16 --> F16, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:output.weight,               torch.float16 --> F16, shape = {4096, 128256}\n",
      "INFO:hf-to-gguf:Set meta model\n",
      "INFO:hf-to-gguf:Set model parameters\n",
      "INFO:hf-to-gguf:gguf: context length = 8192\n",
      "INFO:hf-to-gguf:gguf: embedding length = 4096\n",
      "INFO:hf-to-gguf:gguf: feed forward length = 14336\n",
      "INFO:hf-to-gguf:gguf: head count = 32\n",
      "INFO:hf-to-gguf:gguf: key-value head count = 8\n",
      "INFO:hf-to-gguf:gguf: rope theta = 500000.0\n",
      "INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-05\n",
      "INFO:hf-to-gguf:gguf: file type = 1\n",
      "INFO:hf-to-gguf:Set model quantization version\n",
      "INFO:hf-to-gguf:Set model tokenizer\n",
      "The tokenizer you are loading from 'R:\\Files Ruben\\GitRepos\\DeepDiveV2AI\\TrainedAndMerged\\Version5\\merged_model_temp' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n",
      "WARNING:gguf.vocab:Unknown separator token '<|begin_of_text|>' in TemplateProcessing<pair>\n",
      "INFO:gguf.vocab:Adding 280147 merge(s).\n",
      "INFO:gguf.vocab:Setting special token type bos to 128000\n",
      "INFO:gguf.vocab:Setting special token type eos to 128009\n",
      "INFO:gguf.vocab:Setting special token type pad to 128009\n",
      "INFO:gguf.vocab:Setting add_bos_token to True\n",
      "INFO:gguf.vocab:Setting add_sep_token to False\n",
      "INFO:gguf.vocab:Setting chat_template to {% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n",
      "\n",
      "'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "' }}{% endif %}\n",
      "INFO:gguf.gguf_writer:Writing the following files:\n",
      "INFO:gguf.gguf_writer:R:\\Files Ruben\\GitRepos\\DeepDiveV2AI\\TrainedAndMerged\\Version5\\Llama-3-8B-Gloom-Lore.fp16.gguf: n_tensors = 291, total_size = 16.1G\n",
      "\n",
      "Writing:   0%|          | 0.00/16.1G [00:00<?, ?byte/s]\n",
      "Writing:   7%|â–‹         | 1.05G/16.1G [00:00<00:11, 1.28Gbyte/s]\n",
      "Writing:   8%|â–Š         | 1.29G/16.1G [00:01<00:12, 1.15Gbyte/s]\n",
      "Writing:   9%|â–Š         | 1.40G/16.1G [00:01<00:12, 1.15Gbyte/s]\n",
      "Writing:  10%|â–‰         | 1.60G/16.1G [00:01<00:11, 1.22Gbyte/s]\n",
      "Writing:  11%|â–ˆâ–        | 1.84G/16.1G [00:01<00:11, 1.24Gbyte/s]\n",
      "Writing:  13%|â–ˆâ–Ž        | 2.09G/16.1G [00:01<00:10, 1.30Gbyte/s]\n",
      "Writing:  14%|â–ˆâ–        | 2.33G/16.1G [00:01<00:10, 1.29Gbyte/s]\n",
      "Writing:  16%|â–ˆâ–Œ        | 2.53G/16.1G [00:02<00:10, 1.28Gbyte/s]\n",
      "Writing:  17%|â–ˆâ–‹        | 2.76G/16.1G [00:02<00:10, 1.28Gbyte/s]\n",
      "Writing:  19%|â–ˆâ–Š        | 3.00G/16.1G [00:02<00:10, 1.29Gbyte/s]\n",
      "Writing:  20%|â–ˆâ–‰        | 3.20G/16.1G [00:02<00:09, 1.32Gbyte/s]\n",
      "Writing:  21%|â–ˆâ–ˆâ–       | 3.43G/16.1G [00:02<00:09, 1.32Gbyte/s]\n",
      "Writing:  23%|â–ˆâ–ˆâ–Ž       | 3.63G/16.1G [00:02<00:09, 1.33Gbyte/s]\n",
      "Writing:  24%|â–ˆâ–ˆâ–       | 3.87G/16.1G [00:03<00:09, 1.32Gbyte/s]\n",
      "Writing:  25%|â–ˆâ–ˆâ–Œ       | 4.07G/16.1G [00:03<00:09, 1.33Gbyte/s]\n",
      "Writing:  27%|â–ˆâ–ˆâ–‹       | 4.31G/16.1G [00:03<00:09, 1.28Gbyte/s]\n",
      "Writing:  28%|â–ˆâ–ˆâ–Š       | 4.51G/16.1G [00:03<00:08, 1.30Gbyte/s]\n",
      "Writing:  30%|â–ˆâ–ˆâ–‰       | 4.74G/16.1G [00:03<00:09, 1.24Gbyte/s]\n",
      "Writing:  31%|â–ˆâ–ˆâ–ˆ       | 4.94G/16.1G [00:03<00:08, 1.27Gbyte/s]\n",
      "Writing:  32%|â–ˆâ–ˆâ–ˆâ–      | 5.18G/16.1G [00:04<00:08, 1.26Gbyte/s]\n",
      "Writing:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 5.38G/16.1G [00:04<00:08, 1.28Gbyte/s]\n",
      "Writing:  35%|â–ˆâ–ˆâ–ˆâ–      | 5.61G/16.1G [00:04<00:08, 1.26Gbyte/s]\n",
      "Writing:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 5.82G/16.1G [00:04<00:08, 1.28Gbyte/s]\n",
      "Writing:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 6.02G/16.1G [00:04<00:10, 937Mbyte/s] \n",
      "Writing:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 6.13G/16.1G [00:05<00:10, 956Mbyte/s]\n",
      "Writing:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 6.37G/16.1G [00:05<00:09, 1.05Gbyte/s]\n",
      "Writing:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 6.49G/16.1G [00:05<00:09, 1.05Gbyte/s]\n",
      "Writing:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 6.69G/16.1G [00:05<00:08, 1.15Gbyte/s]\n",
      "Writing:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 6.92G/16.1G [00:05<00:07, 1.18Gbyte/s]\n",
      "Writing:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7.12G/16.1G [00:05<00:07, 1.21Gbyte/s]\n",
      "Writing:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 7.36G/16.1G [00:06<00:07, 1.11Gbyte/s]\n",
      "Writing:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 7.56G/16.1G [00:06<00:07, 1.15Gbyte/s]\n",
      "Writing:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 7.80G/16.1G [00:06<00:07, 1.16Gbyte/s]\n",
      "Writing:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 8.00G/16.1G [00:06<00:06, 1.20Gbyte/s]\n",
      "Writing:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 8.23G/16.1G [00:06<00:06, 1.18Gbyte/s]\n",
      "Writing:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 8.38G/16.1G [00:06<00:06, 1.21Gbyte/s]\n",
      "Writing:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 8.62G/16.1G [00:07<00:06, 1.24Gbyte/s]\n",
      "Writing:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 8.82G/16.1G [00:07<00:05, 1.27Gbyte/s]\n",
      "Writing:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9.05G/16.1G [00:07<00:06, 1.11Gbyte/s]\n",
      "Writing:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 9.26G/16.1G [00:07<00:06, 1.13Gbyte/s]\n",
      "Writing:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 9.49G/16.1G [00:07<00:05, 1.17Gbyte/s]\n",
      "Writing:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 9.69G/16.1G [00:08<00:05, 1.22Gbyte/s]\n",
      "Writing:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 9.89G/16.1G [00:08<00:04, 1.26Gbyte/s]\n",
      "Writing:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10.1G/16.1G [00:08<00:04, 1.26Gbyte/s]\n",
      "Writing:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 10.4G/16.1G [00:08<00:04, 1.26Gbyte/s]\n",
      "Writing:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 10.6G/16.1G [00:08<00:04, 1.29Gbyte/s]\n",
      "Writing:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 10.8G/16.1G [00:08<00:04, 1.14Gbyte/s]\n",
      "Writing:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 11.0G/16.1G [00:09<00:04, 1.19Gbyte/s]\n",
      "Writing:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11.2G/16.1G [00:09<00:03, 1.22Gbyte/s]\n",
      "Writing:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 11.4G/16.1G [00:09<00:03, 1.26Gbyte/s]\n",
      "Writing:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 11.7G/16.1G [00:09<00:03, 1.23Gbyte/s]\n",
      "Writing:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 11.9G/16.1G [00:09<00:03, 1.26Gbyte/s]\n",
      "Writing:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12.1G/16.1G [00:09<00:03, 1.28Gbyte/s]\n",
      "Writing:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 12.3G/16.1G [00:10<00:02, 1.30Gbyte/s]\n",
      "Writing:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 12.5G/16.1G [00:10<00:04, 766Mbyte/s] \n",
      "Writing:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 12.7G/16.1G [00:10<00:04, 718Mbyte/s]\n",
      "Writing:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 13.0G/16.1G [00:11<00:03, 831Mbyte/s]\n",
      "Writing:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13.2G/16.1G [00:11<00:03, 855Mbyte/s]\n",
      "Writing:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 13.3G/16.1G [00:11<00:03, 791Mbyte/s]\n",
      "Writing:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 13.4G/16.1G [00:11<00:03, 748Mbyte/s]\n",
      "Writing:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13.6G/16.1G [00:12<00:03, 780Mbyte/s]\n",
      "Writing:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 13.7G/16.1G [00:12<00:02, 777Mbyte/s]\n",
      "Writing:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 13.9G/16.1G [00:12<00:02, 754Mbyte/s]\n",
      "Writing:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14.1G/16.1G [00:12<00:02, 718Mbyte/s]\n",
      "Writing:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14.2G/16.1G [00:12<00:02, 677Mbyte/s]\n",
      "Writing:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 14.3G/16.1G [00:12<00:02, 707Mbyte/s]\n",
      "Writing:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 14.5G/16.1G [00:13<00:02, 722Mbyte/s]\n",
      "Writing:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 14.6G/16.1G [00:13<00:02, 675Mbyte/s]\n",
      "Writing:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 14.7G/16.1G [00:13<00:02, 654Mbyte/s]\n",
      "Writing:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 14.9G/16.1G [00:13<00:01, 643Mbyte/s]\n",
      "Writing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16.1G/16.1G [00:16<00:00, 566Mbyte/s]\n",
      "Writing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16.1G/16.1G [00:16<00:00, 993Mbyte/s]\n",
      "INFO:hf-to-gguf:Model successfully exported to R:\\Files Ruben\\GitRepos\\DeepDiveV2AI\\TrainedAndMerged\\Version5\\Llama-3-8B-Gloom-Lore.fp16.gguf\n"
     ]
    }
   ],
   "source": [
    "# Paths\n",
    "llama_cpp_folder = os.path.join(TOOLS_DIR, \"llama.cpp\")\n",
    "convert_script = os.path.join(llama_cpp_folder, \"convert_hf_to_gguf.py\")\n",
    "\n",
    "# Points to the temp folder created in the previous cell\n",
    "model_path = os.path.join(OUTPUT_VERSION_DIR, \"merged_model_temp\")\n",
    "# Saves the heavy FP16 GGUF into the version folder\n",
    "outfile_path = os.path.join(OUTPUT_VERSION_DIR, f\"{NEW_MODEL_NAME}.fp16.gguf\")   \n",
    "\n",
    "if not os.path.exists(convert_script):\n",
    "    print(f\"Error: Could not find conversion script at: {convert_script}\")\n",
    "    print(f\"Check if 'TOOLS_DIR' is correct in Cell 1.\")\n",
    "else:\n",
    "    print(f\"Using script: {convert_script}\")\n",
    "    print(f\"Converting: {model_path}\")\n",
    "    !python \"{convert_script}\" \"{model_path}\" --outtype f16 --outfile \"{outfile_path}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9788f2e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantizing...\n",
      "   Input: R:\\Files Ruben\\GitRepos\\DeepDiveV2AI\\TrainedAndMerged\\Version5\\Llama-3-8B-Gloom-Lore.fp16.gguf\n",
      "\n",
      "âœ… Quantization Complete.\n",
      "ðŸ§¹ Cleaning up massive temporary files...\n",
      "   - Deleted: Llama-3-8B-Gloom-Lore.fp16.gguf\n",
      "   - Deleted: merged_model_temp folder\n",
      "\n",
      "ðŸŽ‰ SUCCESS! Final model saved to:\n",
      "R:\\Files Ruben\\GitRepos\\DeepDiveV2AI\\TrainedAndMerged\\Version5\\Llama-3-8B-Gloom-Lore.Q4_K_M.gguf\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "# Define Paths\n",
    "input_gguf = os.path.join(OUTPUT_VERSION_DIR, f\"{NEW_MODEL_NAME}.fp16.gguf\")\n",
    "output_gguf = os.path.join(OUTPUT_VERSION_DIR, f\"{NEW_MODEL_NAME}.Q4_K_M.gguf\")\n",
    "quantize_exe = os.path.join(TOOLS_DIR, \"llama-quantize.exe\")\n",
    "temp_merged_folder = os.path.join(OUTPUT_VERSION_DIR, \"merged_model_temp\")\n",
    "\n",
    "# 1. Pre-Flight Checks\n",
    "if not os.path.exists(quantize_exe):\n",
    "    print(f\"âŒ Error: Tool not found at: {quantize_exe}\")\n",
    "elif not os.path.exists(input_gguf):\n",
    "    print(f\"âŒ Error: Input file not found: {input_gguf}\")\n",
    "    print(\"   ðŸ‘‰ The previous 'Convert' step likely failed or didn't run.\")\n",
    "else:\n",
    "    print(f\"Quantizing...\")\n",
    "    print(f\"   Input: {input_gguf}\")\n",
    "    \n",
    "    try:\n",
    "        # 2. Run Command (Using CWD=TOOLS_DIR to ensure it finds the CUDA DLLs)\n",
    "        result = subprocess.run(\n",
    "            [quantize_exe, input_gguf, output_gguf, \"Q4_K_M\"],\n",
    "            cwd=TOOLS_DIR,        # <--- FIXES THE DLL LOADING ISSUE\n",
    "            capture_output=True,  # Captures the logs\n",
    "            text=True,\n",
    "            check=True\n",
    "        )\n",
    "        \n",
    "        print(\"\\nâœ… Quantization Complete.\")\n",
    "        \n",
    "        # 3. Cleanup Logic\n",
    "        print(\"ðŸ§¹ Cleaning up massive temporary files...\")\n",
    "        \n",
    "        if os.path.exists(input_gguf):\n",
    "            os.remove(input_gguf)\n",
    "            print(f\"   - Deleted: {os.path.basename(input_gguf)}\")\n",
    "            \n",
    "        if os.path.exists(temp_merged_folder):\n",
    "            shutil.rmtree(temp_merged_folder)\n",
    "            print(f\"   - Deleted: merged_model_temp folder\")\n",
    "            \n",
    "        print(f\"\\nðŸŽ‰ SUCCESS! Final model saved to:\\n{output_gguf}\")\n",
    "\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(\"\\nâŒ Quantization failed!\")\n",
    "        print(\"--- Error Details ---\")\n",
    "        print(e.stderr) # This prints the actual error message from the tool\n",
    "        print(\"---------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
