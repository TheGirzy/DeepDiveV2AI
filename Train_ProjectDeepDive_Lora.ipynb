{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14cc4bf0",
   "metadata": {},
   "source": [
    "# Project Deep Dive: NPC Lore LoRA Training & Merging\n",
    "\n",
    "This notebook automates the process of fine-tuning, merging, and quantizing a Llama 3 8B model with custom lore for the Project Deep Dive game.\n",
    "\n",
    "### Workflow:\n",
    "1.  **Configuration:** Set your desired output name and training parameters in Cell 2.\n",
    "2.  **Login:** Run Cell 4 to log into Hugging Face (only needs to be done once).\n",
    "3.  **Training:** Run Cell 6 to train the LoRA adapter using your GPU.\n",
    "4.  **Merge & Quantize:** Run Cells 8, 9, and 10 to merge the LoRA into the base model and create a final GGUF file.\n",
    "5.  **Deployment:** Load your new, custom `...-merged.gguf` file directly into LM Studio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a9fefa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Configuration loaded successfully.\n",
      "   Model ID: meta-llama/Meta-Llama-3-8B-Instruct\n",
      "   Dataset: lore_training_data\n",
      "   Project Root: c:\\Users\\ruben\\Documents\\TrainingAI\\LLaMA-Factory\n",
      "   Output will be saved to: c:\\Users\\ruben\\Documents\\TrainingAI\\LLaMA-Factory\\saves\\Meta-Llama-3-8B-Instruct\\ProjectDeepDive-Lora-v1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import subprocess\n",
    "\n",
    "# --- 1. CORE CONFIGURATION ---\n",
    "MODEL_ID = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "LORA_OUTPUT_NAME = \"ProjectDeepDive-Lora-v1\"\n",
    "DATASET_NAME = \"lore_training_data\"\n",
    "\n",
    "# --- 2. TRAINING HYPERPARAMETERS ---\n",
    "EPOCHS = 5.0\n",
    "BATCH_SIZE = 1\n",
    "GRADIENT_ACCUMULATION = 4\n",
    "\n",
    "# --- 3. SCRIPT SETUP (No need to edit below this line) ---\n",
    "PROJECT_ROOT = os.path.abspath(\"..\")\n",
    "model_folder_name = MODEL_ID.split('/')[-1]\n",
    "output_dir_relative = os.path.join(\"saves\", model_folder_name, LORA_OUTPUT_NAME)\n",
    "training_script_path_relative = os.path.join(\"src\", \"train.py\")\n",
    "dataset_file_path_local = f\"{DATASET_NAME}.json\"\n",
    "\n",
    "if not os.path.exists(dataset_file_path_local):\n",
    "    raise FileNotFoundError(f\"CRITICAL: Dataset file not found at '{os.path.abspath(dataset_file_path_local)}'. Make sure '{dataset_file_path_local}' is in the same folder as this notebook.\")\n",
    "\n",
    "print(\"‚úÖ Configuration loaded successfully.\")\n",
    "print(f\"   Model ID: {MODEL_ID}\")\n",
    "print(f\"   Dataset: {DATASET_NAME}\")\n",
    "print(f\"   Project Root: {PROJECT_ROOT}\")\n",
    "print(f\"   Output will be saved to: {os.path.join(PROJECT_ROOT, output_dir_relative)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2539263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dataset 'lore_training_data.json' loaded successfully.\n",
      "   Found 7 question/answer pairs for training.\n",
      "   ‚ö†Ô∏è WARNING: Dataset is very small. Consider adding more examples for better results.\n"
     ]
    }
   ],
   "source": [
    "# Verify the dataset can be loaded and count the entries\n",
    "try:\n",
    "    with open(dataset_file_path_local, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    num_instructions = len(data) # Changed this to reflect the new structure\n",
    "    print(f\"‚úÖ Dataset '{dataset_file_path_local}' loaded successfully.\")\n",
    "    print(f\"   Found {num_instructions} question/answer pairs for training.\")\n",
    "    if num_instructions < 10:\n",
    "        print(\"   ‚ö†Ô∏è WARNING: Dataset is very small. Consider adding more examples for better results.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ERROR: Failed to read or parse the dataset file. Please check for syntax errors in your JSON.\")\n",
    "    print(f\"   Details: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258ce5b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-5 (_readerthread):\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ruben\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\threading.py\", line 1045, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\Users\\ruben\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\threading.py\", line 982, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"c:\\Users\\ruben\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\subprocess.py\", line 1599, in _readerthread\n",
      "    buffer.append(fh.read())\n",
      "                  ^^^^^^^^^\n",
      "  File \"c:\\Users\\ruben\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\encodings\\cp1252.py\", line 23, in decode\n",
      "    return codecs.charmap_decode(input,self.errors,decoding_table)[0]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "UnicodeDecodeError: 'charmap' codec can't decode byte 0x8f in position 10: character maps to <undefined>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully logged in to Hugging Face.\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "# Replace with your NEW token\n",
    "hf_token = \"\"\n",
    "login_command = [\"huggingface-cli\", \"login\", \"--token\", hf_token]\n",
    "result = subprocess.run(login_command, capture_output=True, text=True)\n",
    "if result.returncode == 0:\n",
    "    print(\"‚úÖ Successfully logged in to Hugging Face.\")\n",
    "else:\n",
    "    print(\"‚ùå Failed to log in to Hugging Face. Check your token and network connection.\")\n",
    "    print(result.stdout); print(result.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4da2e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training Command ---\n",
      "python src\\train.py --model_name_or_path meta-llama/Meta-Llama-3-8B-Instruct --do_train --dataset lore_training_data --finetuning_type lora --output_dir saves\\Meta-Llama-3-8B-Instruct\\ProjectDeepDive-Lora-v1 --lora_target all --per_device_train_batch_size 1 --gradient_accumulation_steps 4 --num_train_epochs 5.0 --plot_loss --fp16\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "command = [\n",
    "    \"python\", training_script_path_relative,\n",
    "    \"--model_name_or_path\", MODEL_ID,\n",
    "    \"--do_train\",\n",
    "    \"--dataset\", DATASET_NAME,\n",
    "    \"--finetuning_type\", \"lora\",\n",
    "    \"--output_dir\", output_dir_relative,\n",
    "    \"--lora_target\", \"all\",\n",
    "    \"--per_device_train_batch_size\", str(BATCH_SIZE),\n",
    "    \"--gradient_accumulation_steps\", str(GRADIENT_ACCUMULATION),\n",
    "    \"--num_train_epochs\", str(EPOCHS),\n",
    "    \"--plot_loss\",\n",
    "    \"--fp16\"\n",
    "]\n",
    "print(\"--- Training Command ---\")\n",
    "print(subprocess.list2cmdline(command))\n",
    "print(\"------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2885c931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting training... This may take a while.\n",
      "Traceback (most recent call last):\n",
      "File \"c:\\Users\\ruben\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\import_utils.py\", line 2317, in __getattr__\n",
      "module = self._get_module(self._class_to_module[name])\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "File \"c:\\Users\\ruben\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\import_utils.py\", line 2347, in _get_module\n",
      "raise e\n",
      "File \"c:\\Users\\ruben\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\import_utils.py\", line 2345, in _get_module\n",
      "return importlib.import_module(\".\" + module_name, self.__name__)\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "File \"c:\\Users\\ruben\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\importlib\\__init__.py\", line 126, in import_module\n",
      "return _bootstrap._gcd_import(name[level:], package, level)\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "File \"<frozen importlib._bootstrap>\", line 1204, in _gcd_import\n",
      "File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\n",
      "File \"<frozen importlib._bootstrap>\", line 1147, in _find_and_load_unlocked\n",
      "File \"<frozen importlib._bootstrap>\", line 690, in _load_unlocked\n",
      "File \"<frozen importlib._bootstrap_external>\", line 940, in exec_module\n",
      "File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
      "File \"c:\\Users\\ruben\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 70, in <module>\n",
      "from .loss.loss_utils import LOSS_MAPPING\n",
      "File \"c:\\Users\\ruben\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\loss\\loss_utils.py\", line 21, in <module>\n",
      "from .loss_d_fine import DFineForObjectDetectionLoss\n",
      "File \"c:\\Users\\ruben\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\loss\\loss_d_fine.py\", line 21, in <module>\n",
      "from .loss_for_object_detection import (\n",
      "File \"c:\\Users\\ruben\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\loss\\loss_for_object_detection.py\", line 32, in <module>\n",
      "from transformers.image_transforms import center_to_corners_format\n",
      "File \"c:\\Users\\ruben\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\image_transforms.py\", line 22, in <module>\n",
      "from .image_utils import (\n",
      "File \"c:\\Users\\ruben\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\image_utils.py\", line 55, in <module>\n",
      "from torchvision.transforms import InterpolationMode\n",
      "File \"c:\\Users\\ruben\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\__init__.py\", line 10, in <module>\n",
      "from torchvision import _meta_registrations, datasets, io, models, ops, transforms, utils  # usort:skip\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "File \"c:\\Users\\ruben\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\_meta_registrations.py\", line 163, in <module>\n",
      "@torch.library.register_fake(\"torchvision::nms\")\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "File \"c:\\Users\\ruben\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\library.py\", line 828, in register\n",
      "use_lib._register_fake(op_name, func, _stacklevel=stacklevel + 1)\n",
      "File \"c:\\Users\\ruben\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\library.py\", line 198, in _register_fake\n",
      "handle = entry.fake_impl.register(func_to_register, source)\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "File \"c:\\Users\\ruben\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_library\\fake_impl.py\", line 31, in register\n",
      "if torch._C._dispatch_has_kernel_for_dispatch_key(self.qualname, \"Meta\"):\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: operator torchvision::nms does not exist\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "File \"c:\\Users\\ruben\\Documents\\TrainingAI\\LLaMA-Factory\\src\\train.py\", line 15, in <module>\n",
      "from llamafactory.train.tuner import run_exp\n",
      "File \"c:\\Users\\ruben\\Documents\\TrainingAI\\LLaMA-Factory\\src\\llamafactory\\train\\tuner.py\", line 21, in <module>\n",
      "from transformers import EarlyStoppingCallback, PreTrainedModel\n",
      "File \"c:\\Users\\ruben\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\import_utils.py\", line 2320, in __getattr__\n",
      "raise ModuleNotFoundError(\n",
      "ModuleNotFoundError: Could not import module 'PreTrainedModel'. Are this object's requirements defined correctly?\n",
      "\n",
      "‚ùå Training failed with exit code 1.\n"
     ]
    }
   ],
   "source": [
    "print(\"üöÄ Starting training... This may take a while.\")\n",
    "process = subprocess.Popen(command, cwd=PROJECT_ROOT, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, encoding='utf-8', bufsize=1)\n",
    "while True:\n",
    "    output = process.stdout.readline()\n",
    "    if output == '' and process.poll() is not None: break\n",
    "    if output: print(output.strip())\n",
    "if process.returncode == 0:\n",
    "    print(\"\\nüéâ Training finished successfully! üéâ\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå Training failed with exit code {process.returncode}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08741845",
   "metadata": {},
   "source": [
    "### Step 2: Merge LoRA and Quantize to GGUF\n",
    "\n",
    "Now that the LoRA adapter is trained, we will perform two final steps:\n",
    "1.  **Merge:** Combine the base Llama 3 model with our LoRA adapter to create a new, full-sized (unquantized) model.\n",
    "2.  **Quantize:** Compress the large, merged model into a single, efficient GGUF file that LM Studio can use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22c3ddbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting model merge process...\n",
      "--- Merge Command ---\n",
      "python src\\export_model.py --model_name_or_path meta-llama/Meta-Llama-3-8B-Instruct --adapter_name_or_path saves\\Meta-Llama-3-8B-Instruct\\ProjectDeepDive-Lora-v1 --template llama3 --export_dir merged_models\\Meta-Llama-3-8B-Instruct-ProjectDeepDive-Lora-v1 --export_size 2\n",
      "---------------------\n",
      "python: can't open file 'c:\\\\Users\\\\ruben\\\\Documents\\\\TrainingAI\\\\LLaMA-Factory\\\\src\\\\export_model.py': [Errno 2] No such file or directory\n",
      "\n",
      "‚ùå Model merge failed with exit code 2.\n"
     ]
    }
   ],
   "source": [
    "# --- MERGE THE TRAINED LORA ---\n",
    "print(\"üöÄ Starting model merge process...\")\n",
    "\n",
    "# Define the directory where the full-precision merged model will be saved\n",
    "MERGED_MODEL_DIR_RELATIVE = os.path.join(\"merged_models\", f\"{model_folder_name}-{LORA_OUTPUT_NAME}\")\n",
    "MERGED_MODEL_DIR_ABSOLUTE = os.path.join(PROJECT_ROOT, MERGED_MODEL_DIR_RELATIVE)\n",
    "\n",
    "merge_command = [\n",
    "    \"python\", os.path.join(\"src\", \"export_model.py\"),\n",
    "    \"--model_name_or_path\", MODEL_ID,\n",
    "    \"--adapter_name_or_path\", output_dir_relative,\n",
    "    \"--template\", \"llama3\",\n",
    "    \"--export_dir\", MERGED_MODEL_DIR_RELATIVE,\n",
    "    \"--export_size\", \"2\" # Shard the model into 2GB chunks\n",
    "]\n",
    "\n",
    "print(\"--- Merge Command ---\")\n",
    "print(subprocess.list2cmdline(merge_command))\n",
    "print(\"---------------------\")\n",
    "\n",
    "process = subprocess.Popen(merge_command, cwd=PROJECT_ROOT, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, encoding='utf-8', bufsize=1)\n",
    "while True:\n",
    "    output = process.stdout.readline()\n",
    "    if output == '' and process.poll() is not None: break\n",
    "    if output: print(output.strip())\n",
    "\n",
    "if process.returncode == 0:\n",
    "    print(f\"\\nüéâ Model merged successfully! Full-precision model saved at:\\n{MERGED_MODEL_DIR_ABSOLUTE}\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå Model merge failed with exit code {process.returncode}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3eb4ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Starting quantization to GGUF format...\n",
      "--- Quantize Command ---\n",
      "python c:\\Users\\ruben\\Documents\\TrainingAI\\llama.cpp\\convert-hf-to-gguf.py c:\\Users\\ruben\\Documents\\TrainingAI\\LLaMA-Factory\\merged_models\\Meta-Llama-3-8B-Instruct-ProjectDeepDive-Lora-v1 --outfile c:\\Users\\ruben\\Documents\\TrainingAI\\LLaMA-Factory\\final_gguf_models\\Meta-Llama-3-8B-Instruct-ProjectDeepDive-Lora-v1-Q4_K_M.gguf --outtype q4_k_m\n",
      "------------------------\n",
      "python: can't open file 'c:\\\\Users\\\\ruben\\\\Documents\\\\TrainingAI\\\\llama.cpp\\\\convert-hf-to-gguf.py': [Errno 2] No such file or directory\n",
      "\n",
      "‚ùå Quantization failed with exit code 2.\n"
     ]
    }
   ],
   "source": [
    "# --- QUANTIZE THE MERGED MODEL TO GGUF ---\n",
    "print(\"\\nüöÄ Starting quantization to GGUF format...\")\n",
    "\n",
    "# Path to the llama.cpp repository (should be next to LLaMA-Factory)\n",
    "LLAMA_CPP_DIR = os.path.abspath(os.path.join(PROJECT_ROOT, \"..\", \"llama.cpp\"))\n",
    "\n",
    "if not os.path.isdir(LLAMA_CPP_DIR):\n",
    "    raise NotADirectoryError(f\"CRITICAL: llama.cpp directory not found at '{LLAMA_CPP_DIR}'. Please ensure it's cloned in the same folder as LLaMA-Factory.\")\n",
    "\n",
    "# Define the final output file for our game\n",
    "FINAL_GGUF_DIR = os.path.join(PROJECT_ROOT, \"final_gguf_models\")\n",
    "os.makedirs(FINAL_GGUF_DIR, exist_ok=True)\n",
    "FINAL_GGUF_FILE = os.path.join(FINAL_GGUF_DIR, f\"{model_folder_name}-{LORA_OUTPUT_NAME}-Q4_K_M.gguf\")\n",
    "\n",
    "quantize_command = [\n",
    "    \"python\", os.path.join(LLAMA_CPP_DIR, \"convert-hf-to-gguf.py\"),\n",
    "    MERGED_MODEL_DIR_ABSOLUTE,\n",
    "    \"--outfile\", FINAL_GGUF_FILE,\n",
    "    \"--outtype\", \"q4_k_m\" # A high-quality, medium-sized quantization type\n",
    "]\n",
    "\n",
    "print(\"--- Quantize Command ---\")\n",
    "print(subprocess.list2cmdline(quantize_command))\n",
    "print(\"------------------------\")\n",
    "\n",
    "process = subprocess.Popen(quantize_command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, encoding='utf-8', bufsize=1)\n",
    "while True:\n",
    "    output = process.stdout.readline()\n",
    "    if output == '' and process.poll() is not None: break\n",
    "    if output: print(output.strip())\n",
    "\n",
    "if process.returncode == 0:\n",
    "    print(f\"\\nüéâ Quantization successful! Your game-ready model is located at:\\n{FINAL_GGUF_FILE}\")\n",
    "    print(\"\\nüí° You can now delete the large merged model folder to save space:\")\n",
    "    print(f\"   {MERGED_MODEL_DIR_ABSOLUTE}\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå Quantization failed with exit code {process.returncode}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aedb3d9",
   "metadata": {},
   "source": [
    "### Workflow Complete!\n",
    "\n",
    "1.  **Locate Your Final Model:**\n",
    "    *   Navigate to your `LLaMA-Factory` folder.\n",
    "    *   You will find a new folder named `final_gguf_models`.\n",
    "    *   Inside is your custom, ready-to-use GGUF file (e.g., `Meta-Llama-3-8B-Instruct-ProjectDeepDive-Lora-v1-Q4_K_M.gguf`).\n",
    "\n",
    "2.  **Load in LM Studio:**\n",
    "    *   Open LM Studio.\n",
    "    *   You can either move this new GGUF file to your `.cache/lm-studio/models` folder, or simply drag-and-drop it from the `final_gguf_models` folder directly onto the LM Studio window.\n",
    "    *   It will now appear in your \"My Models\" list.\n",
    "\n",
    "3.  **Activate and Test:**\n",
    "    *   Select your new, custom model from the dropdown at the top of the Chat or Server tab.\n",
    "    *   Start the local server. **You do NOT need to load any LoRA adapters separately.**\n",
    "    *   Launch your Unity game and interact with an NPC to test the new, lore-aware responses!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
