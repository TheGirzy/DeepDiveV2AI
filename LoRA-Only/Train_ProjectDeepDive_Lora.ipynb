{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14cc4bf0",
   "metadata": {},
   "source": [
    "# Project Deep Dive: NPC Lore LoRA Training & Merging\n",
    "\n",
    "This notebook automates the process of fine-tuning, merging, and quantizing a Llama 3 8B model with custom lore for the Project Deep Dive game.\n",
    "\n",
    "### Workflow:\n",
    "1.  **Configuration:** Set your desired output name and training parameters in Cell 2.\n",
    "2.  **Login:** Run Cell 4 to log into Hugging Face (only needs to be done once).\n",
    "3.  **Training:** Run Cell 6 to train the LoRA adapter using your GPU.\n",
    "4.  **Merge & Quantize:** Run Cells 8, 9, and 10 to merge the LoRA into the base model and create a final GGUF file.\n",
    "5.  **Deployment:** Load your new, custom `...-merged.gguf` file directly into LM Studio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9fefa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Configuration loaded successfully.\n",
      "   Model ID: meta-llama/Meta-Llama-3-8B-Instruct\n",
      "   Dataset: lore_training_data\n",
      "   Notebook Project Root: R:\\Files Ruben\\GitRepos\\DeepDiveV2AI\n",
      "   LLaMA-Factory Path: C:\\Users\\ruben\\Documents\\TrainingAI\\LLaMA-Factory\n",
      "   llama.cpp Path: C:\\Users\\ruben\\Documents\\TrainingAI\\llama.cpp\n",
      "   LoRA Output Directory: R:\\Files Ruben\\GitRepos\\DeepDiveV2AI\\artifacts\\lora_adapters\\Meta-Llama-3-8B-Instruct\\ProjectDeepDive-Lora-v1\n",
      "   Merge Output Directory: R:\\Files Ruben\\GitRepos\\DeepDiveV2AI\\artifacts\\merged_models\\Meta-Llama-3-8B-Instruct-ProjectDeepDive-Lora-v1\n",
      "   Final GGUF Directory: R:\\Files Ruben\\GitRepos\\DeepDiveV2AI\\final_gguf_models\n",
      "   Final GGUF File: R:\\Files Ruben\\GitRepos\\DeepDiveV2AI\\final_gguf_models\\Meta-Llama-3-8B-Instruct-ProjectDeepDive-Lora-v1-Q4_K_M.gguf\n",
      "   Quantize Binary: Not found (build llama.cpp to enable Q4+)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import subprocess\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# --- 1. CORE CONFIGURATION ---\n",
    "MODEL_ID = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "LORA_OUTPUT_NAME = \"ProjectDeepDive-Lora-v1\"\n",
    "DATASET_NAME = \"lore_training_data\"\n",
    "\n",
    "# --- 2. TRAINING HYPERPARAMETERS ---\n",
    "EPOCHS = 5.0\n",
    "BATCH_SIZE = 1\n",
    "GRADIENT_ACCUMULATION = 4\n",
    "\n",
    "# --- 2b. GGUF EXPORT SETTINGS ---\n",
    "QUANT_TYPE = \"q4_k_m\"\n",
    "KEEP_FP16_INTERMEDIATE = False\n",
    "CONVERTER_SUPPORTED_OUTTYPES = {\"f32\", \"f16\", \"bf16\", \"q8_0\", \"tq1_0\", \"tq2_0\", \"auto\"}\n",
    "\n",
    "# --- 3. ENVIRONMENT & PATH SETUP ---\n",
    "PROJECT_ROOT_OVERRIDE = r\"\"  # Optional absolute path if the notebook starts elsewhere.\n",
    "LLAMA_FACTORY_PATH = Path(os.environ.get(\"LLAMA_FACTORY_PATH\", r\"C:\\Users\\ruben\\Documents\\TrainingAI\\LLaMA-Factory\")).expanduser().resolve()\n",
    "LLAMA_CPP_PATH = Path(os.environ.get(\"LLAMA_CPP_PATH\", r\"C:\\Users\\ruben\\Documents\\TrainingAI\\llama.cpp\")).expanduser().resolve()\n",
    "LM_STUDIO_MODELS_DIR = Path(os.environ.get(\"LM_STUDIO_MODELS_DIR\", r\"C:\\Users\\ruben\\.lmstudio\\models\")).expanduser().resolve()\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\", \"...\").strip()\n",
    "PYTHON_EXECUTABLE = os.environ.get(\"PYTHON\", sys.executable)\n",
    "\n",
    "DATASET_FILE_NAME = f\"{DATASET_NAME}.json\"\n",
    "DATASET_INFO_FILE_NAME = \"dataset_info.json\"\n",
    "\n",
    "\n",
    "def resolve_project_root() -> Path:\n",
    "    if PROJECT_ROOT_OVERRIDE:\n",
    "        candidate = Path(PROJECT_ROOT_OVERRIDE).expanduser().resolve()\n",
    "        if candidate.is_dir():\n",
    "            return candidate\n",
    "        raise FileNotFoundError(f\"CRITICAL: PROJECT_ROOT_OVERRIDE='{PROJECT_ROOT_OVERRIDE}' does not exist.\")\n",
    "\n",
    "    cwd = Path().resolve()\n",
    "    for candidate in [cwd, *cwd.parents]:\n",
    "        dataset_file = candidate / DATASET_FILE_NAME\n",
    "        dataset_info_file = candidate / DATASET_INFO_FILE_NAME\n",
    "        if dataset_file.exists() and dataset_info_file.exists():\n",
    "            return candidate\n",
    "\n",
    "    raise FileNotFoundError(\n",
    "        \"CRITICAL: Unable to locate dataset files. Set PROJECT_ROOT_OVERRIDE to the folder that contains both \"\n",
    "        f\"'{DATASET_FILE_NAME}' and '{DATASET_INFO_FILE_NAME}'.\"\n",
    "    )\n",
    "\n",
    "\n",
    "PROJECT_ROOT = resolve_project_root()\n",
    "model_folder_name = MODEL_ID.split('/')[-1]\n",
    "DATASET_FILE_PATH = PROJECT_ROOT / DATASET_FILE_NAME\n",
    "DATASET_INFO_FILE_PATH = PROJECT_ROOT / DATASET_INFO_FILE_NAME\n",
    "\n",
    "if not DATASET_FILE_PATH.exists():\n",
    "    raise FileNotFoundError(f\"CRITICAL: Dataset file not found at '{DATASET_FILE_PATH}'.\")\n",
    "if not DATASET_INFO_FILE_PATH.exists():\n",
    "    raise FileNotFoundError(f\"CRITICAL: dataset_info.json not found at '{DATASET_INFO_FILE_PATH}'.\")\n",
    "\n",
    "ARTIFACTS_ROOT = PROJECT_ROOT / \"artifacts\"\n",
    "LORA_OUTPUT_DIR = ARTIFACTS_ROOT / \"lora_adapters\" / model_folder_name / LORA_OUTPUT_NAME\n",
    "MERGED_MODEL_DIR = ARTIFACTS_ROOT / \"merged_models\" / f\"{model_folder_name}-{LORA_OUTPUT_NAME}\"\n",
    "FINAL_GGUF_DIR = PROJECT_ROOT / \"final_gguf_models\"\n",
    "FINAL_GGUF_FILE = FINAL_GGUF_DIR / f\"{model_folder_name}-{LORA_OUTPUT_NAME}-{QUANT_TYPE.upper()}.gguf\"\n",
    "\n",
    "for path in [ARTIFACTS_ROOT, LORA_OUTPUT_DIR.parent, MERGED_MODEL_DIR.parent, FINAL_GGUF_DIR]:\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "LLAMA_FACTORY_SRC = LLAMA_FACTORY_PATH / \"src\"\n",
    "if not LLAMA_FACTORY_SRC.exists():\n",
    "    raise FileNotFoundError(f\"CRITICAL: '{LLAMA_FACTORY_SRC}' not found. Verify LLAMA_FACTORY_PATH.\")\n",
    "\n",
    "TRAIN_SCRIPT = LLAMA_FACTORY_SRC / \"train.py\"\n",
    "\n",
    "CONVERT_SCRIPT = None\n",
    "for script_name in (\"convert-hf-to-gguf.py\", \"convert_hf_to_gguf.py\"):\n",
    "    candidate = LLAMA_CPP_PATH / script_name\n",
    "    if candidate.exists():\n",
    "        CONVERT_SCRIPT = candidate\n",
    "        break\n",
    "\n",
    "\n",
    "def detect_quantize_binary(root: Path) -> Path | None:\n",
    "    candidates = [\n",
    "        root / \"build\" / \"bin\" / \"quantize\",\n",
    "        root / \"build\" / \"bin\" / \"quantize.exe\",\n",
    "        root / \"build\" / \"bin\" / \"Release\" / \"quantize.exe\",\n",
    "        root / \"build\" / \"Release\" / \"quantize.exe\",\n",
    "        root / \"build\" / \"quantize.exe\",\n",
    "        root / \"quantize\",\n",
    "        root / \"quantize.exe\",\n",
    "    ]\n",
    "    for candidate in candidates:\n",
    "        if candidate.exists():\n",
    "            return candidate.resolve()\n",
    "    return None\n",
    "\n",
    "QUANTIZE_BINARY = detect_quantize_binary(LLAMA_CPP_PATH)\n",
    "\n",
    "if not TRAIN_SCRIPT.exists():\n",
    "    raise FileNotFoundError(f\"CRITICAL: train.py not found at '{TRAIN_SCRIPT}'. Update LLAMA_FACTORY_PATH.\")\n",
    "if CONVERT_SCRIPT is None:\n",
    "    raise FileNotFoundError(\n",
    "        \"CRITICAL: convert-hf-to-gguf.py (or convert_hf_to_gguf.py) not found inside llama.cpp. \"\n",
    "        \"Update LLAMA_CPP_PATH to point at your llama.cpp clone.\"\n",
    "    )\n",
    "\n",
    "\n",
    "def build_llamafactory_env() -> dict[str, str]:\n",
    "    env = os.environ.copy()\n",
    "    llama_src_str = str(LLAMA_FACTORY_SRC)\n",
    "    existing = env.get(\"PYTHONPATH\", \"\")\n",
    "    paths = [p for p in existing.split(os.pathsep) if p]\n",
    "    if llama_src_str not in paths:\n",
    "        paths.insert(0, llama_src_str)\n",
    "    env[\"PYTHONPATH\"] = os.pathsep.join(paths) if paths else llama_src_str\n",
    "    return env\n",
    "\n",
    "\n",
    "LLAMA_FACTORY_ENV = build_llamafactory_env()\n",
    "\n",
    "print(\"‚úÖ Configuration loaded successfully.\")\n",
    "print(f\"   Model ID: {MODEL_ID}\")\n",
    "print(f\"   Dataset: {DATASET_NAME}\")\n",
    "print(f\"   Notebook Project Root: {PROJECT_ROOT}\")\n",
    "print(f\"   LLaMA-Factory Path: {LLAMA_FACTORY_PATH}\")\n",
    "print(f\"   llama.cpp Path: {LLAMA_CPP_PATH}\")\n",
    "print(f\"   LoRA Output Directory: {LORA_OUTPUT_DIR}\")\n",
    "print(f\"   Merge Output Directory: {MERGED_MODEL_DIR}\")\n",
    "print(f\"   Final GGUF Directory: {FINAL_GGUF_DIR}\")\n",
    "print(f\"   Final GGUF File: {FINAL_GGUF_FILE}\")\n",
    "print(f\"   Quantize Binary: {QUANTIZE_BINARY if QUANTIZE_BINARY else 'Not found (build llama.cpp to enable Q4+)'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2539263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dataset 'R:\\Files Ruben\\GitRepos\\DeepDiveV2AI\\lore_training_data.json' loaded successfully.\n",
      "   Found 7 question/answer pairs for training.\n",
      "   ‚ö†Ô∏è WARNING: Dataset is very small. Consider adding more examples for better results.\n"
     ]
    }
   ],
   "source": [
    "# Verify the dataset can be loaded and count the entries\n",
    "try:\n",
    "    with open(DATASET_FILE_PATH, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    num_instructions = len(data)\n",
    "    print(f\"‚úÖ Dataset '{DATASET_FILE_PATH}' loaded successfully.\")\n",
    "    print(f\"   Found {num_instructions} question/answer pairs for training.\")\n",
    "    if num_instructions < 10:\n",
    "        print(\"   ‚ö†Ô∏è WARNING: Dataset is very small. Consider adding more examples for better results.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ERROR: Failed to read or parse the dataset file. Please check for syntax errors in your JSON.\")\n",
    "    print(f\"   Details: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "258ce5b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully stored Hugging Face token.\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfFolder\n",
    "\n",
    "if not HF_TOKEN:\n",
    "    raise ValueError(\"CRITICAL: HF_TOKEN is empty. Set it in Cell 1 or via the HF_TOKEN env var.\")\n",
    "\n",
    "try:\n",
    "    HfFolder.save_token(HF_TOKEN)\n",
    "    print(\"‚úÖ Successfully stored Hugging Face token.\")\n",
    "except Exception as e:\n",
    "    print(\"‚ùå Failed to store Hugging Face token.\")\n",
    "    print(f\"   Details: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d4da2e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training Command ---\n",
      "c:\\Users\\ruben\\AppData\\Local\\Programs\\Python\\Python311\\python.exe C:\\Users\\ruben\\Documents\\TrainingAI\\LLaMA-Factory\\src\\train.py --model_name_or_path meta-llama/Meta-Llama-3-8B-Instruct --do_train --dataset lore_training_data --dataset_dir \"R:\\Files Ruben\\GitRepos\\DeepDiveV2AI\" --finetuning_type lora --output_dir \"R:\\Files Ruben\\GitRepos\\DeepDiveV2AI\\artifacts\\lora_adapters\\Meta-Llama-3-8B-Instruct\\ProjectDeepDive-Lora-v1\" --lora_target all --per_device_train_batch_size 1 --gradient_accumulation_steps 4 --num_train_epochs 5.0 --overwrite_output_dir --plot_loss --fp16\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "command = [\n",
    "    PYTHON_EXECUTABLE, str(TRAIN_SCRIPT),\n",
    "    \"--model_name_or_path\", MODEL_ID,\n",
    "    \"--do_train\",\n",
    "    \"--dataset\", DATASET_NAME,\n",
    "    \"--dataset_dir\", str(PROJECT_ROOT),\n",
    "    \"--finetuning_type\", \"lora\",\n",
    "    \"--output_dir\", str(LORA_OUTPUT_DIR),\n",
    "    \"--lora_target\", \"all\",\n",
    "    \"--per_device_train_batch_size\", str(BATCH_SIZE),\n",
    "    \"--gradient_accumulation_steps\", str(GRADIENT_ACCUMULATION),\n",
    "    \"--num_train_epochs\", str(EPOCHS),\n",
    "    \"--overwrite_output_dir\",\n",
    "    \"--plot_loss\",\n",
    "    \"--fp16\"\n",
    "]\n",
    "print(\"--- Training Command ---\")\n",
    "print(subprocess.list2cmdline(command))\n",
    "print(\"------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2885c931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting training... This may take a while.\n",
      "[INFO|2025-11-18 17:48:25] llamafactory.hparams.parser:468 >> Process rank: 0, world size: 1, device: cpu, distributed training: False, compute dtype: torch.float16\n",
      "[INFO|tokenization_utils_base.py:2095] 2025-11-18 17:48:25,620 >> loading file tokenizer.json from cache at C:\\Users\\ruben\\.cache\\huggingface\\hub\\models--meta-llama--Meta-Llama-3-8B-Instruct\\snapshots\\8afb486c1db24fe5011ec46dfbe5b5dccdb575c2\\tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2095] 2025-11-18 17:48:25,620 >> loading file tokenizer.model from cache at None\n",
      "[INFO|tokenization_utils_base.py:2095] 2025-11-18 17:48:25,620 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2095] 2025-11-18 17:48:25,620 >> loading file special_tokens_map.json from cache at C:\\Users\\ruben\\.cache\\huggingface\\hub\\models--meta-llama--Meta-Llama-3-8B-Instruct\\snapshots\\8afb486c1db24fe5011ec46dfbe5b5dccdb575c2\\special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2095] 2025-11-18 17:48:25,620 >> loading file tokenizer_config.json from cache at C:\\Users\\ruben\\.cache\\huggingface\\hub\\models--meta-llama--Meta-Llama-3-8B-Instruct\\snapshots\\8afb486c1db24fe5011ec46dfbe5b5dccdb575c2\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2095] 2025-11-18 17:48:25,620 >> loading file chat_template.jinja from cache at None\n",
      "[INFO|tokenization_utils_base.py:2364] 2025-11-18 17:48:25,880 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|configuration_utils.py:765] 2025-11-18 17:48:26,540 >> loading configuration file config.json from cache at C:\\Users\\ruben\\.cache\\huggingface\\hub\\models--meta-llama--Meta-Llama-3-8B-Instruct\\snapshots\\8afb486c1db24fe5011ec46dfbe5b5dccdb575c2\\config.json\n",
      "[INFO|configuration_utils.py:839] 2025-11-18 17:48:26,543 >> Model config LlamaConfig {\n",
      "\"architectures\": [\n",
      "\"LlamaForCausalLM\"\n",
      "],\n",
      "\"attention_bias\": false,\n",
      "\"attention_dropout\": 0.0,\n",
      "\"bos_token_id\": 128000,\n",
      "\"dtype\": \"bfloat16\",\n",
      "\"eos_token_id\": 128009,\n",
      "\"head_dim\": 128,\n",
      "\"hidden_act\": \"silu\",\n",
      "\"hidden_size\": 4096,\n",
      "\"initializer_range\": 0.02,\n",
      "\"intermediate_size\": 14336,\n",
      "\"max_position_embeddings\": 8192,\n",
      "\"mlp_bias\": false,\n",
      "\"model_type\": \"llama\",\n",
      "\"num_attention_heads\": 32,\n",
      "\"num_hidden_layers\": 32,\n",
      "\"num_key_value_heads\": 8,\n",
      "\"pretraining_tp\": 1,\n",
      "\"rms_norm_eps\": 1e-05,\n",
      "\"rope_scaling\": null,\n",
      "\"rope_theta\": 500000.0,\n",
      "\"tie_word_embeddings\": false,\n",
      "\"transformers_version\": \"4.57.1\",\n",
      "\"use_cache\": true,\n",
      "\"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2095] 2025-11-18 17:48:26,773 >> loading file tokenizer.json from cache at C:\\Users\\ruben\\.cache\\huggingface\\hub\\models--meta-llama--Meta-Llama-3-8B-Instruct\\snapshots\\8afb486c1db24fe5011ec46dfbe5b5dccdb575c2\\tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2095] 2025-11-18 17:48:26,773 >> loading file tokenizer.model from cache at None\n",
      "[INFO|tokenization_utils_base.py:2095] 2025-11-18 17:48:26,773 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2095] 2025-11-18 17:48:26,773 >> loading file special_tokens_map.json from cache at C:\\Users\\ruben\\.cache\\huggingface\\hub\\models--meta-llama--Meta-Llama-3-8B-Instruct\\snapshots\\8afb486c1db24fe5011ec46dfbe5b5dccdb575c2\\special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2095] 2025-11-18 17:48:26,773 >> loading file tokenizer_config.json from cache at C:\\Users\\ruben\\.cache\\huggingface\\hub\\models--meta-llama--Meta-Llama-3-8B-Instruct\\snapshots\\8afb486c1db24fe5011ec46dfbe5b5dccdb575c2\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2095] 2025-11-18 17:48:26,773 >> loading file chat_template.jinja from cache at None\n",
      "[INFO|tokenization_utils_base.py:2364] 2025-11-18 17:48:27,031 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[WARNING|2025-11-18 17:48:27] llamafactory.data.template:148 >> `template` was not specified, try parsing the chat template from the tokenizer.\n",
      "[INFO|2025-11-18 17:48:27] llamafactory.data.template:143 >> Add pad token: <|eot_id|>\n",
      "[INFO|2025-11-18 17:48:27] llamafactory.data.loader:143 >> Loading dataset lore_training_data.json...\n",
      "training example:\n",
      "input_ids:\n",
      "[128000, 128006, 882, 128007, 271, 3923, 374, 279, 480, 18981, 30, 128009, 128006, 78191, 128007, 271, 791, 480, 18981, 374, 279, 2324, 51105, 323, 279, 41100, 315, 1521, 43957, 13, 1102, 596, 459, 4907, 11, 264, 9546, 1131, 2555, 14154, 13, 1102, 6835, 603, 279, 48473, 584, 6056, 11, 719, 433, 1101, 66332, 279, 70618, 430, 19614, 603, 13, 4418, 956, 46943, 1139, 433, 2288, 1317, 26, 433, 95502, 1203, 13, 128009]\n",
      "inputs:\n",
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What is the Gloom?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "The Gloom is the lifeblood and the curse of these depths. It's an energy, a presence... something ancient. It gives us the crystals we seek, but it also births the horrors that hunt us. Don't stare into it too long; it stares back.<|eot_id|>\n",
      "label_ids:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 791, 480, 18981, 374, 279, 2324, 51105, 323, 279, 41100, 315, 1521, 43957, 13, 1102, 596, 459, 4907, 11, 264, 9546, 1131, 2555, 14154, 13, 1102, 6835, 603, 279, 48473, 584, 6056, 11, 719, 433, 1101, 66332, 279, 70618, 430, 19614, 603, 13, 4418, 956, 46943, 1139, 433, 2288, 1317, 26, 433, 95502, 1203, 13, 128009]\n",
      "labels:\n",
      "The Gloom is the lifeblood and the curse of these depths. It's an energy, a presence... something ancient. It gives us the crystals we seek, but it also births the horrors that hunt us. Don't stare into it too long; it stares back.<|eot_id|>\n",
      "[INFO|configuration_utils.py:765] 2025-11-18 17:48:27,750 >> loading configuration file config.json from cache at C:\\Users\\ruben\\.cache\\huggingface\\hub\\models--meta-llama--Meta-Llama-3-8B-Instruct\\snapshots\\8afb486c1db24fe5011ec46dfbe5b5dccdb575c2\\config.json\n",
      "[INFO|configuration_utils.py:839] 2025-11-18 17:48:27,750 >> Model config LlamaConfig {\n",
      "\"architectures\": [\n",
      "\"LlamaForCausalLM\"\n",
      "],\n",
      "\"attention_bias\": false,\n",
      "\"attention_dropout\": 0.0,\n",
      "\"bos_token_id\": 128000,\n",
      "\"dtype\": \"bfloat16\",\n",
      "\"eos_token_id\": 128009,\n",
      "\"head_dim\": 128,\n",
      "\"hidden_act\": \"silu\",\n",
      "\"hidden_size\": 4096,\n",
      "\"initializer_range\": 0.02,\n",
      "\"intermediate_size\": 14336,\n",
      "\"max_position_embeddings\": 8192,\n",
      "\"mlp_bias\": false,\n",
      "\"model_type\": \"llama\",\n",
      "\"num_attention_heads\": 32,\n",
      "\"num_hidden_layers\": 32,\n",
      "\"num_key_value_heads\": 8,\n",
      "\"pretraining_tp\": 1,\n",
      "\"rms_norm_eps\": 1e-05,\n",
      "\"rope_scaling\": null,\n",
      "\"rope_theta\": 500000.0,\n",
      "\"tie_word_embeddings\": false,\n",
      "\"transformers_version\": \"4.57.1\",\n",
      "\"use_cache\": true,\n",
      "\"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|2025-11-18 17:48:27] llamafactory.model.model_utils.kv_cache:143 >> KV cache is disabled during training.\n",
      "[WARNING|logging.py:328] 2025-11-18 17:48:27,951 >> `torch_dtype` is deprecated! Use `dtype` instead!\n",
      "[INFO|modeling_utils.py:1172] 2025-11-18 17:48:27,953 >> loading weights file model.safetensors from cache at C:\\Users\\ruben\\.cache\\huggingface\\hub\\models--meta-llama--Meta-Llama-3-8B-Instruct\\snapshots\\8afb486c1db24fe5011ec46dfbe5b5dccdb575c2\\model.safetensors.index.json\n",
      "[INFO|modeling_utils.py:2341] 2025-11-18 17:48:27,954 >> Instantiating LlamaForCausalLM model under default dtype torch.float16.\n",
      "[INFO|configuration_utils.py:986] 2025-11-18 17:48:27,954 >> Generate config GenerationConfig {\n",
      "\"bos_token_id\": 128000,\n",
      "\"eos_token_id\": 128009,\n",
      "\"use_cache\": false\n",
      "}\n",
      "\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\n",
      "Loading checkpoint shards:  25%|‚ñà‚ñà‚ñå       | 1/4 [00:01<00:03,  1.03s/it]\n",
      "\n",
      "‚ùå Training failed with exit code 3221225477.\n"
     ]
    }
   ],
   "source": [
    "print(\"üöÄ Starting training... This may take a while.\")\n",
    "process = subprocess.Popen(\n",
    "    command,\n",
    "    cwd=str(LLAMA_FACTORY_PATH),\n",
    "    env=LLAMA_FACTORY_ENV,\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.STDOUT,\n",
    "    text=True,\n",
    "    encoding='utf-8',\n",
    "    bufsize=1,\n",
    ")\n",
    "while True:\n",
    "    output = process.stdout.readline()\n",
    "    if output == '' and process.poll() is not None:\n",
    "        break\n",
    "    if output:\n",
    "        print(output.strip())\n",
    "if process.returncode == 0:\n",
    "    print(\"\\nüéâ Training finished successfully! üéâ\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå Training failed with exit code {process.returncode}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08741845",
   "metadata": {},
   "source": [
    "### Step 2: Merge LoRA and Quantize to GGUF\n",
    "\n",
    "Now that the LoRA adapter is trained, we will perform two final steps:\n",
    "1.  **Merge:** Combine the base Llama 3 model with our LoRA adapter to create a new, full-sized (unquantized) model.\n",
    "2.  **Quantize:** Compress the large, merged model into a single, efficient GGUF file that LM Studio can use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22c3ddbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting model merge process...\n",
      "--- Merge Command ---\n",
      "c:\\Users\\ruben\\AppData\\Local\\Programs\\Python\\Python311\\python.exe -m llamafactory.launcher export --model_name_or_path meta-llama/Meta-Llama-3-8B-Instruct --adapter_name_or_path \"R:\\Files Ruben\\GitRepos\\DeepDiveV2AI\\artifacts\\lora_adapters\\Meta-Llama-3-8B-Instruct\\ProjectDeepDive-Lora-v1\" --template llama3 --export_dir \"R:\\Files Ruben\\GitRepos\\DeepDiveV2AI\\artifacts\\merged_models\\Meta-Llama-3-8B-Instruct-ProjectDeepDive-Lora-v1\" --export_size 2\n",
      "---------------------\n",
      "usage: launcher.py [-h] [--model_name_or_path MODEL_NAME_OR_PATH]\n",
      "[--adapter_name_or_path ADAPTER_NAME_OR_PATH]\n",
      "[--adapter_folder ADAPTER_FOLDER] [--cache_dir CACHE_DIR]\n",
      "[--use_fast_tokenizer [USE_FAST_TOKENIZER]]\n",
      "[--no_use_fast_tokenizer] [--resize_vocab [RESIZE_VOCAB]]\n",
      "[--split_special_tokens [SPLIT_SPECIAL_TOKENS]]\n",
      "[--add_tokens ADD_TOKENS]\n",
      "[--add_special_tokens ADD_SPECIAL_TOKENS]\n",
      "[--new_special_tokens_config NEW_SPECIAL_TOKENS_CONFIG]\n",
      "[--init_special_tokens {noise_init,desc_init,desc_init_w_noise}]\n",
      "[--model_revision MODEL_REVISION]\n",
      "[--low_cpu_mem_usage [LOW_CPU_MEM_USAGE]]\n",
      "[--no_low_cpu_mem_usage]\n",
      "[--rope_scaling {linear,dynamic,yarn,llama3}]\n",
      "[--flash_attn {auto,disabled,sdpa,fa2}]\n",
      "[--shift_attn [SHIFT_ATTN]]\n",
      "[--mixture_of_depths {convert,load}]\n",
      "[--use_unsloth [USE_UNSLOTH]]\n",
      "[--use_unsloth_gc [USE_UNSLOTH_GC]]\n",
      "[--enable_liger_kernel [ENABLE_LIGER_KERNEL]]\n",
      "[--moe_aux_loss_coef MOE_AUX_LOSS_COEF]\n",
      "[--disable_gradient_checkpointing [DISABLE_GRADIENT_CHECKPOINTING]]\n",
      "[--use_reentrant_gc [USE_REENTRANT_GC]]\n",
      "[--no_use_reentrant_gc]\n",
      "[--upcast_layernorm [UPCAST_LAYERNORM]]\n",
      "[--upcast_lmhead_output [UPCAST_LMHEAD_OUTPUT]]\n",
      "[--train_from_scratch [TRAIN_FROM_SCRATCH]]\n",
      "[--infer_backend {huggingface,vllm,sglang,ktransformers}]\n",
      "[--offload_folder OFFLOAD_FOLDER]\n",
      "[--use_kv_cache [USE_KV_CACHE]] [--no_use_kv_cache]\n",
      "[--infer_dtype {auto,float16,bfloat16,float32}]\n",
      "[--hf_hub_token HF_HUB_TOKEN] [--ms_hub_token MS_HUB_TOKEN]\n",
      "[--om_hub_token OM_HUB_TOKEN]\n",
      "[--print_param_status [PRINT_PARAM_STATUS]]\n",
      "[--trust_remote_code [TRUST_REMOTE_CODE]]\n",
      "[--quantization_method {bnb,gptq,awq,aqlm,quanto,eetq,hqq,mxfp4}]\n",
      "[--quantization_bit QUANTIZATION_BIT]\n",
      "[--quantization_type {fp4,nf4}]\n",
      "[--double_quantization [DOUBLE_QUANTIZATION]]\n",
      "[--no_double_quantization]\n",
      "[--quantization_device_map {auto}] [--fp8 [FP8]]\n",
      "[--fp8_backend FP8_BACKEND]\n",
      "[--fp8_enable_fsdp_float8_all_gather [FP8_ENABLE_FSDP_FLOAT8_ALL_GATHER]]\n",
      "[--image_max_pixels IMAGE_MAX_PIXELS]\n",
      "[--image_min_pixels IMAGE_MIN_PIXELS]\n",
      "[--image_do_pan_and_scan [IMAGE_DO_PAN_AND_SCAN]]\n",
      "[--crop_to_patches [CROP_TO_PATCHES]]\n",
      "[--video_max_pixels VIDEO_MAX_PIXELS]\n",
      "[--video_min_pixels VIDEO_MIN_PIXELS]\n",
      "[--video_fps VIDEO_FPS] [--video_maxlen VIDEO_MAXLEN]\n",
      "[--use_audio_in_video [USE_AUDIO_IN_VIDEO]]\n",
      "[--audio_sampling_rate AUDIO_SAMPLING_RATE]\n",
      "[--export_dir EXPORT_DIR] [--export_size EXPORT_SIZE]\n",
      "[--export_device {cpu,auto}]\n",
      "[--export_quantization_bit EXPORT_QUANTIZATION_BIT]\n",
      "[--export_quantization_dataset EXPORT_QUANTIZATION_DATASET]\n",
      "[--export_quantization_nsamples EXPORT_QUANTIZATION_NSAMPLES]\n",
      "[--export_quantization_maxlen EXPORT_QUANTIZATION_MAXLEN]\n",
      "[--export_legacy_format [EXPORT_LEGACY_FORMAT]]\n",
      "[--export_hub_model_id EXPORT_HUB_MODEL_ID]\n",
      "[--use_kt [USE_KT]] [--kt_optimize_rule KT_OPTIMIZE_RULE]\n",
      "[--cpu_infer CPU_INFER] [--chunk_size CHUNK_SIZE]\n",
      "[--mode MODE] [--kt_maxlen KT_MAXLEN]\n",
      "[--kt_use_cuda_graph [KT_USE_CUDA_GRAPH]]\n",
      "[--no_kt_use_cuda_graph] [--kt_mode KT_MODE]\n",
      "[--kt_force_think [KT_FORCE_THINK]]\n",
      "[--vllm_maxlen VLLM_MAXLEN] [--vllm_gpu_util VLLM_GPU_UTIL]\n",
      "[--vllm_enforce_eager [VLLM_ENFORCE_EAGER]]\n",
      "[--vllm_max_lora_rank VLLM_MAX_LORA_RANK]\n",
      "[--vllm_config VLLM_CONFIG] [--sglang_maxlen SGLANG_MAXLEN]\n",
      "[--sglang_mem_fraction SGLANG_MEM_FRACTION]\n",
      "[--sglang_tp_size SGLANG_TP_SIZE]\n",
      "[--sglang_config SGLANG_CONFIG]\n",
      "[--sglang_lora_backend {triton,flashinfer}]\n",
      "[--template TEMPLATE] [--dataset DATASET]\n",
      "[--eval_dataset EVAL_DATASET] [--dataset_dir DATASET_DIR]\n",
      "[--media_dir MEDIA_DIR] [--cutoff_len CUTOFF_LEN]\n",
      "[--train_on_prompt [TRAIN_ON_PROMPT]]\n",
      "[--mask_history [MASK_HISTORY]] [--streaming [STREAMING]]\n",
      "[--buffer_size BUFFER_SIZE]\n",
      "[--mix_strategy {concat,interleave_under,interleave_over}]\n",
      "[--interleave_probs INTERLEAVE_PROBS]\n",
      "[--overwrite_cache [OVERWRITE_CACHE]]\n",
      "[--preprocessing_batch_size PREPROCESSING_BATCH_SIZE]\n",
      "[--preprocessing_num_workers PREPROCESSING_NUM_WORKERS]\n",
      "[--max_samples MAX_SAMPLES]\n",
      "[--eval_num_beams EVAL_NUM_BEAMS]\n",
      "[--ignore_pad_token_for_loss [IGNORE_PAD_TOKEN_FOR_LOSS]]\n",
      "[--no_ignore_pad_token_for_loss] [--val_size VAL_SIZE]\n",
      "[--eval_on_each_dataset [EVAL_ON_EACH_DATASET]]\n",
      "[--packing PACKING] [--neat_packing [NEAT_PACKING]]\n",
      "[--tool_format TOOL_FORMAT]\n",
      "[--default_system DEFAULT_SYSTEM]\n",
      "[--enable_thinking [ENABLE_THINKING]]\n",
      "[--no_enable_thinking] [--tokenized_path TOKENIZED_PATH]\n",
      "[--data_shared_file_system [DATA_SHARED_FILE_SYSTEM]]\n",
      "[--output_dir OUTPUT_DIR]\n",
      "[--overwrite_output_dir [OVERWRITE_OUTPUT_DIR]]\n",
      "[--do_train [DO_TRAIN]] [--do_eval [DO_EVAL]]\n",
      "[--do_predict [DO_PREDICT]]\n",
      "[--eval_strategy {no,steps,epoch}]\n",
      "[--prediction_loss_only [PREDICTION_LOSS_ONLY]]\n",
      "[--per_device_train_batch_size PER_DEVICE_TRAIN_BATCH_SIZE]\n",
      "[--per_device_eval_batch_size PER_DEVICE_EVAL_BATCH_SIZE]\n",
      "[--per_gpu_train_batch_size PER_GPU_TRAIN_BATCH_SIZE]\n",
      "[--per_gpu_eval_batch_size PER_GPU_EVAL_BATCH_SIZE]\n",
      "[--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS]\n",
      "[--eval_accumulation_steps EVAL_ACCUMULATION_STEPS]\n",
      "[--eval_delay EVAL_DELAY]\n",
      "[--torch_empty_cache_steps TORCH_EMPTY_CACHE_STEPS]\n",
      "[--learning_rate LEARNING_RATE]\n",
      "[--weight_decay WEIGHT_DECAY] [--adam_beta1 ADAM_BETA1]\n",
      "[--adam_beta2 ADAM_BETA2] [--adam_epsilon ADAM_EPSILON]\n",
      "[--max_grad_norm MAX_GRAD_NORM]\n",
      "[--num_train_epochs NUM_TRAIN_EPOCHS]\n",
      "[--max_steps MAX_STEPS]\n",
      "[--lr_scheduler_type {linear,cosine,cosine_with_restarts,polynomial,constant,constant_with_warmup,inverse_sqrt,reduce_lr_on_plateau,cosine_with_min_lr,cosine_warmup_with_min_lr,warmup_stable_decay}]\n",
      "[--lr_scheduler_kwargs LR_SCHEDULER_KWARGS]\n",
      "[--warmup_ratio WARMUP_RATIO] [--warmup_steps WARMUP_STEPS]\n",
      "[--log_level {detail,debug,info,warning,error,critical,passive}]\n",
      "[--log_level_replica {detail,debug,info,warning,error,critical,passive}]\n",
      "[--log_on_each_node [LOG_ON_EACH_NODE]]\n",
      "[--no_log_on_each_node] [--logging_dir LOGGING_DIR]\n",
      "[--logging_strategy {no,steps,epoch}]\n",
      "[--logging_first_step [LOGGING_FIRST_STEP]]\n",
      "[--logging_steps LOGGING_STEPS]\n",
      "[--logging_nan_inf_filter [LOGGING_NAN_INF_FILTER]]\n",
      "[--no_logging_nan_inf_filter]\n",
      "[--save_strategy {no,steps,epoch,best}]\n",
      "[--save_steps SAVE_STEPS]\n",
      "[--save_total_limit SAVE_TOTAL_LIMIT]\n",
      "[--save_safetensors [SAVE_SAFETENSORS]]\n",
      "[--no_save_safetensors]\n",
      "[--save_on_each_node [SAVE_ON_EACH_NODE]]\n",
      "[--save_only_model [SAVE_ONLY_MODEL]]\n",
      "[--restore_callback_states_from_checkpoint [RESTORE_CALLBACK_STATES_FROM_CHECKPOINT]]\n",
      "[--no_cuda [NO_CUDA]] [--use_cpu [USE_CPU]]\n",
      "[--use_mps_device [USE_MPS_DEVICE]] [--seed SEED]\n",
      "[--data_seed DATA_SEED] [--jit_mode_eval [JIT_MODE_EVAL]]\n",
      "[--bf16 [BF16]] [--fp16 [FP16]]\n",
      "[--fp16_opt_level FP16_OPT_LEVEL]\n",
      "[--half_precision_backend {auto,apex,cpu_amp}]\n",
      "[--bf16_full_eval [BF16_FULL_EVAL]]\n",
      "[--fp16_full_eval [FP16_FULL_EVAL]] [--tf32 TF32]\n",
      "[--local_rank LOCAL_RANK]\n",
      "[--ddp_backend {nccl,gloo,mpi,ccl,hccl,cncl,mccl}]\n",
      "[--tpu_num_cores TPU_NUM_CORES]\n",
      "[--tpu_metrics_debug [TPU_METRICS_DEBUG]]\n",
      "[--debug DEBUG [DEBUG ...]]\n",
      "[--dataloader_drop_last [DATALOADER_DROP_LAST]]\n",
      "[--eval_steps EVAL_STEPS]\n",
      "[--dataloader_num_workers DATALOADER_NUM_WORKERS]\n",
      "[--dataloader_prefetch_factor DATALOADER_PREFETCH_FACTOR]\n",
      "[--past_index PAST_INDEX] [--run_name RUN_NAME]\n",
      "[--disable_tqdm DISABLE_TQDM]\n",
      "[--remove_unused_columns [REMOVE_UNUSED_COLUMNS]]\n",
      "[--no_remove_unused_columns]\n",
      "[--label_names LABEL_NAMES [LABEL_NAMES ...]]\n",
      "[--load_best_model_at_end [LOAD_BEST_MODEL_AT_END]]\n",
      "[--metric_for_best_model METRIC_FOR_BEST_MODEL]\n",
      "[--greater_is_better GREATER_IS_BETTER]\n",
      "[--ignore_data_skip [IGNORE_DATA_SKIP]] [--fsdp FSDP]\n",
      "[--fsdp_min_num_params FSDP_MIN_NUM_PARAMS]\n",
      "[--fsdp_config FSDP_CONFIG]\n",
      "[--fsdp_transformer_layer_cls_to_wrap FSDP_TRANSFORMER_LAYER_CLS_TO_WRAP]\n",
      "[--accelerator_config ACCELERATOR_CONFIG]\n",
      "[--parallelism_config PARALLELISM_CONFIG]\n",
      "[--deepspeed DEEPSPEED]\n",
      "[--label_smoothing_factor LABEL_SMOOTHING_FACTOR]\n",
      "[--optim {adamw_torch,adamw_torch_fused,adamw_torch_xla,adamw_torch_npu_fused,adamw_apex_fused,adafactor,adamw_anyprecision,adamw_torch_4bit,adamw_torch_8bit,ademamix,sgd,adagrad,adamw_bnb_8bit,adamw_8bit,ademamix_8bit,lion_8bit,lion_32bit,paged_adamw_32bit,paged_adamw_8bit,paged_ademamix_32bit,paged_ademamix_8bit,paged_lion_32bit,paged_lion_8bit,rmsprop,rmsprop_bnb,rmsprop_bnb_8bit,rmsprop_bnb_32bit,galore_adamw,galore_adamw_8bit,galore_adafactor,galore_adamw_layerwise,galore_adamw_8bit_layerwise,galore_adafactor_layerwise,lomo,adalomo,grokadamw,schedule_free_radam,schedule_free_adamw,schedule_free_sgd,apollo_adamw,apollo_adamw_layerwise,stable_adamw}]\n",
      "[--optim_args OPTIM_ARGS] [--adafactor [ADAFACTOR]]\n",
      "[--group_by_length [GROUP_BY_LENGTH]]\n",
      "[--length_column_name LENGTH_COLUMN_NAME]\n",
      "[--report_to REPORT_TO] [--project PROJECT]\n",
      "[--trackio_space_id TRACKIO_SPACE_ID]\n",
      "[--ddp_find_unused_parameters DDP_FIND_UNUSED_PARAMETERS]\n",
      "[--ddp_bucket_cap_mb DDP_BUCKET_CAP_MB]\n",
      "[--ddp_broadcast_buffers DDP_BROADCAST_BUFFERS]\n",
      "[--dataloader_pin_memory [DATALOADER_PIN_MEMORY]]\n",
      "[--no_dataloader_pin_memory]\n",
      "[--dataloader_persistent_workers [DATALOADER_PERSISTENT_WORKERS]]\n",
      "[--skip_memory_metrics [SKIP_MEMORY_METRICS]]\n",
      "[--no_skip_memory_metrics]\n",
      "[--use_legacy_prediction_loop [USE_LEGACY_PREDICTION_LOOP]]\n",
      "[--push_to_hub [PUSH_TO_HUB]]\n",
      "[--resume_from_checkpoint RESUME_FROM_CHECKPOINT]\n",
      "[--hub_model_id HUB_MODEL_ID]\n",
      "[--hub_strategy {end,every_save,checkpoint,all_checkpoints}]\n",
      "[--hub_token HUB_TOKEN]\n",
      "[--hub_private_repo HUB_PRIVATE_REPO]\n",
      "[--hub_always_push [HUB_ALWAYS_PUSH]]\n",
      "[--hub_revision HUB_REVISION]\n",
      "[--gradient_checkpointing [GRADIENT_CHECKPOINTING]]\n",
      "[--gradient_checkpointing_kwargs GRADIENT_CHECKPOINTING_KWARGS]\n",
      "[--include_inputs_for_metrics [INCLUDE_INPUTS_FOR_METRICS]]\n",
      "[--include_for_metrics INCLUDE_FOR_METRICS [INCLUDE_FOR_METRICS ...]]\n",
      "[--eval_do_concat_batches [EVAL_DO_CONCAT_BATCHES]]\n",
      "[--no_eval_do_concat_batches]\n",
      "[--fp16_backend {auto,apex,cpu_amp}]\n",
      "[--push_to_hub_model_id PUSH_TO_HUB_MODEL_ID]\n",
      "[--push_to_hub_organization PUSH_TO_HUB_ORGANIZATION]\n",
      "[--push_to_hub_token PUSH_TO_HUB_TOKEN]\n",
      "[--mp_parameters MP_PARAMETERS]\n",
      "[--auto_find_batch_size [AUTO_FIND_BATCH_SIZE]]\n",
      "[--full_determinism [FULL_DETERMINISM]]\n",
      "[--torchdynamo TORCHDYNAMO] [--ray_scope RAY_SCOPE]\n",
      "[--ddp_timeout DDP_TIMEOUT]\n",
      "[--torch_compile [TORCH_COMPILE]]\n",
      "[--torch_compile_backend TORCH_COMPILE_BACKEND]\n",
      "[--torch_compile_mode TORCH_COMPILE_MODE]\n",
      "[--include_tokens_per_second [INCLUDE_TOKENS_PER_SECOND]]\n",
      "[--include_num_input_tokens_seen [INCLUDE_NUM_INPUT_TOKENS_SEEN]]\n",
      "[--neftune_noise_alpha NEFTUNE_NOISE_ALPHA]\n",
      "[--optim_target_modules OPTIM_TARGET_MODULES]\n",
      "[--batch_eval_metrics [BATCH_EVAL_METRICS]]\n",
      "[--eval_on_start [EVAL_ON_START]]\n",
      "[--use_liger_kernel [USE_LIGER_KERNEL]]\n",
      "[--liger_kernel_config LIGER_KERNEL_CONFIG]\n",
      "[--eval_use_gather_object [EVAL_USE_GATHER_OBJECT]]\n",
      "[--average_tokens_across_devices [AVERAGE_TOKENS_ACROSS_DEVICES]]\n",
      "[--no_average_tokens_across_devices]\n",
      "[--sortish_sampler [SORTISH_SAMPLER]]\n",
      "[--predict_with_generate [PREDICT_WITH_GENERATE]]\n",
      "[--generation_max_length GENERATION_MAX_LENGTH]\n",
      "[--generation_num_beams GENERATION_NUM_BEAMS]\n",
      "[--generation_config GENERATION_CONFIG]\n",
      "[--ray_run_name RAY_RUN_NAME]\n",
      "[--ray_storage_path RAY_STORAGE_PATH]\n",
      "[--ray_storage_filesystem {s3,gs,gcs}]\n",
      "[--ray_num_workers RAY_NUM_WORKERS]\n",
      "[--resources_per_worker RESOURCES_PER_WORKER]\n",
      "[--placement_strategy {SPREAD,PACK,STRICT_SPREAD,STRICT_PACK}]\n",
      "[--ray_init_kwargs RAY_INIT_KWARGS]\n",
      "[--freeze_trainable_layers FREEZE_TRAINABLE_LAYERS]\n",
      "[--freeze_trainable_modules FREEZE_TRAINABLE_MODULES]\n",
      "[--freeze_extra_modules FREEZE_EXTRA_MODULES]\n",
      "[--additional_target ADDITIONAL_TARGET]\n",
      "[--module_dropout MODULE_DROPOUT] [--oft_rank OFT_RANK]\n",
      "[--oft_block_size OFT_BLOCK_SIZE] [--oft_target OFT_TARGET]\n",
      "[--create_new_adapter [CREATE_NEW_ADAPTER]]\n",
      "[--lora_alpha LORA_ALPHA] [--lora_dropout LORA_DROPOUT]\n",
      "[--lora_rank LORA_RANK] [--lora_target LORA_TARGET]\n",
      "[--loraplus_lr_ratio LORAPLUS_LR_RATIO]\n",
      "[--loraplus_lr_embedding LORAPLUS_LR_EMBEDDING]\n",
      "[--use_rslora [USE_RSLORA]] [--use_dora [USE_DORA]]\n",
      "[--pissa_init [PISSA_INIT]] [--pissa_iter PISSA_ITER]\n",
      "[--pissa_convert [PISSA_CONVERT]] [--pref_beta PREF_BETA]\n",
      "[--pref_ftx PREF_FTX] [--pref_bco_weight PREF_BCO_WEIGHT]\n",
      "[--pref_loss {sigmoid,hinge,ipo,kto_pair,orpo,simpo}]\n",
      "[--dpo_label_smoothing DPO_LABEL_SMOOTHING]\n",
      "[--kto_chosen_weight KTO_CHOSEN_WEIGHT]\n",
      "[--kto_rejected_weight KTO_REJECTED_WEIGHT]\n",
      "[--simpo_gamma SIMPO_GAMMA]\n",
      "[--ppo_buffer_size PPO_BUFFER_SIZE]\n",
      "[--ppo_epochs PPO_EPOCHS]\n",
      "[--ppo_score_norm [PPO_SCORE_NORM]]\n",
      "[--ppo_target PPO_TARGET]\n",
      "[--ppo_whiten_rewards [PPO_WHITEN_REWARDS]]\n",
      "[--ref_model REF_MODEL]\n",
      "[--ref_model_adapters REF_MODEL_ADAPTERS]\n",
      "[--ref_model_quantization_bit REF_MODEL_QUANTIZATION_BIT]\n",
      "[--reward_model REWARD_MODEL]\n",
      "[--reward_model_adapters REWARD_MODEL_ADAPTERS]\n",
      "[--reward_model_quantization_bit REWARD_MODEL_QUANTIZATION_BIT]\n",
      "[--reward_model_type {lora,full,api}] [--ld_alpha LD_ALPHA]\n",
      "[--use_galore [USE_GALORE]] [--galore_target GALORE_TARGET]\n",
      "[--galore_rank GALORE_RANK]\n",
      "[--galore_update_interval GALORE_UPDATE_INTERVAL]\n",
      "[--galore_scale GALORE_SCALE]\n",
      "[--galore_proj_type {std,reverse_std,right,left,full}]\n",
      "[--galore_layerwise [GALORE_LAYERWISE]]\n",
      "[--use_apollo [USE_APOLLO]] [--apollo_target APOLLO_TARGET]\n",
      "[--apollo_rank APOLLO_RANK]\n",
      "[--apollo_update_interval APOLLO_UPDATE_INTERVAL]\n",
      "[--apollo_scale APOLLO_SCALE] [--apollo_proj {svd,random}]\n",
      "[--apollo_proj_type {std,right,left}]\n",
      "[--apollo_scale_type {channel,tensor}]\n",
      "[--apollo_layerwise [APOLLO_LAYERWISE]]\n",
      "[--apollo_scale_front [APOLLO_SCALE_FRONT]]\n",
      "[--use_badam [USE_BADAM]] [--badam_mode {layer,ratio}]\n",
      "[--badam_start_block BADAM_START_BLOCK]\n",
      "[--badam_switch_mode {ascending,descending,random,fixed}]\n",
      "[--badam_switch_interval BADAM_SWITCH_INTERVAL]\n",
      "[--badam_update_ratio BADAM_UPDATE_RATIO]\n",
      "[--badam_mask_mode {adjacent,scatter}]\n",
      "[--badam_verbose BADAM_VERBOSE]\n",
      "[--use_swanlab [USE_SWANLAB]]\n",
      "[--swanlab_project SWANLAB_PROJECT]\n",
      "[--swanlab_workspace SWANLAB_WORKSPACE]\n",
      "[--swanlab_run_name SWANLAB_RUN_NAME]\n",
      "[--swanlab_mode {cloud,local}]\n",
      "[--swanlab_api_key SWANLAB_API_KEY]\n",
      "[--swanlab_logdir SWANLAB_LOGDIR]\n",
      "[--swanlab_lark_webhook_url SWANLAB_LARK_WEBHOOK_URL]\n",
      "[--swanlab_lark_secret SWANLAB_LARK_SECRET]\n",
      "[--pure_bf16 [PURE_BF16]] [--stage {pt,sft,rm,ppo,dpo,kto}]\n",
      "[--finetuning_type {lora,oft,freeze,full}]\n",
      "[--use_llama_pro [USE_LLAMA_PRO]]\n",
      "[--use_adam_mini [USE_ADAM_MINI]] [--use_mca [USE_MCA]]\n",
      "[--use_muon [USE_MUON]] [--use_dft_loss [USE_DFT_LOSS]]\n",
      "[--freeze_vision_tower [FREEZE_VISION_TOWER]]\n",
      "[--no_freeze_vision_tower]\n",
      "[--freeze_multi_modal_projector [FREEZE_MULTI_MODAL_PROJECTOR]]\n",
      "[--no_freeze_multi_modal_projector]\n",
      "[--freeze_language_model [FREEZE_LANGUAGE_MODEL]]\n",
      "[--compute_accuracy [COMPUTE_ACCURACY]]\n",
      "[--disable_shuffling [DISABLE_SHUFFLING]]\n",
      "[--early_stopping_steps EARLY_STOPPING_STEPS]\n",
      "[--plot_loss [PLOT_LOSS]]\n",
      "[--include_effective_tokens_per_second [INCLUDE_EFFECTIVE_TOKENS_PER_SECOND]]\n",
      "[--do_sample [DO_SAMPLE]] [--no_do_sample]\n",
      "[--temperature TEMPERATURE] [--top_p TOP_P] [--top_k TOP_K]\n",
      "[--num_beams NUM_BEAMS] [--max_length MAX_LENGTH]\n",
      "[--max_new_tokens MAX_NEW_TOKENS]\n",
      "[--repetition_penalty REPETITION_PENALTY]\n",
      "[--length_penalty LENGTH_PENALTY]\n",
      "[--skip_special_tokens [SKIP_SPECIAL_TOKENS]]\n",
      "[--no_skip_special_tokens]\n",
      "\n",
      "options:\n",
      "-h, --help            show this help message and exit\n",
      "--model_name_or_path MODEL_NAME_OR_PATH, --model-name-or-path MODEL_NAME_OR_PATH\n",
      "Path to the model weight or identifier from\n",
      "huggingface.co/models or modelscope.cn/models.\n",
      "(default: None)\n",
      "--adapter_name_or_path ADAPTER_NAME_OR_PATH, --adapter-name-or-path ADAPTER_NAME_OR_PATH\n",
      "Path to the adapter weight or identifier from\n",
      "huggingface.co/models. Use commas to separate multiple\n",
      "adapters. (default: None)\n",
      "--adapter_folder ADAPTER_FOLDER, --adapter-folder ADAPTER_FOLDER\n",
      "The folder containing the adapter weights to load.\n",
      "(default: None)\n",
      "--cache_dir CACHE_DIR, --cache-dir CACHE_DIR\n",
      "Where to store the pre-trained models downloaded from\n",
      "huggingface.co or modelscope.cn. (default: None)\n",
      "--use_fast_tokenizer [USE_FAST_TOKENIZER], --use-fast-tokenizer [USE_FAST_TOKENIZER]\n",
      "Whether or not to use one of the fast tokenizer\n",
      "(backed by the tokenizers library). (default: True)\n",
      "--no_use_fast_tokenizer, --no-use-fast-tokenizer\n",
      "Whether or not to use one of the fast tokenizer\n",
      "(backed by the tokenizers library). (default: False)\n",
      "--resize_vocab [RESIZE_VOCAB], --resize-vocab [RESIZE_VOCAB]\n",
      "Whether or not to resize the tokenizer vocab and the\n",
      "embedding layers. (default: False)\n",
      "--split_special_tokens [SPLIT_SPECIAL_TOKENS], --split-special-tokens [SPLIT_SPECIAL_TOKENS]\n",
      "Whether or not the special tokens should be split\n",
      "during the tokenization process. (default: False)\n",
      "--add_tokens ADD_TOKENS, --add-tokens ADD_TOKENS\n",
      "Non-special tokens to be added into the tokenizer. Use\n",
      "commas to separate multiple tokens. (default: None)\n",
      "--add_special_tokens ADD_SPECIAL_TOKENS, --add-special-tokens ADD_SPECIAL_TOKENS\n",
      "Special tokens to be added into the tokenizer. Use\n",
      "commas to separate multiple tokens. (default: None)\n",
      "--new_special_tokens_config NEW_SPECIAL_TOKENS_CONFIG, --new-special-tokens-config NEW_SPECIAL_TOKENS_CONFIG\n",
      "Path to YAML config with special token descriptions\n",
      "for semantic initialization. If set, this takes\n",
      "precedence over add_special_tokens. YAML format:\n",
      "{'<token>': 'description text', ...} (default: None)\n",
      "--init_special_tokens {noise_init,desc_init,desc_init_w_noise}, --init-special-tokens {noise_init,desc_init,desc_init_w_noise}\n",
      "Initialization method for new special tokens:\n",
      "'noise_init' (default, random noise around mean),\n",
      "'desc_init' (semantic initialization from\n",
      "descriptions), 'desc_init_w_noise' (semantic + random\n",
      "noise). Note: 'desc_init' methods require\n",
      "new_special_tokens_config. (default: noise_init)\n",
      "--model_revision MODEL_REVISION, --model-revision MODEL_REVISION\n",
      "The specific model version to use (can be a branch\n",
      "name, tag name or commit id). (default: main)\n",
      "--low_cpu_mem_usage [LOW_CPU_MEM_USAGE], --low-cpu-mem-usage [LOW_CPU_MEM_USAGE]\n",
      "Whether or not to use memory-efficient model loading.\n",
      "(default: True)\n",
      "--no_low_cpu_mem_usage, --no-low-cpu-mem-usage\n",
      "Whether or not to use memory-efficient model loading.\n",
      "(default: False)\n",
      "--rope_scaling {linear,dynamic,yarn,llama3}, --rope-scaling {linear,dynamic,yarn,llama3}\n",
      "Which scaling strategy should be adopted for the RoPE\n",
      "embeddings. (default: None)\n",
      "--flash_attn {auto,disabled,sdpa,fa2}, --flash-attn {auto,disabled,sdpa,fa2}\n",
      "Enable FlashAttention for faster training and\n",
      "inference. (default: AttentionFunction.AUTO)\n",
      "--shift_attn [SHIFT_ATTN], --shift-attn [SHIFT_ATTN]\n",
      "Enable shift short attention (S^2-Attn) proposed by\n",
      "LongLoRA. (default: False)\n",
      "--mixture_of_depths {convert,load}, --mixture-of-depths {convert,load}\n",
      "Convert the model to mixture-of-depths (MoD) or load\n",
      "the MoD model. (default: None)\n",
      "--use_unsloth [USE_UNSLOTH], --use-unsloth [USE_UNSLOTH]\n",
      "Whether or not to use unsloth's optimization for the\n",
      "LoRA training. (default: False)\n",
      "--use_unsloth_gc [USE_UNSLOTH_GC], --use-unsloth-gc [USE_UNSLOTH_GC]\n",
      "Whether or not to use unsloth's gradient checkpointing\n",
      "(no need to install unsloth). (default: False)\n",
      "--enable_liger_kernel [ENABLE_LIGER_KERNEL], --enable-liger-kernel [ENABLE_LIGER_KERNEL]\n",
      "Whether or not to enable liger kernel for faster\n",
      "training. (default: False)\n",
      "--moe_aux_loss_coef MOE_AUX_LOSS_COEF, --moe-aux-loss-coef MOE_AUX_LOSS_COEF\n",
      "Coefficient of the auxiliary router loss in mixture-\n",
      "of-experts model. (default: None)\n",
      "--disable_gradient_checkpointing [DISABLE_GRADIENT_CHECKPOINTING], --disable-gradient-checkpointing [DISABLE_GRADIENT_CHECKPOINTING]\n",
      "Whether or not to disable gradient checkpointing.\n",
      "(default: False)\n",
      "--use_reentrant_gc [USE_REENTRANT_GC], --use-reentrant-gc [USE_REENTRANT_GC]\n",
      "Whether or not to use reentrant gradient\n",
      "checkpointing. (default: True)\n",
      "--no_use_reentrant_gc, --no-use-reentrant-gc\n",
      "Whether or not to use reentrant gradient\n",
      "checkpointing. (default: False)\n",
      "--upcast_layernorm [UPCAST_LAYERNORM], --upcast-layernorm [UPCAST_LAYERNORM]\n",
      "Whether or not to upcast the layernorm weights in\n",
      "fp32. (default: False)\n",
      "--upcast_lmhead_output [UPCAST_LMHEAD_OUTPUT], --upcast-lmhead-output [UPCAST_LMHEAD_OUTPUT]\n",
      "Whether or not to upcast the output of lm_head in\n",
      "fp32. (default: False)\n",
      "--train_from_scratch [TRAIN_FROM_SCRATCH], --train-from-scratch [TRAIN_FROM_SCRATCH]\n",
      "Whether or not to randomly initialize the model\n",
      "weights. (default: False)\n",
      "--infer_backend {huggingface,vllm,sglang,ktransformers}, --infer-backend {huggingface,vllm,sglang,ktransformers}\n",
      "Backend engine used at inference. (default:\n",
      "EngineName.HF)\n",
      "--offload_folder OFFLOAD_FOLDER, --offload-folder OFFLOAD_FOLDER\n",
      "Path to offload model weights. (default: offload)\n",
      "--use_kv_cache [USE_KV_CACHE], --use-kv-cache [USE_KV_CACHE]\n",
      "Whether or not to use KV cache in generation.\n",
      "(default: True)\n",
      "--no_use_kv_cache, --no-use-kv-cache\n",
      "Whether or not to use KV cache in generation.\n",
      "(default: False)\n",
      "--infer_dtype {auto,float16,bfloat16,float32}, --infer-dtype {auto,float16,bfloat16,float32}\n",
      "Data type for model weights and activations at\n",
      "inference. (default: auto)\n",
      "--hf_hub_token HF_HUB_TOKEN, --hf-hub-token HF_HUB_TOKEN\n",
      "Auth token to log in with Hugging Face Hub. (default:\n",
      "None)\n",
      "--ms_hub_token MS_HUB_TOKEN, --ms-hub-token MS_HUB_TOKEN\n",
      "Auth token to log in with ModelScope Hub. (default:\n",
      "None)\n",
      "--om_hub_token OM_HUB_TOKEN, --om-hub-token OM_HUB_TOKEN\n",
      "Auth token to log in with Modelers Hub. (default:\n",
      "None)\n",
      "--print_param_status [PRINT_PARAM_STATUS], --print-param-status [PRINT_PARAM_STATUS]\n",
      "For debugging purposes, print the status of the\n",
      "parameters in the model. (default: False)\n",
      "--trust_remote_code [TRUST_REMOTE_CODE], --trust-remote-code [TRUST_REMOTE_CODE]\n",
      "Whether to trust the execution of code from\n",
      "datasets/models defined on the Hub or not. (default:\n",
      "False)\n",
      "--quantization_method {bnb,gptq,awq,aqlm,quanto,eetq,hqq,mxfp4}, --quantization-method {bnb,gptq,awq,aqlm,quanto,eetq,hqq,mxfp4}\n",
      "Quantization method to use for on-the-fly\n",
      "quantization. (default: QuantizationMethod.BNB)\n",
      "--quantization_bit QUANTIZATION_BIT, --quantization-bit QUANTIZATION_BIT\n",
      "The number of bits to quantize the model using on-the-\n",
      "fly quantization. (default: None)\n",
      "--quantization_type {fp4,nf4}, --quantization-type {fp4,nf4}\n",
      "Quantization data type to use in bitsandbytes int4\n",
      "training. (default: nf4)\n",
      "--double_quantization [DOUBLE_QUANTIZATION], --double-quantization [DOUBLE_QUANTIZATION]\n",
      "Whether or not to use double quantization in\n",
      "bitsandbytes int4 training. (default: True)\n",
      "--no_double_quantization, --no-double-quantization\n",
      "Whether or not to use double quantization in\n",
      "bitsandbytes int4 training. (default: False)\n",
      "--quantization_device_map {auto}, --quantization-device-map {auto}\n",
      "Device map used to infer the 4-bit quantized model,\n",
      "needs bitsandbytes>=0.43.0. (default: None)\n",
      "--fp8 [FP8]           Enable FP8 mixed precision training via HuggingFace\n",
      "Accelerate. Requires PyTorch 2.7+ and Hopper\n",
      "architecture GPUs. (default: False)\n",
      "--fp8_backend FP8_BACKEND, --fp8-backend FP8_BACKEND\n",
      "FP8 backend to use ('auto', 'torchao', 'te', 'msamp').\n",
      "'auto' selects best available backend. (default: auto)\n",
      "--fp8_enable_fsdp_float8_all_gather [FP8_ENABLE_FSDP_FLOAT8_ALL_GATHER], --fp8-enable-fsdp-float8-all-gather [FP8_ENABLE_FSDP_FLOAT8_ALL_GATHER]\n",
      "Enable FP8 optimizations for FSDP2 all-gather\n",
      "operations. (default: False)\n",
      "--image_max_pixels IMAGE_MAX_PIXELS, --image-max-pixels IMAGE_MAX_PIXELS\n",
      "The maximum number of pixels of image inputs.\n",
      "(default: 589824)\n",
      "--image_min_pixels IMAGE_MIN_PIXELS, --image-min-pixels IMAGE_MIN_PIXELS\n",
      "The minimum number of pixels of image inputs.\n",
      "(default: 1024)\n",
      "--image_do_pan_and_scan [IMAGE_DO_PAN_AND_SCAN], --image-do-pan-and-scan [IMAGE_DO_PAN_AND_SCAN]\n",
      "Use pan and scan to process image for gemma3.\n",
      "(default: False)\n",
      "--crop_to_patches [CROP_TO_PATCHES], --crop-to-patches [CROP_TO_PATCHES]\n",
      "Whether to crop the image to patches for internvl.\n",
      "(default: False)\n",
      "--video_max_pixels VIDEO_MAX_PIXELS, --video-max-pixels VIDEO_MAX_PIXELS\n",
      "The maximum number of pixels of video inputs.\n",
      "(default: 65536)\n",
      "--video_min_pixels VIDEO_MIN_PIXELS, --video-min-pixels VIDEO_MIN_PIXELS\n",
      "The minimum number of pixels of video inputs.\n",
      "(default: 256)\n",
      "--video_fps VIDEO_FPS, --video-fps VIDEO_FPS\n",
      "The frames to sample per second for video inputs.\n",
      "(default: 2.0)\n",
      "--video_maxlen VIDEO_MAXLEN, --video-maxlen VIDEO_MAXLEN\n",
      "The maximum number of sampled frames for video inputs.\n",
      "(default: 128)\n",
      "--use_audio_in_video [USE_AUDIO_IN_VIDEO], --use-audio-in-video [USE_AUDIO_IN_VIDEO]\n",
      "Whether or not to use audio in video inputs. (default:\n",
      "False)\n",
      "--audio_sampling_rate AUDIO_SAMPLING_RATE, --audio-sampling-rate AUDIO_SAMPLING_RATE\n",
      "The sampling rate of audio inputs. (default: 16000)\n",
      "--export_dir EXPORT_DIR, --export-dir EXPORT_DIR\n",
      "Path to the directory to save the exported model.\n",
      "(default: None)\n",
      "--export_size EXPORT_SIZE, --export-size EXPORT_SIZE\n",
      "The file shard size (in GB) of the exported model.\n",
      "(default: 5)\n",
      "--export_device {cpu,auto}, --export-device {cpu,auto}\n",
      "The device used in model export, use `auto` to\n",
      "accelerate exporting. (default: cpu)\n",
      "--export_quantization_bit EXPORT_QUANTIZATION_BIT, --export-quantization-bit EXPORT_QUANTIZATION_BIT\n",
      "The number of bits to quantize the exported model.\n",
      "(default: None)\n",
      "--export_quantization_dataset EXPORT_QUANTIZATION_DATASET, --export-quantization-dataset EXPORT_QUANTIZATION_DATASET\n",
      "Path to the dataset or dataset name to use in\n",
      "quantizing the exported model. (default: None)\n",
      "--export_quantization_nsamples EXPORT_QUANTIZATION_NSAMPLES, --export-quantization-nsamples EXPORT_QUANTIZATION_NSAMPLES\n",
      "The number of samples used for quantization. (default:\n",
      "128)\n",
      "--export_quantization_maxlen EXPORT_QUANTIZATION_MAXLEN, --export-quantization-maxlen EXPORT_QUANTIZATION_MAXLEN\n",
      "The maximum length of the model inputs used for\n",
      "quantization. (default: 1024)\n",
      "--export_legacy_format [EXPORT_LEGACY_FORMAT], --export-legacy-format [EXPORT_LEGACY_FORMAT]\n",
      "Whether or not to save the `.bin` files instead of\n",
      "`.safetensors`. (default: False)\n",
      "--export_hub_model_id EXPORT_HUB_MODEL_ID, --export-hub-model-id EXPORT_HUB_MODEL_ID\n",
      "The name of the repository if push the model to the\n",
      "Hugging Face hub. (default: None)\n",
      "--use_kt [USE_KT], --use-kt [USE_KT]\n",
      "Whether To Use KTransformers Optimizations For LoRA\n",
      "Training. (default: False)\n",
      "--kt_optimize_rule KT_OPTIMIZE_RULE, --kt-optimize-rule KT_OPTIMIZE_RULE\n",
      "Path To The KTransformers Optimize Rule; See\n",
      "https://github.com/kvcache-ai/ktransformers/.\n",
      "(default: None)\n",
      "--cpu_infer CPU_INFER, --cpu-infer CPU_INFER\n",
      "Number Of CPU Cores Used For Computation. (default:\n",
      "32)\n",
      "--chunk_size CHUNK_SIZE, --chunk-size CHUNK_SIZE\n",
      "Chunk Size Used For CPU Compute In KTransformers.\n",
      "(default: 8192)\n",
      "--mode MODE           Normal Or Long_Context For Llama Models. (default:\n",
      "normal)\n",
      "--kt_maxlen KT_MAXLEN, --kt-maxlen KT_MAXLEN\n",
      "Maximum Sequence (Prompt + Response) Length Of The KT\n",
      "Engine. (default: 4096)\n",
      "--kt_use_cuda_graph [KT_USE_CUDA_GRAPH], --kt-use-cuda-graph [KT_USE_CUDA_GRAPH]\n",
      "Whether To Use CUDA Graphs For The KT Engine.\n",
      "(default: True)\n",
      "--no_kt_use_cuda_graph, --no-kt-use-cuda-graph\n",
      "Whether To Use CUDA Graphs For The KT Engine.\n",
      "(default: False)\n",
      "--kt_mode KT_MODE, --kt-mode KT_MODE\n",
      "Normal Or Long_Context Mode For The KT Engine.\n",
      "(default: normal)\n",
      "--kt_force_think [KT_FORCE_THINK], --kt-force-think [KT_FORCE_THINK]\n",
      "Force-Think Toggle For The KT Engine. (default: False)\n",
      "--vllm_maxlen VLLM_MAXLEN, --vllm-maxlen VLLM_MAXLEN\n",
      "Maximum sequence (prompt + response) length of the\n",
      "vLLM engine. (default: 4096)\n",
      "--vllm_gpu_util VLLM_GPU_UTIL, --vllm-gpu-util VLLM_GPU_UTIL\n",
      "The fraction of GPU memory in (0,1) to be used for the\n",
      "vLLM engine. (default: 0.7)\n",
      "--vllm_enforce_eager [VLLM_ENFORCE_EAGER], --vllm-enforce-eager [VLLM_ENFORCE_EAGER]\n",
      "Whether or not to disable CUDA graph in the vLLM\n",
      "engine. (default: False)\n",
      "--vllm_max_lora_rank VLLM_MAX_LORA_RANK, --vllm-max-lora-rank VLLM_MAX_LORA_RANK\n",
      "Maximum rank of all LoRAs in the vLLM engine.\n",
      "(default: 32)\n",
      "--vllm_config VLLM_CONFIG, --vllm-config VLLM_CONFIG\n",
      "Config to initialize the vllm engine. Please use JSON\n",
      "strings. (default: None)\n",
      "--sglang_maxlen SGLANG_MAXLEN, --sglang-maxlen SGLANG_MAXLEN\n",
      "Maximum sequence (prompt + response) length of the\n",
      "SGLang engine. (default: 4096)\n",
      "--sglang_mem_fraction SGLANG_MEM_FRACTION, --sglang-mem-fraction SGLANG_MEM_FRACTION\n",
      "The memory fraction (0-1) to be used for the SGLang\n",
      "engine. (default: 0.7)\n",
      "--sglang_tp_size SGLANG_TP_SIZE, --sglang-tp-size SGLANG_TP_SIZE\n",
      "Tensor parallel size for the SGLang engine. (default:\n",
      "-1)\n",
      "--sglang_config SGLANG_CONFIG, --sglang-config SGLANG_CONFIG\n",
      "Config to initialize the SGLang engine. Please use\n",
      "JSON strings. (default: None)\n",
      "--sglang_lora_backend {triton,flashinfer}, --sglang-lora-backend {triton,flashinfer}\n",
      "The backend of running GEMM kernels for Lora modules.\n",
      "Recommend using the Triton LoRA backend for better\n",
      "performance and stability. (default: triton)\n",
      "--template TEMPLATE   Which template to use for constructing prompts in\n",
      "training and inference. (default: None)\n",
      "--dataset DATASET     The name of dataset(s) to use for training. Use commas\n",
      "to separate multiple datasets. (default: None)\n",
      "--eval_dataset EVAL_DATASET, --eval-dataset EVAL_DATASET\n",
      "The name of dataset(s) to use for evaluation. Use\n",
      "commas to separate multiple datasets. (default: None)\n",
      "--dataset_dir DATASET_DIR, --dataset-dir DATASET_DIR\n",
      "Path to the folder containing the datasets. (default:\n",
      "data)\n",
      "--media_dir MEDIA_DIR, --media-dir MEDIA_DIR\n",
      "Path to the folder containing the images, videos or\n",
      "audios. Defaults to `dataset_dir`. (default: None)\n",
      "--cutoff_len CUTOFF_LEN, --cutoff-len CUTOFF_LEN\n",
      "The cutoff length of the tokenized inputs in the\n",
      "dataset. (default: 2048)\n",
      "--train_on_prompt [TRAIN_ON_PROMPT], --train-on-prompt [TRAIN_ON_PROMPT]\n",
      "Whether or not to disable the mask on the prompt.\n",
      "(default: False)\n",
      "--mask_history [MASK_HISTORY], --mask-history [MASK_HISTORY]\n",
      "Whether or not to mask the history and train on the\n",
      "last turn only. (default: False)\n",
      "--streaming [STREAMING]\n",
      "Enable dataset streaming. (default: False)\n",
      "--buffer_size BUFFER_SIZE, --buffer-size BUFFER_SIZE\n",
      "Size of the buffer to randomly sample examples from in\n",
      "dataset streaming. (default: 16384)\n",
      "--mix_strategy {concat,interleave_under,interleave_over}, --mix-strategy {concat,interleave_under,interleave_over}\n",
      "Strategy to use in dataset mixing (concat/interleave)\n",
      "(undersampling/oversampling). (default: concat)\n",
      "--interleave_probs INTERLEAVE_PROBS, --interleave-probs INTERLEAVE_PROBS\n",
      "Probabilities to sample data from datasets. Use commas\n",
      "to separate multiple datasets. (default: None)\n",
      "--overwrite_cache [OVERWRITE_CACHE], --overwrite-cache [OVERWRITE_CACHE]\n",
      "Overwrite the cached training and evaluation sets.\n",
      "(default: False)\n",
      "--preprocessing_batch_size PREPROCESSING_BATCH_SIZE, --preprocessing-batch-size PREPROCESSING_BATCH_SIZE\n",
      "The number of examples in one group in pre-processing.\n",
      "(default: 1000)\n",
      "--preprocessing_num_workers PREPROCESSING_NUM_WORKERS, --preprocessing-num-workers PREPROCESSING_NUM_WORKERS\n",
      "The number of processes to use for the pre-processing.\n",
      "(default: None)\n",
      "--max_samples MAX_SAMPLES, --max-samples MAX_SAMPLES\n",
      "For debugging purposes, truncate the number of\n",
      "examples for each dataset. (default: None)\n",
      "--eval_num_beams EVAL_NUM_BEAMS, --eval-num-beams EVAL_NUM_BEAMS\n",
      "Number of beams to use for evaluation. This argument\n",
      "will be passed to `model.generate` (default: None)\n",
      "--ignore_pad_token_for_loss [IGNORE_PAD_TOKEN_FOR_LOSS], --ignore-pad-token-for-loss [IGNORE_PAD_TOKEN_FOR_LOSS]\n",
      "Whether or not to ignore the tokens corresponding to\n",
      "the pad label in loss computation. (default: True)\n",
      "--no_ignore_pad_token_for_loss, --no-ignore-pad-token-for-loss\n",
      "Whether or not to ignore the tokens corresponding to\n",
      "the pad label in loss computation. (default: False)\n",
      "--val_size VAL_SIZE, --val-size VAL_SIZE\n",
      "Size of the validation set, should be an integer or a\n",
      "float in range `[0,1)`. (default: 0.0)\n",
      "--eval_on_each_dataset [EVAL_ON_EACH_DATASET], --eval-on-each-dataset [EVAL_ON_EACH_DATASET]\n",
      "Whether or not to evaluate on each dataset separately.\n",
      "(default: False)\n",
      "--packing PACKING     Enable sequences packing in training. Will\n",
      "automatically enable in pre-training. (default: None)\n",
      "--neat_packing [NEAT_PACKING], --neat-packing [NEAT_PACKING]\n",
      "Enable sequence packing without cross-attention.\n",
      "(default: False)\n",
      "--tool_format TOOL_FORMAT, --tool-format TOOL_FORMAT\n",
      "Tool format to use for constructing function calling\n",
      "examples. (default: None)\n",
      "--default_system DEFAULT_SYSTEM, --default-system DEFAULT_SYSTEM\n",
      "Override the default system message in the template.\n",
      "(default: None)\n",
      "--enable_thinking [ENABLE_THINKING], --enable-thinking [ENABLE_THINKING]\n",
      "Whether or not to enable thinking mode for reasoning\n",
      "models. (default: True)\n",
      "--no_enable_thinking, --no-enable-thinking\n",
      "Whether or not to enable thinking mode for reasoning\n",
      "models. (default: False)\n",
      "--tokenized_path TOKENIZED_PATH, --tokenized-path TOKENIZED_PATH\n",
      "Path to save or load the tokenized datasets. If\n",
      "tokenized_path not exists, it will save the tokenized\n",
      "datasets. If tokenized_path exists, it will load the\n",
      "tokenized datasets. (default: None)\n",
      "--data_shared_file_system [DATA_SHARED_FILE_SYSTEM], --data-shared-file-system [DATA_SHARED_FILE_SYSTEM]\n",
      "Whether or not to use a shared file system for the\n",
      "datasets. (default: False)\n",
      "--output_dir OUTPUT_DIR, --output-dir OUTPUT_DIR\n",
      "The output directory where the model predictions and\n",
      "checkpoints will be written. Defaults to\n",
      "'trainer_output' if not provided. (default: None)\n",
      "--overwrite_output_dir [OVERWRITE_OUTPUT_DIR], --overwrite-output-dir [OVERWRITE_OUTPUT_DIR]\n",
      "deprecated (default: False)\n",
      "--do_train [DO_TRAIN], --do-train [DO_TRAIN]\n",
      "Whether to run training. (default: False)\n",
      "--do_eval [DO_EVAL], --do-eval [DO_EVAL]\n",
      "Whether to run eval on the dev set. (default: False)\n",
      "--do_predict [DO_PREDICT], --do-predict [DO_PREDICT]\n",
      "Whether to run predictions on the test set. (default:\n",
      "False)\n",
      "--eval_strategy {no,steps,epoch}, --eval-strategy {no,steps,epoch}\n",
      "The evaluation strategy to use. (default: no)\n",
      "--prediction_loss_only [PREDICTION_LOSS_ONLY], --prediction-loss-only [PREDICTION_LOSS_ONLY]\n",
      "When performing evaluation and predictions, only\n",
      "returns the loss. (default: False)\n",
      "--per_device_train_batch_size PER_DEVICE_TRAIN_BATCH_SIZE, --per-device-train-batch-size PER_DEVICE_TRAIN_BATCH_SIZE\n",
      "Batch size per device accelerator core/CPU for\n",
      "training. (default: 8)\n",
      "--per_device_eval_batch_size PER_DEVICE_EVAL_BATCH_SIZE, --per-device-eval-batch-size PER_DEVICE_EVAL_BATCH_SIZE\n",
      "Batch size per device accelerator core/CPU for\n",
      "evaluation. (default: 8)\n",
      "--per_gpu_train_batch_size PER_GPU_TRAIN_BATCH_SIZE, --per-gpu-train-batch-size PER_GPU_TRAIN_BATCH_SIZE\n",
      "Deprecated, the use of `--per_device_train_batch_size`\n",
      "is preferred. Batch size per GPU/TPU core/CPU for\n",
      "training. (default: None)\n",
      "--per_gpu_eval_batch_size PER_GPU_EVAL_BATCH_SIZE, --per-gpu-eval-batch-size PER_GPU_EVAL_BATCH_SIZE\n",
      "Deprecated, the use of `--per_device_eval_batch_size`\n",
      "is preferred. Batch size per GPU/TPU core/CPU for\n",
      "evaluation. (default: None)\n",
      "--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS, --gradient-accumulation-steps GRADIENT_ACCUMULATION_STEPS\n",
      "Number of updates steps to accumulate before\n",
      "performing a backward/update pass. (default: 1)\n",
      "--eval_accumulation_steps EVAL_ACCUMULATION_STEPS, --eval-accumulation-steps EVAL_ACCUMULATION_STEPS\n",
      "Number of predictions steps to accumulate before\n",
      "moving the tensors to the CPU. (default: None)\n",
      "--eval_delay EVAL_DELAY, --eval-delay EVAL_DELAY\n",
      "Number of epochs or steps to wait for before the first\n",
      "evaluation can be performed, depending on the\n",
      "eval_strategy. (default: 0)\n",
      "--torch_empty_cache_steps TORCH_EMPTY_CACHE_STEPS, --torch-empty-cache-steps TORCH_EMPTY_CACHE_STEPS\n",
      "Number of steps to wait before calling\n",
      "`torch.<device>.empty_cache()`.This can help avoid\n",
      "CUDA out-of-memory errors by lowering peak VRAM usage\n",
      "at a cost of about [10{'option_strings': ['--\n",
      "torch_empty_cache_steps', '--torch-empty-cache-\n",
      "steps'], 'dest': 'torch_empty_cache_steps', 'nargs':\n",
      "None, 'const': None, 'default': None, 'type': 'int',\n",
      "'choices': None, 'required': False, 'help': 'Number of\n",
      "steps to wait before calling\n",
      "`torch.<device>.empty_cache()`.This can help avoid\n",
      "CUDA out-of-memory errors by lowering peak VRAM usage\n",
      "at a cost of about [10% slower performance](https://gi\n",
      "thub.com/huggingface/transformers/issues/31372).If\n",
      "left unset or set to None, cache will not be\n",
      "emptied.', 'metavar': None, 'container':\n",
      "<argparse._ArgumentGroup object at\n",
      "0x000001DDBAD5E650>, 'prog': 'launcher.py'}lower perfo\n",
      "rmance](https://github.com/huggingface/transformers/is\n",
      "sues/31372).If left unset or set to None, cache will\n",
      "not be emptied. (default: None)\n",
      "--learning_rate LEARNING_RATE, --learning-rate LEARNING_RATE\n",
      "The initial learning rate for AdamW. (default: 5e-05)\n",
      "--weight_decay WEIGHT_DECAY, --weight-decay WEIGHT_DECAY\n",
      "Weight decay for AdamW if we apply some. (default:\n",
      "0.0)\n",
      "--adam_beta1 ADAM_BETA1, --adam-beta1 ADAM_BETA1\n",
      "Beta1 for AdamW optimizer (default: 0.9)\n",
      "--adam_beta2 ADAM_BETA2, --adam-beta2 ADAM_BETA2\n",
      "Beta2 for AdamW optimizer (default: 0.999)\n",
      "--adam_epsilon ADAM_EPSILON, --adam-epsilon ADAM_EPSILON\n",
      "Epsilon for AdamW optimizer. (default: 1e-08)\n",
      "--max_grad_norm MAX_GRAD_NORM, --max-grad-norm MAX_GRAD_NORM\n",
      "Max gradient norm. (default: 1.0)\n",
      "--num_train_epochs NUM_TRAIN_EPOCHS, --num-train-epochs NUM_TRAIN_EPOCHS\n",
      "Total number of training epochs to perform. (default:\n",
      "3.0)\n",
      "--max_steps MAX_STEPS, --max-steps MAX_STEPS\n",
      "If > 0: set total number of training steps to perform.\n",
      "Override num_train_epochs. (default: -1)\n",
      "--lr_scheduler_type {linear,cosine,cosine_with_restarts,polynomial,constant,constant_with_warmup,inverse_sqrt,reduce_lr_on_plateau,cosine_with_min_lr,cosine_warmup_with_min_lr,warmup_stable_decay}, --lr-scheduler-type {linear,cosine,cosine_with_restarts,polynomial,constant,constant_with_warmup,inverse_sqrt,reduce_lr_on_plateau,cosine_with_min_lr,cosine_warmup_with_min_lr,warmup_stable_decay}\n",
      "The scheduler type to use. (default: linear)\n",
      "--lr_scheduler_kwargs LR_SCHEDULER_KWARGS, --lr-scheduler-kwargs LR_SCHEDULER_KWARGS\n",
      "Extra parameters for the lr_scheduler such as\n",
      "{'num_cycles': 1} for the cosine with hard restarts.\n",
      "(default: {})\n",
      "--warmup_ratio WARMUP_RATIO, --warmup-ratio WARMUP_RATIO\n",
      "Linear warmup over warmup_ratio fraction of total\n",
      "steps. (default: 0.0)\n",
      "--warmup_steps WARMUP_STEPS, --warmup-steps WARMUP_STEPS\n",
      "Linear warmup over warmup_steps. (default: 0)\n",
      "--log_level {detail,debug,info,warning,error,critical,passive}, --log-level {detail,debug,info,warning,error,critical,passive}\n",
      "Logger log level to use on the main node. Possible\n",
      "choices are the log levels as strings: 'debug',\n",
      "'info', 'warning', 'error' and 'critical', plus a\n",
      "'passive' level which doesn't set anything and lets\n",
      "the application set the level. Defaults to 'passive'.\n",
      "(default: passive)\n",
      "--log_level_replica {detail,debug,info,warning,error,critical,passive}, --log-level-replica {detail,debug,info,warning,error,critical,passive}\n",
      "Logger log level to use on replica nodes. Same choices\n",
      "and defaults as ``log_level`` (default: warning)\n",
      "--log_on_each_node [LOG_ON_EACH_NODE], --log-on-each-node [LOG_ON_EACH_NODE]\n",
      "When doing a multinode distributed training, whether\n",
      "to log once per node or just once on the main node.\n",
      "(default: True)\n",
      "--no_log_on_each_node, --no-log-on-each-node\n",
      "When doing a multinode distributed training, whether\n",
      "to log once per node or just once on the main node.\n",
      "(default: False)\n",
      "--logging_dir LOGGING_DIR, --logging-dir LOGGING_DIR\n",
      "Tensorboard log dir. (default: None)\n",
      "--logging_strategy {no,steps,epoch}, --logging-strategy {no,steps,epoch}\n",
      "The logging strategy to use. (default: steps)\n",
      "--logging_first_step [LOGGING_FIRST_STEP], --logging-first-step [LOGGING_FIRST_STEP]\n",
      "Log the first global_step (default: False)\n",
      "--logging_steps LOGGING_STEPS, --logging-steps LOGGING_STEPS\n",
      "Log every X updates steps. Should be an integer or a\n",
      "float in range `[0,1)`. If smaller than 1, will be\n",
      "interpreted as ratio of total training steps.\n",
      "(default: 500)\n",
      "--logging_nan_inf_filter [LOGGING_NAN_INF_FILTER], --logging-nan-inf-filter [LOGGING_NAN_INF_FILTER]\n",
      "Filter nan and inf losses for logging. (default: True)\n",
      "--no_logging_nan_inf_filter, --no-logging-nan-inf-filter\n",
      "Filter nan and inf losses for logging. (default:\n",
      "False)\n",
      "--save_strategy {no,steps,epoch,best}, --save-strategy {no,steps,epoch,best}\n",
      "The checkpoint save strategy to use. (default: steps)\n",
      "--save_steps SAVE_STEPS, --save-steps SAVE_STEPS\n",
      "Save checkpoint every X updates steps. Should be an\n",
      "integer or a float in range `[0,1)`. If smaller than\n",
      "1, will be interpreted as ratio of total training\n",
      "steps. (default: 500)\n",
      "--save_total_limit SAVE_TOTAL_LIMIT, --save-total-limit SAVE_TOTAL_LIMIT\n",
      "If a value is passed, will limit the total amount of\n",
      "checkpoints. Deletes the older checkpoints in\n",
      "`output_dir`. When `load_best_model_at_end` is\n",
      "enabled, the 'best' checkpoint according to\n",
      "`metric_for_best_model` will always be retained in\n",
      "addition to the most recent ones. For example, for\n",
      "`save_total_limit=5` and\n",
      "`load_best_model_at_end=True`, the four last\n",
      "checkpoints will always be retained alongside the best\n",
      "model. When `save_total_limit=1` and\n",
      "`load_best_model_at_end=True`, it is possible that two\n",
      "checkpoints are saved: the last one and the best one\n",
      "(if they are different). Default is unlimited\n",
      "checkpoints (default: None)\n",
      "--save_safetensors [SAVE_SAFETENSORS], --save-safetensors [SAVE_SAFETENSORS]\n",
      "Use safetensors saving and loading for state dicts\n",
      "instead of default torch.load and torch.save.\n",
      "(default: True)\n",
      "--no_save_safetensors, --no-save-safetensors\n",
      "Use safetensors saving and loading for state dicts\n",
      "instead of default torch.load and torch.save.\n",
      "(default: False)\n",
      "--save_on_each_node [SAVE_ON_EACH_NODE], --save-on-each-node [SAVE_ON_EACH_NODE]\n",
      "When doing multi-node distributed training, whether to\n",
      "save models and checkpoints on each node, or only on\n",
      "the main one (default: False)\n",
      "--save_only_model [SAVE_ONLY_MODEL], --save-only-model [SAVE_ONLY_MODEL]\n",
      "When checkpointing, whether to only save the model, or\n",
      "also the optimizer, scheduler & rng state.Note that\n",
      "when this is true, you won't be able to resume\n",
      "training from checkpoint.This enables you to save\n",
      "storage by not storing the optimizer, scheduler & rng\n",
      "state.You can only load the model using\n",
      "from_pretrained with this option set to True.\n",
      "(default: False)\n",
      "--restore_callback_states_from_checkpoint [RESTORE_CALLBACK_STATES_FROM_CHECKPOINT], --restore-callback-states-from-checkpoint [RESTORE_CALLBACK_STATES_FROM_CHECKPOINT]\n",
      "Whether to restore the callback states from the\n",
      "checkpoint. If `True`, will override callbacks passed\n",
      "to the `Trainer` if they exist in the checkpoint.\n",
      "(default: False)\n",
      "--no_cuda [NO_CUDA], --no-cuda [NO_CUDA]\n",
      "This argument is deprecated. It will be removed in\n",
      "version 5.0 of ü§ó Transformers. (default: False)\n",
      "--use_cpu [USE_CPU], --use-cpu [USE_CPU]\n",
      "Whether or not to use cpu. If left to False, we will\n",
      "use the available torch device/backend\n",
      "(cuda/mps/xpu/hpu etc.) (default: False)\n",
      "--use_mps_device [USE_MPS_DEVICE], --use-mps-device [USE_MPS_DEVICE]\n",
      "This argument is deprecated. `mps` device will be used\n",
      "if available similar to `cuda` device. It will be\n",
      "removed in version 5.0 of ü§ó Transformers (default:\n",
      "False)\n",
      "--seed SEED           Random seed that will be set at the beginning of\n",
      "training. (default: 42)\n",
      "--data_seed DATA_SEED, --data-seed DATA_SEED\n",
      "Random seed to be used with data samplers. (default:\n",
      "None)\n",
      "--jit_mode_eval [JIT_MODE_EVAL], --jit-mode-eval [JIT_MODE_EVAL]\n",
      "Whether or not to use PyTorch jit trace for inference\n",
      "(default: False)\n",
      "--bf16 [BF16]         Whether to use bf16 (mixed) precision instead of\n",
      "32-bit. Requires Ampere or higher NVIDIA architecture\n",
      "or using CPU (use_cpu) or Ascend NPU. This is an\n",
      "experimental API and it may change. (default: False)\n",
      "--fp16 [FP16]         Whether to use fp16 (mixed) precision instead of\n",
      "32-bit (default: False)\n",
      "--fp16_opt_level FP16_OPT_LEVEL, --fp16-opt-level FP16_OPT_LEVEL\n",
      "For fp16: Apex AMP optimization level selected in\n",
      "['O0', 'O1', 'O2', and 'O3']. See details at\n",
      "https://nvidia.github.io/apex/amp.html (default: O1)\n",
      "--half_precision_backend {auto,apex,cpu_amp}, --half-precision-backend {auto,apex,cpu_amp}\n",
      "The backend to be used for half precision. (default:\n",
      "auto)\n",
      "--bf16_full_eval [BF16_FULL_EVAL], --bf16-full-eval [BF16_FULL_EVAL]\n",
      "Whether to use full bfloat16 evaluation instead of\n",
      "32-bit. This is an experimental API and it may change.\n",
      "(default: False)\n",
      "--fp16_full_eval [FP16_FULL_EVAL], --fp16-full-eval [FP16_FULL_EVAL]\n",
      "Whether to use full float16 evaluation instead of\n",
      "32-bit (default: False)\n",
      "--tf32 TF32           Whether to enable tf32 mode, available in Ampere and\n",
      "newer GPU architectures. This is an experimental API\n",
      "and it may change. (default: None)\n",
      "--local_rank LOCAL_RANK, --local-rank LOCAL_RANK\n",
      "For distributed training: local_rank (default: -1)\n",
      "--ddp_backend {nccl,gloo,mpi,ccl,hccl,cncl,mccl}, --ddp-backend {nccl,gloo,mpi,ccl,hccl,cncl,mccl}\n",
      "The backend to be used for distributed training\n",
      "(default: None)\n",
      "--tpu_num_cores TPU_NUM_CORES, --tpu-num-cores TPU_NUM_CORES\n",
      "TPU: Number of TPU cores (automatically passed by\n",
      "launcher script) (default: None)\n",
      "--tpu_metrics_debug [TPU_METRICS_DEBUG], --tpu-metrics-debug [TPU_METRICS_DEBUG]\n",
      "Deprecated, the use of `--debug tpu_metrics_debug` is\n",
      "preferred. TPU: Whether to print debug metrics\n",
      "(default: False)\n",
      "--debug DEBUG [DEBUG ...]\n",
      "Whether or not to enable debug mode. Current options:\n",
      "`underflow_overflow` (Detect underflow and overflow in\n",
      "activations and weights), `tpu_metrics_debug` (print\n",
      "debug metrics on TPU). (default: None)\n",
      "--dataloader_drop_last [DATALOADER_DROP_LAST], --dataloader-drop-last [DATALOADER_DROP_LAST]\n",
      "Drop the last incomplete batch if it is not divisible\n",
      "by the batch size. (default: False)\n",
      "--eval_steps EVAL_STEPS, --eval-steps EVAL_STEPS\n",
      "Run an evaluation every X steps. Should be an integer\n",
      "or a float in range `[0,1)`. If smaller than 1, will\n",
      "be interpreted as ratio of total training steps.\n",
      "(default: None)\n",
      "--dataloader_num_workers DATALOADER_NUM_WORKERS, --dataloader-num-workers DATALOADER_NUM_WORKERS\n",
      "Number of subprocesses to use for data loading\n",
      "(PyTorch only). 0 means that the data will be loaded\n",
      "in the main process. (default: 0)\n",
      "--dataloader_prefetch_factor DATALOADER_PREFETCH_FACTOR, --dataloader-prefetch-factor DATALOADER_PREFETCH_FACTOR\n",
      "Number of batches loaded in advance by each worker. 2\n",
      "means there will be a total of 2 * num_workers batches\n",
      "prefetched across all workers. (default: None)\n",
      "--past_index PAST_INDEX, --past-index PAST_INDEX\n",
      "If >=0, uses the corresponding part of the output as\n",
      "the past state for next step. (default: -1)\n",
      "--run_name RUN_NAME, --run-name RUN_NAME\n",
      "An optional descriptor for the run. Notably used for\n",
      "trackio, wandb, mlflow comet and swanlab logging.\n",
      "(default: None)\n",
      "--disable_tqdm DISABLE_TQDM, --disable-tqdm DISABLE_TQDM\n",
      "Whether or not to disable the tqdm progress bars.\n",
      "(default: None)\n",
      "--remove_unused_columns [REMOVE_UNUSED_COLUMNS], --remove-unused-columns [REMOVE_UNUSED_COLUMNS]\n",
      "Remove columns not required by the model when using an\n",
      "nlp.Dataset. (default: True)\n",
      "--no_remove_unused_columns, --no-remove-unused-columns\n",
      "Remove columns not required by the model when using an\n",
      "nlp.Dataset. (default: False)\n",
      "--label_names LABEL_NAMES [LABEL_NAMES ...], --label-names LABEL_NAMES [LABEL_NAMES ...]\n",
      "The list of keys in your dictionary of inputs that\n",
      "correspond to the labels. (default: None)\n",
      "--load_best_model_at_end [LOAD_BEST_MODEL_AT_END], --load-best-model-at-end [LOAD_BEST_MODEL_AT_END]\n",
      "Whether or not to load the best model found during\n",
      "training at the end of training. When this option is\n",
      "enabled, the best checkpoint will always be saved. See\n",
      "`save_total_limit` for more. (default: False)\n",
      "--metric_for_best_model METRIC_FOR_BEST_MODEL, --metric-for-best-model METRIC_FOR_BEST_MODEL\n",
      "The metric to use to compare two different models.\n",
      "(default: None)\n",
      "--greater_is_better GREATER_IS_BETTER, --greater-is-better GREATER_IS_BETTER\n",
      "Whether the `metric_for_best_model` should be\n",
      "maximized or not. (default: None)\n",
      "--ignore_data_skip [IGNORE_DATA_SKIP], --ignore-data-skip [IGNORE_DATA_SKIP]\n",
      "When resuming training, whether or not to skip the\n",
      "first epochs and batches to get to the same training\n",
      "data. (default: False)\n",
      "--fsdp FSDP           Whether or not to use PyTorch Fully Sharded Data\n",
      "Parallel (FSDP) training (in distributed training\n",
      "only). The base option should be `full_shard`,\n",
      "`shard_grad_op` or `no_shard` and you can add CPU-\n",
      "offload to `full_shard` or `shard_grad_op` like this:\n",
      "full_shard offload` or `shard_grad_op offload`. You\n",
      "can add auto-wrap to `full_shard` or `shard_grad_op`\n",
      "with the same syntax: full_shard auto_wrap` or\n",
      "`shard_grad_op auto_wrap`. (default: None)\n",
      "--fsdp_min_num_params FSDP_MIN_NUM_PARAMS, --fsdp-min-num-params FSDP_MIN_NUM_PARAMS\n",
      "This parameter is deprecated. FSDP's minimum number of\n",
      "parameters for Default Auto Wrapping. (useful only\n",
      "when `fsdp` field is passed). (default: 0)\n",
      "--fsdp_config FSDP_CONFIG, --fsdp-config FSDP_CONFIG\n",
      "Config to be used with FSDP (Pytorch Fully Sharded\n",
      "Data Parallel). The value is either a fsdp json config\n",
      "file (e.g., `fsdp_config.json`) or an already loaded\n",
      "json file as `dict`. (default: None)\n",
      "--fsdp_transformer_layer_cls_to_wrap FSDP_TRANSFORMER_LAYER_CLS_TO_WRAP, --fsdp-transformer-layer-cls-to-wrap FSDP_TRANSFORMER_LAYER_CLS_TO_WRAP\n",
      "This parameter is deprecated. Transformer layer class\n",
      "name (case-sensitive) to wrap, e.g, `BertLayer`,\n",
      "`GPTJBlock`, `T5Block` .... (useful only when `fsdp`\n",
      "flag is passed). (default: None)\n",
      "--accelerator_config ACCELERATOR_CONFIG, --accelerator-config ACCELERATOR_CONFIG\n",
      "Config to be used with the internal Accelerator object\n",
      "initialization. The value is either a accelerator json\n",
      "config file (e.g., `accelerator_config.json`) or an\n",
      "already loaded json file as `dict`. (default: None)\n",
      "--parallelism_config PARALLELISM_CONFIG, --parallelism-config PARALLELISM_CONFIG\n",
      "Parallelism configuration for the training run.\n",
      "Requires Accelerate `1.10.1` (default: None)\n",
      "--deepspeed DEEPSPEED\n",
      "Enable deepspeed and pass the path to deepspeed json\n",
      "config file (e.g. `ds_config.json`) or an already\n",
      "loaded json file as a dict (default: None)\n",
      "--label_smoothing_factor LABEL_SMOOTHING_FACTOR, --label-smoothing-factor LABEL_SMOOTHING_FACTOR\n",
      "The label smoothing epsilon to apply (zero means no\n",
      "label smoothing). (default: 0.0)\n",
      "--optim {adamw_torch,adamw_torch_fused,adamw_torch_xla,adamw_torch_npu_fused,adamw_apex_fused,adafactor,adamw_anyprecision,adamw_torch_4bit,adamw_torch_8bit,ademamix,sgd,adagrad,adamw_bnb_8bit,adamw_8bit,ademamix_8bit,lion_8bit,lion_32bit,paged_adamw_32bit,paged_adamw_8bit,paged_ademamix_32bit,paged_ademamix_8bit,paged_lion_32bit,paged_lion_8bit,rmsprop,rmsprop_bnb,rmsprop_bnb_8bit,rmsprop_bnb_32bit,galore_adamw,galore_adamw_8bit,galore_adafactor,galore_adamw_layerwise,galore_adamw_8bit_layerwise,galore_adafactor_layerwise,lomo,adalomo,grokadamw,schedule_free_radam,schedule_free_adamw,schedule_free_sgd,apollo_adamw,apollo_adamw_layerwise,stable_adamw}\n",
      "The optimizer to use. (default: adamw_torch)\n",
      "--optim_args OPTIM_ARGS, --optim-args OPTIM_ARGS\n",
      "Optional arguments to supply to optimizer. (default:\n",
      "None)\n",
      "--adafactor [ADAFACTOR]\n",
      "Whether or not to replace AdamW by Adafactor.\n",
      "(default: False)\n",
      "--group_by_length [GROUP_BY_LENGTH], --group-by-length [GROUP_BY_LENGTH]\n",
      "Whether or not to group samples of roughly the same\n",
      "length together when batching. (default: False)\n",
      "--length_column_name LENGTH_COLUMN_NAME, --length-column-name LENGTH_COLUMN_NAME\n",
      "Column name with precomputed lengths to use when\n",
      "grouping by length. (default: length)\n",
      "--report_to REPORT_TO, --report-to REPORT_TO\n",
      "The list of integrations to report the results and\n",
      "logs to. (default: None)\n",
      "--project PROJECT     The name of the project to use for logging. Currenly,\n",
      "only used by Trackio. (default: huggingface)\n",
      "--trackio_space_id TRACKIO_SPACE_ID, --trackio-space-id TRACKIO_SPACE_ID\n",
      "The Hugging Face Space ID to deploy to when using\n",
      "Trackio. Should be a complete Space name like\n",
      "'username/reponame' or 'orgname/reponame', or just\n",
      "'reponame' in which case the Space will be created in\n",
      "the currently-logged-in Hugging Face user's namespace.\n",
      "If `None`, will log to a local directory. Note that\n",
      "this Space will be public unless you set\n",
      "`hub_private_repo=True` or your organization's default\n",
      "is to create private Spaces. (default: trackio)\n",
      "--ddp_find_unused_parameters DDP_FIND_UNUSED_PARAMETERS, --ddp-find-unused-parameters DDP_FIND_UNUSED_PARAMETERS\n",
      "When using distributed training, the value of the flag\n",
      "`find_unused_parameters` passed to\n",
      "`DistributedDataParallel`. (default: None)\n",
      "--ddp_bucket_cap_mb DDP_BUCKET_CAP_MB, --ddp-bucket-cap-mb DDP_BUCKET_CAP_MB\n",
      "When using distributed training, the value of the flag\n",
      "`bucket_cap_mb` passed to `DistributedDataParallel`.\n",
      "(default: None)\n",
      "--ddp_broadcast_buffers DDP_BROADCAST_BUFFERS, --ddp-broadcast-buffers DDP_BROADCAST_BUFFERS\n",
      "When using distributed training, the value of the flag\n",
      "`broadcast_buffers` passed to\n",
      "`DistributedDataParallel`. (default: None)\n",
      "--dataloader_pin_memory [DATALOADER_PIN_MEMORY], --dataloader-pin-memory [DATALOADER_PIN_MEMORY]\n",
      "Whether or not to pin memory for DataLoader. (default:\n",
      "True)\n",
      "--no_dataloader_pin_memory, --no-dataloader-pin-memory\n",
      "Whether or not to pin memory for DataLoader. (default:\n",
      "False)\n",
      "--dataloader_persistent_workers [DATALOADER_PERSISTENT_WORKERS], --dataloader-persistent-workers [DATALOADER_PERSISTENT_WORKERS]\n",
      "If True, the data loader will not shut down the worker\n",
      "processes after a dataset has been consumed once. This\n",
      "allows to maintain the workers Dataset instances\n",
      "alive. Can potentially speed up training, but will\n",
      "increase RAM usage. (default: False)\n",
      "--skip_memory_metrics [SKIP_MEMORY_METRICS], --skip-memory-metrics [SKIP_MEMORY_METRICS]\n",
      "Whether or not to skip adding of memory profiler\n",
      "reports to metrics. (default: True)\n",
      "--no_skip_memory_metrics, --no-skip-memory-metrics\n",
      "Whether or not to skip adding of memory profiler\n",
      "reports to metrics. (default: False)\n",
      "--use_legacy_prediction_loop [USE_LEGACY_PREDICTION_LOOP], --use-legacy-prediction-loop [USE_LEGACY_PREDICTION_LOOP]\n",
      "Whether or not to use the legacy prediction_loop in\n",
      "the Trainer. (default: False)\n",
      "--push_to_hub [PUSH_TO_HUB], --push-to-hub [PUSH_TO_HUB]\n",
      "Whether or not to upload the trained model to the\n",
      "model hub after training. (default: False)\n",
      "--resume_from_checkpoint RESUME_FROM_CHECKPOINT, --resume-from-checkpoint RESUME_FROM_CHECKPOINT\n",
      "The path to a folder with a valid checkpoint for your\n",
      "model. (default: None)\n",
      "--hub_model_id HUB_MODEL_ID, --hub-model-id HUB_MODEL_ID\n",
      "The name of the repository to keep in sync with the\n",
      "local `output_dir`. (default: None)\n",
      "--hub_strategy {end,every_save,checkpoint,all_checkpoints}, --hub-strategy {end,every_save,checkpoint,all_checkpoints}\n",
      "The hub strategy to use when `--push_to_hub` is\n",
      "activated. (default: every_save)\n",
      "--hub_token HUB_TOKEN, --hub-token HUB_TOKEN\n",
      "The token to use to push to the Model Hub. (default:\n",
      "None)\n",
      "--hub_private_repo HUB_PRIVATE_REPO, --hub-private-repo HUB_PRIVATE_REPO\n",
      "Whether to make the repo private. If `None` (default),\n",
      "the repo will be public unless the organization's\n",
      "default is private. This value is ignored if the repo\n",
      "already exists. If reporting to Trackio with\n",
      "deployment to Hugging Face Spaces enabled, the same\n",
      "logic determines whether the Space is private.\n",
      "(default: None)\n",
      "--hub_always_push [HUB_ALWAYS_PUSH], --hub-always-push [HUB_ALWAYS_PUSH]\n",
      "Unless `True`, the Trainer will skip pushes if the\n",
      "previous one wasn't finished yet. (default: False)\n",
      "--hub_revision HUB_REVISION, --hub-revision HUB_REVISION\n",
      "The revision to use when pushing to the Hub. Can be a\n",
      "branch name, a tag, or a commit hash. (default: None)\n",
      "--gradient_checkpointing [GRADIENT_CHECKPOINTING], --gradient-checkpointing [GRADIENT_CHECKPOINTING]\n",
      "If True, use gradient checkpointing to save memory at\n",
      "the expense of slower backward pass. (default: False)\n",
      "--gradient_checkpointing_kwargs GRADIENT_CHECKPOINTING_KWARGS, --gradient-checkpointing-kwargs GRADIENT_CHECKPOINTING_KWARGS\n",
      "Gradient checkpointing key word arguments such as\n",
      "`use_reentrant`. Will be passed to\n",
      "`torch.utils.checkpoint.checkpoint` through\n",
      "`model.gradient_checkpointing_enable`. (default: None)\n",
      "--include_inputs_for_metrics [INCLUDE_INPUTS_FOR_METRICS], --include-inputs-for-metrics [INCLUDE_INPUTS_FOR_METRICS]\n",
      "This argument is deprecated and will be removed in\n",
      "version 5 of ü§ó Transformers. Use `include_for_metrics`\n",
      "instead. (default: False)\n",
      "--include_for_metrics INCLUDE_FOR_METRICS [INCLUDE_FOR_METRICS ...], --include-for-metrics INCLUDE_FOR_METRICS [INCLUDE_FOR_METRICS ...]\n",
      "List of strings to specify additional data to include\n",
      "in the `compute_metrics` function.Options: 'inputs',\n",
      "'loss'. (default: [])\n",
      "--eval_do_concat_batches [EVAL_DO_CONCAT_BATCHES], --eval-do-concat-batches [EVAL_DO_CONCAT_BATCHES]\n",
      "Whether to recursively concat\n",
      "inputs/losses/labels/predictions across batches. If\n",
      "`False`, will instead store them as lists, with each\n",
      "batch kept separate. (default: True)\n",
      "--no_eval_do_concat_batches, --no-eval-do-concat-batches\n",
      "Whether to recursively concat\n",
      "inputs/losses/labels/predictions across batches. If\n",
      "`False`, will instead store them as lists, with each\n",
      "batch kept separate. (default: False)\n",
      "--fp16_backend {auto,apex,cpu_amp}, --fp16-backend {auto,apex,cpu_amp}\n",
      "Deprecated. Use half_precision_backend instead\n",
      "(default: auto)\n",
      "--push_to_hub_model_id PUSH_TO_HUB_MODEL_ID, --push-to-hub-model-id PUSH_TO_HUB_MODEL_ID\n",
      "The name of the repository to which push the\n",
      "`Trainer`. (default: None)\n",
      "--push_to_hub_organization PUSH_TO_HUB_ORGANIZATION, --push-to-hub-organization PUSH_TO_HUB_ORGANIZATION\n",
      "The name of the organization in with to which push the\n",
      "`Trainer`. (default: None)\n",
      "--push_to_hub_token PUSH_TO_HUB_TOKEN, --push-to-hub-token PUSH_TO_HUB_TOKEN\n",
      "The token to use to push to the Model Hub. (default:\n",
      "None)\n",
      "--mp_parameters MP_PARAMETERS, --mp-parameters MP_PARAMETERS\n",
      "Used by the SageMaker launcher to send mp-specific\n",
      "args. Ignored in Trainer (default: )\n",
      "--auto_find_batch_size [AUTO_FIND_BATCH_SIZE], --auto-find-batch-size [AUTO_FIND_BATCH_SIZE]\n",
      "Whether to automatically decrease the batch size in\n",
      "half and rerun the training loop again each time a\n",
      "CUDA Out-of-Memory was reached (default: False)\n",
      "--full_determinism [FULL_DETERMINISM], --full-determinism [FULL_DETERMINISM]\n",
      "Whether to call enable_full_determinism instead of\n",
      "set_seed for reproducibility in distributed training.\n",
      "Important: this will negatively impact the\n",
      "performance, so only use it for debugging. (default:\n",
      "False)\n",
      "--torchdynamo TORCHDYNAMO\n",
      "This argument is deprecated, use\n",
      "`--torch_compile_backend` instead. (default: None)\n",
      "--ray_scope RAY_SCOPE, --ray-scope RAY_SCOPE\n",
      "The scope to use when doing hyperparameter search with\n",
      "Ray. By default, `\"last\"` will be used. Ray will then\n",
      "use the last checkpoint of all trials, compare those,\n",
      "and select the best one. However, other options are\n",
      "also available. See the Ray documentation (https://doc\n",
      "s.ray.io/en/latest/tune/api_docs/analysis.html#ray.tun\n",
      "e.ExperimentAnalysis.get_best_trial) for more options.\n",
      "(default: last)\n",
      "--ddp_timeout DDP_TIMEOUT, --ddp-timeout DDP_TIMEOUT\n",
      "Overrides the default timeout for distributed training\n",
      "(value should be given in seconds). (default: 1800)\n",
      "--torch_compile [TORCH_COMPILE], --torch-compile [TORCH_COMPILE]\n",
      "If set to `True`, the model will be wrapped in\n",
      "`torch.compile`. (default: False)\n",
      "--torch_compile_backend TORCH_COMPILE_BACKEND, --torch-compile-backend TORCH_COMPILE_BACKEND\n",
      "Which backend to use with `torch.compile`, passing one\n",
      "will trigger a model compilation. (default: None)\n",
      "--torch_compile_mode TORCH_COMPILE_MODE, --torch-compile-mode TORCH_COMPILE_MODE\n",
      "Which mode to use with `torch.compile`, passing one\n",
      "will trigger a model compilation. (default: None)\n",
      "--include_tokens_per_second [INCLUDE_TOKENS_PER_SECOND], --include-tokens-per-second [INCLUDE_TOKENS_PER_SECOND]\n",
      "If set to `True`, the speed metrics will include `tgs`\n",
      "(tokens per second per device). (default: False)\n",
      "--include_num_input_tokens_seen [INCLUDE_NUM_INPUT_TOKENS_SEEN], --include-num-input-tokens-seen [INCLUDE_NUM_INPUT_TOKENS_SEEN]\n",
      "Whether to track the number of input tokens seen. Can\n",
      "be `'all'` to count all tokens, `'non_padding'` to\n",
      "count only non-padding tokens, or a boolean (`True`\n",
      "maps to `'all'`, `False` to `'no'`). (default: False)\n",
      "--neftune_noise_alpha NEFTUNE_NOISE_ALPHA, --neftune-noise-alpha NEFTUNE_NOISE_ALPHA\n",
      "Activates neftune noise embeddings into the model.\n",
      "NEFTune has been proven to drastically improve model\n",
      "performances for instruction fine-tuning. Check out\n",
      "the original paper here:\n",
      "https://huggingface.co/papers/2310.05914 and the\n",
      "original code here:\n",
      "https://github.com/neelsjain/NEFTune. Only supported\n",
      "for `PreTrainedModel` and `PeftModel` classes.\n",
      "(default: None)\n",
      "--optim_target_modules OPTIM_TARGET_MODULES, --optim-target-modules OPTIM_TARGET_MODULES\n",
      "Target modules for the optimizer defined in the\n",
      "`optim` argument. Only used for the GaLore optimizer\n",
      "at the moment. (default: None)\n",
      "--batch_eval_metrics [BATCH_EVAL_METRICS], --batch-eval-metrics [BATCH_EVAL_METRICS]\n",
      "Break eval metrics calculation into batches to save\n",
      "memory. (default: False)\n",
      "--eval_on_start [EVAL_ON_START], --eval-on-start [EVAL_ON_START]\n",
      "Whether to run through the entire `evaluation` step at\n",
      "the very beginning of training as a sanity check.\n",
      "(default: False)\n",
      "--use_liger_kernel [USE_LIGER_KERNEL], --use-liger-kernel [USE_LIGER_KERNEL]\n",
      "Whether or not to enable the Liger Kernel for model\n",
      "training. (default: False)\n",
      "--liger_kernel_config LIGER_KERNEL_CONFIG, --liger-kernel-config LIGER_KERNEL_CONFIG\n",
      "Configuration to be used for Liger Kernel. When\n",
      "use_liger_kernel=True, this dict is passed as keyword\n",
      "arguments to the `_apply_liger_kernel_to_instance`\n",
      "function, which specifies which kernels to apply.\n",
      "Available options vary by model but typically include:\n",
      "'rope', 'swiglu', 'cross_entropy',\n",
      "'fused_linear_cross_entropy', 'rms_norm', etc. If\n",
      "None, use the default kernel configurations. (default:\n",
      "None)\n",
      "--eval_use_gather_object [EVAL_USE_GATHER_OBJECT], --eval-use-gather-object [EVAL_USE_GATHER_OBJECT]\n",
      "Whether to run recursively gather object in a nested\n",
      "list/tuple/dictionary of objects from all devices.\n",
      "(default: False)\n",
      "--average_tokens_across_devices [AVERAGE_TOKENS_ACROSS_DEVICES], --average-tokens-across-devices [AVERAGE_TOKENS_ACROSS_DEVICES]\n",
      "Whether or not to average tokens across devices. If\n",
      "enabled, will use all_reduce to synchronize\n",
      "num_tokens_in_batch for precise loss calculation.\n",
      "Reference: https://github.com/huggingface/transformers\n",
      "/issues/34242 (default: True)\n",
      "--no_average_tokens_across_devices, --no-average-tokens-across-devices\n",
      "Whether or not to average tokens across devices. If\n",
      "enabled, will use all_reduce to synchronize\n",
      "num_tokens_in_batch for precise loss calculation.\n",
      "Reference: https://github.com/huggingface/transformers\n",
      "/issues/34242 (default: False)\n",
      "--sortish_sampler [SORTISH_SAMPLER], --sortish-sampler [SORTISH_SAMPLER]\n",
      "Whether to use SortishSampler or not. (default: False)\n",
      "--predict_with_generate [PREDICT_WITH_GENERATE], --predict-with-generate [PREDICT_WITH_GENERATE]\n",
      "Whether to use generate to calculate generative\n",
      "metrics (ROUGE, BLEU). (default: False)\n",
      "--generation_max_length GENERATION_MAX_LENGTH, --generation-max-length GENERATION_MAX_LENGTH\n",
      "The `max_length` to use on each evaluation loop when\n",
      "`predict_with_generate=True`. Will default to the\n",
      "`max_length` value of the model configuration.\n",
      "(default: None)\n",
      "--generation_num_beams GENERATION_NUM_BEAMS, --generation-num-beams GENERATION_NUM_BEAMS\n",
      "The `num_beams` to use on each evaluation loop when\n",
      "`predict_with_generate=True`. Will default to the\n",
      "`num_beams` value of the model configuration.\n",
      "(default: None)\n",
      "--generation_config GENERATION_CONFIG, --generation-config GENERATION_CONFIG\n",
      "Model id, file path or url pointing to a\n",
      "GenerationConfig json file, to use during prediction.\n",
      "(default: None)\n",
      "--ray_run_name RAY_RUN_NAME, --ray-run-name RAY_RUN_NAME\n",
      "The training results will be saved at\n",
      "`<ray_storage_path>/ray_run_name`. (default: None)\n",
      "--ray_storage_path RAY_STORAGE_PATH, --ray-storage-path RAY_STORAGE_PATH\n",
      "The storage path to save training results to (default:\n",
      "./saves)\n",
      "--ray_storage_filesystem {s3,gs,gcs}, --ray-storage-filesystem {s3,gs,gcs}\n",
      "The storage filesystem to use. If None specified,\n",
      "local filesystem will be used. (default: None)\n",
      "--ray_num_workers RAY_NUM_WORKERS, --ray-num-workers RAY_NUM_WORKERS\n",
      "The number of workers for Ray training. Default is 1\n",
      "worker. (default: 1)\n",
      "--resources_per_worker RESOURCES_PER_WORKER, --resources-per-worker RESOURCES_PER_WORKER\n",
      "The resources per worker for Ray training. Default is\n",
      "to use 1 GPU per worker. (default: {'GPU': 1})\n",
      "--placement_strategy {SPREAD,PACK,STRICT_SPREAD,STRICT_PACK}, --placement-strategy {SPREAD,PACK,STRICT_SPREAD,STRICT_PACK}\n",
      "The placement strategy for Ray training. Default is\n",
      "PACK. (default: PACK)\n",
      "--ray_init_kwargs RAY_INIT_KWARGS, --ray-init-kwargs RAY_INIT_KWARGS\n",
      "The arguments to pass to ray.init for Ray training.\n",
      "Default is None. (default: None)\n",
      "--freeze_trainable_layers FREEZE_TRAINABLE_LAYERS, --freeze-trainable-layers FREEZE_TRAINABLE_LAYERS\n",
      "The number of trainable layers for freeze (partial-\n",
      "parameter) fine-tuning. Positive numbers mean the last\n",
      "n layers are set as trainable, negative numbers mean\n",
      "the first n layers are set as trainable. (default: 2)\n",
      "--freeze_trainable_modules FREEZE_TRAINABLE_MODULES, --freeze-trainable-modules FREEZE_TRAINABLE_MODULES\n",
      "Name(s) of trainable modules for freeze (partial-\n",
      "parameter) fine-tuning. Use commas to separate\n",
      "multiple modules. Use `all` to specify all the\n",
      "available modules. (default: all)\n",
      "--freeze_extra_modules FREEZE_EXTRA_MODULES, --freeze-extra-modules FREEZE_EXTRA_MODULES\n",
      "Name(s) of modules apart from hidden layers to be set\n",
      "as trainable for freeze (partial-parameter) fine-\n",
      "tuning. Use commas to separate multiple modules.\n",
      "(default: None)\n",
      "--additional_target ADDITIONAL_TARGET, --additional-target ADDITIONAL_TARGET\n",
      "Name(s) of modules apart from LoRA layers to be set as\n",
      "trainable and saved in the final checkpoint. Use\n",
      "commas to separate multiple modules. (default: None)\n",
      "--module_dropout MODULE_DROPOUT, --module-dropout MODULE_DROPOUT\n",
      "Dropout rate for the OFT fine-tuning. (default: 0.0)\n",
      "--oft_rank OFT_RANK, --oft-rank OFT_RANK\n",
      "The intrinsic dimension for OFT fine-tuning. (default:\n",
      "0)\n",
      "--oft_block_size OFT_BLOCK_SIZE, --oft-block-size OFT_BLOCK_SIZE\n",
      "The intrinsic dimension for OFT fine-tuning. (default:\n",
      "32)\n",
      "--oft_target OFT_TARGET, --oft-target OFT_TARGET\n",
      "Name(s) of target modules to apply OFT. Use commas to\n",
      "separate multiple modules. Use `all` to specify all\n",
      "the linear modules. (default: all)\n",
      "--create_new_adapter [CREATE_NEW_ADAPTER], --create-new-adapter [CREATE_NEW_ADAPTER]\n",
      "Whether or not to create a new adapter with randomly\n",
      "initialized weight. (default: False)\n",
      "--lora_alpha LORA_ALPHA, --lora-alpha LORA_ALPHA\n",
      "The scale factor for LoRA fine-tuning (default:\n",
      "lora_rank * 2). (default: None)\n",
      "--lora_dropout LORA_DROPOUT, --lora-dropout LORA_DROPOUT\n",
      "Dropout rate for the LoRA fine-tuning. (default: 0.0)\n",
      "--lora_rank LORA_RANK, --lora-rank LORA_RANK\n",
      "The intrinsic dimension for LoRA fine-tuning.\n",
      "(default: 8)\n",
      "--lora_target LORA_TARGET, --lora-target LORA_TARGET\n",
      "Name(s) of target modules to apply LoRA. Use commas to\n",
      "separate multiple modules. Use `all` to specify all\n",
      "the linear modules. (default: all)\n",
      "--loraplus_lr_ratio LORAPLUS_LR_RATIO, --loraplus-lr-ratio LORAPLUS_LR_RATIO\n",
      "LoRA plus learning rate ratio (lr_B / lr_A). (default:\n",
      "None)\n",
      "--loraplus_lr_embedding LORAPLUS_LR_EMBEDDING, --loraplus-lr-embedding LORAPLUS_LR_EMBEDDING\n",
      "LoRA plus learning rate for lora embedding layers.\n",
      "(default: 1e-06)\n",
      "--use_rslora [USE_RSLORA], --use-rslora [USE_RSLORA]\n",
      "Whether or not to use the rank stabilization scaling\n",
      "factor for LoRA layer. (default: False)\n",
      "--use_dora [USE_DORA], --use-dora [USE_DORA]\n",
      "Whether or not to use the weight-decomposed lora\n",
      "method (DoRA). (default: False)\n",
      "--pissa_init [PISSA_INIT], --pissa-init [PISSA_INIT]\n",
      "Whether or not to initialize a PiSSA adapter.\n",
      "(default: False)\n",
      "--pissa_iter PISSA_ITER, --pissa-iter PISSA_ITER\n",
      "The number of iteration steps performed by FSVD in\n",
      "PiSSA. Use -1 to disable it. (default: 16)\n",
      "--pissa_convert [PISSA_CONVERT], --pissa-convert [PISSA_CONVERT]\n",
      "Whether or not to convert the PiSSA adapter to a\n",
      "normal LoRA adapter. (default: False)\n",
      "--pref_beta PREF_BETA, --pref-beta PREF_BETA\n",
      "The beta parameter in the preference loss. (default:\n",
      "0.1)\n",
      "--pref_ftx PREF_FTX, --pref-ftx PREF_FTX\n",
      "The supervised fine-tuning loss coefficient in DPO\n",
      "training. (default: 0.0)\n",
      "--pref_bco_weight PREF_BCO_WEIGHT, --pref-bco-weight PREF_BCO_WEIGHT\n",
      "The Binary Classifier Optimization coefficient in DPO\n",
      "training. (default: 0.0)\n",
      "--pref_loss {sigmoid,hinge,ipo,kto_pair,orpo,simpo}, --pref-loss {sigmoid,hinge,ipo,kto_pair,orpo,simpo}\n",
      "The type of DPO loss to use. (default: sigmoid)\n",
      "--dpo_label_smoothing DPO_LABEL_SMOOTHING, --dpo-label-smoothing DPO_LABEL_SMOOTHING\n",
      "The robust DPO label smoothing parameter in cDPO that\n",
      "should be between 0 and 0.5. (default: 0.0)\n",
      "--kto_chosen_weight KTO_CHOSEN_WEIGHT, --kto-chosen-weight KTO_CHOSEN_WEIGHT\n",
      "The weight factor of the desirable losses in KTO\n",
      "training. (default: 1.0)\n",
      "--kto_rejected_weight KTO_REJECTED_WEIGHT, --kto-rejected-weight KTO_REJECTED_WEIGHT\n",
      "The weight factor of the undesirable losses in KTO\n",
      "training. (default: 1.0)\n",
      "--simpo_gamma SIMPO_GAMMA, --simpo-gamma SIMPO_GAMMA\n",
      "The target reward margin term in SimPO loss. (default:\n",
      "0.5)\n",
      "--ppo_buffer_size PPO_BUFFER_SIZE, --ppo-buffer-size PPO_BUFFER_SIZE\n",
      "The number of mini-batches to make experience buffer\n",
      "in a PPO optimization step. (default: 1)\n",
      "--ppo_epochs PPO_EPOCHS, --ppo-epochs PPO_EPOCHS\n",
      "The number of epochs to perform in a PPO optimization\n",
      "step. (default: 4)\n",
      "--ppo_score_norm [PPO_SCORE_NORM], --ppo-score-norm [PPO_SCORE_NORM]\n",
      "Use score normalization in PPO training. (default:\n",
      "False)\n",
      "--ppo_target PPO_TARGET, --ppo-target PPO_TARGET\n",
      "Target KL value for adaptive KL control in PPO\n",
      "training. (default: 6.0)\n",
      "--ppo_whiten_rewards [PPO_WHITEN_REWARDS], --ppo-whiten-rewards [PPO_WHITEN_REWARDS]\n",
      "Whiten the rewards before compute advantages in PPO\n",
      "training. (default: False)\n",
      "--ref_model REF_MODEL, --ref-model REF_MODEL\n",
      "Path to the reference model used for the PPO or DPO\n",
      "training. (default: None)\n",
      "--ref_model_adapters REF_MODEL_ADAPTERS, --ref-model-adapters REF_MODEL_ADAPTERS\n",
      "Path to the adapters of the reference model. (default:\n",
      "None)\n",
      "--ref_model_quantization_bit REF_MODEL_QUANTIZATION_BIT, --ref-model-quantization-bit REF_MODEL_QUANTIZATION_BIT\n",
      "The number of bits to quantize the reference model.\n",
      "(default: None)\n",
      "--reward_model REWARD_MODEL, --reward-model REWARD_MODEL\n",
      "Path to the reward model used for the PPO training.\n",
      "(default: None)\n",
      "--reward_model_adapters REWARD_MODEL_ADAPTERS, --reward-model-adapters REWARD_MODEL_ADAPTERS\n",
      "Path to the adapters of the reward model. (default:\n",
      "None)\n",
      "--reward_model_quantization_bit REWARD_MODEL_QUANTIZATION_BIT, --reward-model-quantization-bit REWARD_MODEL_QUANTIZATION_BIT\n",
      "The number of bits to quantize the reward model.\n",
      "(default: None)\n",
      "--reward_model_type {lora,full,api}, --reward-model-type {lora,full,api}\n",
      "The type of the reward model in PPO training. Lora\n",
      "model only supports lora training. (default: lora)\n",
      "--ld_alpha LD_ALPHA, --ld-alpha LD_ALPHA\n",
      "Alpha parameter from the LD-DPO paper, which controls\n",
      "the weighting of the verbose token log-probabilities\n",
      "in responses. (default: None)\n",
      "--use_galore [USE_GALORE], --use-galore [USE_GALORE]\n",
      "Whether or not to use the gradient low-Rank projection\n",
      "(GaLore). (default: False)\n",
      "--galore_target GALORE_TARGET, --galore-target GALORE_TARGET\n",
      "Name(s) of modules to apply GaLore. Use commas to\n",
      "separate multiple modules. Use `all` to specify all\n",
      "the linear modules. (default: all)\n",
      "--galore_rank GALORE_RANK, --galore-rank GALORE_RANK\n",
      "The rank of GaLore gradients. (default: 16)\n",
      "--galore_update_interval GALORE_UPDATE_INTERVAL, --galore-update-interval GALORE_UPDATE_INTERVAL\n",
      "Number of steps to update the GaLore projection.\n",
      "(default: 200)\n",
      "--galore_scale GALORE_SCALE, --galore-scale GALORE_SCALE\n",
      "GaLore scaling coefficient. (default: 2.0)\n",
      "--galore_proj_type {std,reverse_std,right,left,full}, --galore-proj-type {std,reverse_std,right,left,full}\n",
      "Type of GaLore projection. (default: std)\n",
      "--galore_layerwise [GALORE_LAYERWISE], --galore-layerwise [GALORE_LAYERWISE]\n",
      "Whether or not to enable layer-wise update to further\n",
      "save memory. (default: False)\n",
      "--use_apollo [USE_APOLLO], --use-apollo [USE_APOLLO]\n",
      "Whether or not to use the APOLLO optimizer. (default:\n",
      "False)\n",
      "--apollo_target APOLLO_TARGET, --apollo-target APOLLO_TARGET\n",
      "Name(s) of modules to apply APOLLO. Use commas to\n",
      "separate multiple modules. Use `all` to specify all\n",
      "the linear modules. (default: all)\n",
      "--apollo_rank APOLLO_RANK, --apollo-rank APOLLO_RANK\n",
      "The rank of APOLLO gradients. (default: 16)\n",
      "--apollo_update_interval APOLLO_UPDATE_INTERVAL, --apollo-update-interval APOLLO_UPDATE_INTERVAL\n",
      "Number of steps to update the APOLLO projection.\n",
      "(default: 200)\n",
      "--apollo_scale APOLLO_SCALE, --apollo-scale APOLLO_SCALE\n",
      "APOLLO scaling coefficient. (default: 32.0)\n",
      "--apollo_proj {svd,random}, --apollo-proj {svd,random}\n",
      "Type of APOLLO low-rank projection algorithm (svd or\n",
      "random). (default: random)\n",
      "--apollo_proj_type {std,right,left}, --apollo-proj-type {std,right,left}\n",
      "Type of APOLLO projection. (default: std)\n",
      "--apollo_scale_type {channel,tensor}, --apollo-scale-type {channel,tensor}\n",
      "Type of APOLLO scaling (channel or tensor). (default:\n",
      "channel)\n",
      "--apollo_layerwise [APOLLO_LAYERWISE], --apollo-layerwise [APOLLO_LAYERWISE]\n",
      "Whether or not to enable layer-wise update to further\n",
      "save memory. (default: False)\n",
      "--apollo_scale_front [APOLLO_SCALE_FRONT], --apollo-scale-front [APOLLO_SCALE_FRONT]\n",
      "Whether or not to use the norm-growth limiter in front\n",
      "of gradient scaling. (default: False)\n",
      "--use_badam [USE_BADAM], --use-badam [USE_BADAM]\n",
      "Whether or not to use the BAdam optimizer. (default:\n",
      "False)\n",
      "--badam_mode {layer,ratio}, --badam-mode {layer,ratio}\n",
      "Whether to use layer-wise or ratio-wise BAdam\n",
      "optimizer. (default: layer)\n",
      "--badam_start_block BADAM_START_BLOCK, --badam-start-block BADAM_START_BLOCK\n",
      "The starting block index for layer-wise BAdam.\n",
      "(default: None)\n",
      "--badam_switch_mode {ascending,descending,random,fixed}, --badam-switch-mode {ascending,descending,random,fixed}\n",
      "the strategy of picking block to update for layer-wise\n",
      "BAdam. (default: ascending)\n",
      "--badam_switch_interval BADAM_SWITCH_INTERVAL, --badam-switch-interval BADAM_SWITCH_INTERVAL\n",
      "Number of steps to update the block for layer-wise\n",
      "BAdam. Use -1 to disable the block update. (default:\n",
      "50)\n",
      "--badam_update_ratio BADAM_UPDATE_RATIO, --badam-update-ratio BADAM_UPDATE_RATIO\n",
      "The ratio of the update for ratio-wise BAdam.\n",
      "(default: 0.05)\n",
      "--badam_mask_mode {adjacent,scatter}, --badam-mask-mode {adjacent,scatter}\n",
      "The mode of the mask for BAdam optimizer. `adjacent`\n",
      "means that the trainable parameters are adjacent to\n",
      "each other, `scatter` means that trainable parameters\n",
      "are randomly choosed from the weight. (default:\n",
      "adjacent)\n",
      "--badam_verbose BADAM_VERBOSE, --badam-verbose BADAM_VERBOSE\n",
      "The verbosity level of BAdam optimizer. 0 for no\n",
      "print, 1 for print the block prefix, 2 for print\n",
      "trainable parameters. (default: 0)\n",
      "--use_swanlab [USE_SWANLAB], --use-swanlab [USE_SWANLAB]\n",
      "Whether or not to use the SwanLab (an experiment\n",
      "tracking and visualization tool). (default: False)\n",
      "--swanlab_project SWANLAB_PROJECT, --swanlab-project SWANLAB_PROJECT\n",
      "The project name in SwanLab. (default: llamafactory)\n",
      "--swanlab_workspace SWANLAB_WORKSPACE, --swanlab-workspace SWANLAB_WORKSPACE\n",
      "The workspace name in SwanLab. (default: None)\n",
      "--swanlab_run_name SWANLAB_RUN_NAME, --swanlab-run-name SWANLAB_RUN_NAME\n",
      "The experiment name in SwanLab. (default: None)\n",
      "--swanlab_mode {cloud,local}, --swanlab-mode {cloud,local}\n",
      "The mode of SwanLab. (default: cloud)\n",
      "--swanlab_api_key SWANLAB_API_KEY, --swanlab-api-key SWANLAB_API_KEY\n",
      "The API key for SwanLab. (default: None)\n",
      "--swanlab_logdir SWANLAB_LOGDIR, --swanlab-logdir SWANLAB_LOGDIR\n",
      "The log directory for SwanLab. (default: None)\n",
      "--swanlab_lark_webhook_url SWANLAB_LARK_WEBHOOK_URL, --swanlab-lark-webhook-url SWANLAB_LARK_WEBHOOK_URL\n",
      "The Lark(È£û‰π¶) webhook URL for SwanLab. (default: None)\n",
      "--swanlab_lark_secret SWANLAB_LARK_SECRET, --swanlab-lark-secret SWANLAB_LARK_SECRET\n",
      "The Lark(È£û‰π¶) secret for SwanLab. (default: None)\n",
      "--pure_bf16 [PURE_BF16], --pure-bf16 [PURE_BF16]\n",
      "Whether or not to train model in purely bf16 precision\n",
      "(without AMP). (default: False)\n",
      "--stage {pt,sft,rm,ppo,dpo,kto}\n",
      "Which stage will be performed in training. (default:\n",
      "sft)\n",
      "--finetuning_type {lora,oft,freeze,full}, --finetuning-type {lora,oft,freeze,full}\n",
      "Which fine-tuning method to use. (default: lora)\n",
      "--use_llama_pro [USE_LLAMA_PRO], --use-llama-pro [USE_LLAMA_PRO]\n",
      "Whether or not to make only the parameters in the\n",
      "expanded blocks trainable. (default: False)\n",
      "--use_adam_mini [USE_ADAM_MINI], --use-adam-mini [USE_ADAM_MINI]\n",
      "Whether or not to use the Adam-mini optimizer.\n",
      "(default: False)\n",
      "--use_mca [USE_MCA], --use-mca [USE_MCA]\n",
      "Whether or not to use MCA (Megatron Core Adapter)\n",
      "training. Controlled by USE_MCA environment variable.\n",
      "(default: False)\n",
      "--use_muon [USE_MUON], --use-muon [USE_MUON]\n",
      "Whether or not to use the Muon optimizer. (default:\n",
      "False)\n",
      "--use_dft_loss [USE_DFT_LOSS], --use-dft-loss [USE_DFT_LOSS]\n",
      "Whether to use the DFT loss. (default: False)\n",
      "--freeze_vision_tower [FREEZE_VISION_TOWER], --freeze-vision-tower [FREEZE_VISION_TOWER]\n",
      "Whether ot not to freeze the vision tower in MLLM\n",
      "training. (default: True)\n",
      "--no_freeze_vision_tower, --no-freeze-vision-tower\n",
      "Whether ot not to freeze the vision tower in MLLM\n",
      "training. (default: False)\n",
      "--freeze_multi_modal_projector [FREEZE_MULTI_MODAL_PROJECTOR], --freeze-multi-modal-projector [FREEZE_MULTI_MODAL_PROJECTOR]\n",
      "Whether or not to freeze the multi modal projector in\n",
      "MLLM training. (default: True)\n",
      "--no_freeze_multi_modal_projector, --no-freeze-multi-modal-projector\n",
      "Whether or not to freeze the multi modal projector in\n",
      "MLLM training. (default: False)\n",
      "--freeze_language_model [FREEZE_LANGUAGE_MODEL], --freeze-language-model [FREEZE_LANGUAGE_MODEL]\n",
      "Whether or not to freeze the language model in MLLM\n",
      "training. (default: False)\n",
      "--compute_accuracy [COMPUTE_ACCURACY], --compute-accuracy [COMPUTE_ACCURACY]\n",
      "Whether or not to compute the token-level accuracy at\n",
      "evaluation. (default: False)\n",
      "--disable_shuffling [DISABLE_SHUFFLING], --disable-shuffling [DISABLE_SHUFFLING]\n",
      "Whether or not to disable the shuffling of the\n",
      "training set. (default: False)\n",
      "--early_stopping_steps EARLY_STOPPING_STEPS, --early-stopping-steps EARLY_STOPPING_STEPS\n",
      "Number of steps to stop training if the\n",
      "`metric_for_best_model` does not improve. (default:\n",
      "None)\n",
      "--plot_loss [PLOT_LOSS], --plot-loss [PLOT_LOSS]\n",
      "Whether or not to save the training loss curves.\n",
      "(default: False)\n",
      "--include_effective_tokens_per_second [INCLUDE_EFFECTIVE_TOKENS_PER_SECOND], --include-effective-tokens-per-second [INCLUDE_EFFECTIVE_TOKENS_PER_SECOND]\n",
      "Whether or not to compute effective tokens per second.\n",
      "(default: False)\n",
      "--do_sample [DO_SAMPLE], --do-sample [DO_SAMPLE]\n",
      "Whether or not to use sampling, use greedy decoding\n",
      "otherwise. (default: True)\n",
      "--no_do_sample, --no-do-sample\n",
      "Whether or not to use sampling, use greedy decoding\n",
      "otherwise. (default: False)\n",
      "--temperature TEMPERATURE\n",
      "The value used to modulate the next token\n",
      "probabilities. (default: 0.95)\n",
      "--top_p TOP_P, --top-p TOP_P\n",
      "The smallest set of most probable tokens with\n",
      "probabilities that add up to top_p or higher are kept.\n",
      "(default: 0.7)\n",
      "--top_k TOP_K, --top-k TOP_K\n",
      "The number of highest probability vocabulary tokens to\n",
      "keep for top-k filtering. (default: 50)\n",
      "--num_beams NUM_BEAMS, --num-beams NUM_BEAMS\n",
      "Number of beams for beam search. 1 means no beam\n",
      "search. (default: 1)\n",
      "--max_length MAX_LENGTH, --max-length MAX_LENGTH\n",
      "The maximum length the generated tokens can have. It\n",
      "can be overridden by max_new_tokens. (default: 1024)\n",
      "--max_new_tokens MAX_NEW_TOKENS, --max-new-tokens MAX_NEW_TOKENS\n",
      "The maximum numbers of tokens to generate, ignoring\n",
      "the number of tokens in the prompt. (default: 1024)\n",
      "--repetition_penalty REPETITION_PENALTY, --repetition-penalty REPETITION_PENALTY\n",
      "The parameter for repetition penalty. 1.0 means no\n",
      "penalty. (default: 1.0)\n",
      "--length_penalty LENGTH_PENALTY, --length-penalty LENGTH_PENALTY\n",
      "Exponential penalty to the length that is used with\n",
      "beam-based generation. (default: 1.0)\n",
      "--skip_special_tokens [SKIP_SPECIAL_TOKENS], --skip-special-tokens [SKIP_SPECIAL_TOKENS]\n",
      "Whether or not to remove special tokens in the\n",
      "decoding. (default: True)\n",
      "--no_skip_special_tokens, --no-skip-special-tokens\n",
      "Whether or not to remove special tokens in the\n",
      "decoding. (default: False)\n",
      "\n",
      "Got unknown args, potentially deprecated arguments: ['export']\n",
      "Traceback (most recent call last):\n",
      "File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "File \"<frozen runpy>\", line 88, in _run_code\n",
      "File \"C:\\Users\\ruben\\Documents\\TrainingAI\\LLaMA-Factory\\src\\llamafactory\\launcher.py\", line 185, in <module>\n",
      "run_exp()\n",
      "File \"C:\\Users\\ruben\\Documents\\TrainingAI\\LLaMA-Factory\\src\\llamafactory\\train\\tuner.py\", line 132, in run_exp\n",
      "_training_function(config={\"args\": args, \"callbacks\": callbacks})\n",
      "File \"C:\\Users\\ruben\\Documents\\TrainingAI\\LLaMA-Factory\\src\\llamafactory\\train\\tuner.py\", line 55, in _training_function\n",
      "model_args, data_args, training_args, finetuning_args, generating_args = get_train_args(args)\n",
      "^^^^^^^^^^^^^^^^^^^^\n",
      "File \"C:\\Users\\ruben\\Documents\\TrainingAI\\LLaMA-Factory\\src\\llamafactory\\hparams\\parser.py\", line 257, in get_train_args\n",
      "model_args, data_args, training_args, finetuning_args, generating_args = _parse_train_args(args)\n",
      "^^^^^^^^^^^^^^^^^^^^^^^\n",
      "File \"C:\\Users\\ruben\\Documents\\TrainingAI\\LLaMA-Factory\\src\\llamafactory\\hparams\\parser.py\", line 211, in _parse_train_args\n",
      "return _parse_args(parser, args, allow_extra_keys=allow_extra_keys)\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "File \"C:\\Users\\ruben\\Documents\\TrainingAI\\LLaMA-Factory\\src\\llamafactory\\hparams\\parser.py\", line 97, in _parse_args\n",
      "raise ValueError(f\"Some specified arguments are not used by the HfArgumentParser: {unknown_args}\")\n",
      "ValueError: Some specified arguments are not used by the HfArgumentParser: ['export']\n",
      "\n",
      "‚ùå Model merge failed with exit code 1.\n"
     ]
    }
   ],
   "source": [
    "# --- MERGE THE TRAINED LORA ---\n",
    "print(\"üöÄ Starting model merge process...\")\n",
    "\n",
    "merge_command = [\n",
    "    PYTHON_EXECUTABLE,\n",
    "    \"-m\", \"llamafactory.launcher\",\n",
    "    \"export\",\n",
    "    \"--model_name_or_path\", MODEL_ID,\n",
    "    \"--adapter_name_or_path\", str(LORA_OUTPUT_DIR),\n",
    "    \"--template\", \"llama3\",\n",
    "    \"--export_dir\", str(MERGED_MODEL_DIR),\n",
    "    \"--export_size\", \"2\"\n",
    "]\n",
    "\n",
    "print(\"--- Merge Command ---\")\n",
    "print(subprocess.list2cmdline(merge_command))\n",
    "print(\"---------------------\")\n",
    "\n",
    "process = subprocess.Popen(\n",
    "    merge_command,\n",
    "    cwd=str(LLAMA_FACTORY_PATH),\n",
    "    env=LLAMA_FACTORY_ENV,\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.STDOUT,\n",
    "    text=True,\n",
    "    encoding='utf-8',\n",
    "    bufsize=1,\n",
    ")\n",
    "while True:\n",
    "    output = process.stdout.readline()\n",
    "    if output == '' and process.poll() is not None:\n",
    "        break\n",
    "    if output:\n",
    "        print(output.strip())\n",
    "\n",
    "if process.returncode == 0:\n",
    "    print(f\"\\nüéâ Model merged successfully! Full-precision model saved at:\\n{MERGED_MODEL_DIR}\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå Model merge failed with exit code {process.returncode}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d3eb4ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Starting quantization to GGUF format...\n",
      "--- Convert Command ---\n",
      "c:\\Users\\ruben\\AppData\\Local\\Programs\\Python\\Python311\\python.exe C:\\Users\\ruben\\Documents\\TrainingAI\\llama.cpp\\convert_hf_to_gguf.py \"R:\\Files Ruben\\GitRepos\\DeepDiveV2AI\\artifacts\\merged_models\\Meta-Llama-3-8B-Instruct-ProjectDeepDive-Lora-v1\" --outfile \"R:\\Files Ruben\\GitRepos\\DeepDiveV2AI\\final_gguf_models\\Meta-Llama-3-8B-Instruct-ProjectDeepDive-Lora-v1-F16.gguf\" --outtype f16\n",
      "-----------------------\n",
      "INFO:hf-to-gguf:Loading model: Meta-Llama-3-8B-Instruct-ProjectDeepDive-Lora-v1\n",
      "WARNING:hf-to-gguf:Failed to load model config from R:\\Files Ruben\\GitRepos\\DeepDiveV2AI\\artifacts\\merged_models\\Meta-Llama-3-8B-Instruct-ProjectDeepDive-Lora-v1: Unrecognized model in R:\\Files Ruben\\GitRepos\\DeepDiveV2AI\\artifacts\\merged_models\\Meta-Llama-3-8B-Instruct-ProjectDeepDive-Lora-v1. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: aimv2, aimv2_vision_model, albert, align, altclip, apertus, arcee, aria, aria_text, audio-spectrogram-transformer, autoformer, aya_vision, bamba, bark, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, biogpt, bit, bitnet, blenderbot, blenderbot-small, blip, blip-2, blip_2_qformer, bloom, blt, bridgetower, bros, camembert, canine, chameleon, chinese_clip, chinese_clip_vision_model, clap, clip, clip_text_model, clip_vision_model, clipseg, clvp, code_llama, codegen, cohere, cohere2, cohere2_vision, colpali, colqwen2, conditional_detr, convbert, convnext, convnextv2, cpmant, csm, ctrl, cvt, d_fine, dab-detr, dac, data2vec-audio, data2vec-text, data2vec-vision, dbrx, deberta, deberta-v2, decision_transformer, deepseek_v2, deepseek_v3, deepseek_vl, deepseek_vl_hybrid, deformable_detr, deit, depth_anything, depth_pro, deta, detr, dia, diffllama, dinat, dinov2, dinov2_with_registers, dinov3_convnext, dinov3_vit, distilbert, doge, donut-swin, dots1, dpr, dpt, edgetam, edgetam_video, edgetam_vision_model, efficientformer, efficientloftr, efficientnet, electra, emu3, encodec, encoder-decoder, eomt, ernie, ernie4_5, ernie4_5_moe, ernie_m, esm, evolla, exaone4, falcon, falcon_h1, falcon_mamba, fastspeech2_conformer, fastspeech2_conformer_with_hifigan, flaubert, flava, flex_olmo, florence2, fnet, focalnet, fsmt, funnel, fuyu, gemma, gemma2, gemma3, gemma3_text, gemma3n, gemma3n_audio, gemma3n_text, gemma3n_vision, git, glm, glm4, glm4_moe, glm4v, glm4v_moe, glm4v_moe_text, glm4v_text, glpn, got_ocr2, gpt-sw3, gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gpt_oss, gptj, gptsan-japanese, granite, granite_speech, granitemoe, granitemoehybrid, granitemoeshared, granitevision, graphormer, grounding-dino, groupvit, helium, hgnet_v2, hiera, hubert, hunyuan_v1_dense, hunyuan_v1_moe, ibert, idefics, idefics2, idefics3, idefics3_vision, ijepa, imagegpt, informer, instructblip, instructblipvideo, internvl, internvl_vision, jamba, janus, jetmoe, jukebox, kosmos-2, kosmos-2.5, kyutai_speech_to_text, layoutlm, layoutlmv2, layoutlmv3, led, levit, lfm2, lfm2_vl, lightglue, lilt, llama, llama4, llama4_text, llava, llava_next, llava_next_video, llava_onevision, longcat_flash, longformer, longt5, luke, lxmert, m2m_100, mamba, mamba2, marian, markuplm, mask2former, maskformer, maskformer-swin, mbart, mctct, mega, megatron-bert, metaclip_2, mgp-str, mimi, minimax, ministral, mistral, mistral3, mixtral, mlcd, mllama, mm-grounding-dino, mobilebert, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, modernbert, modernbert-decoder, moonshine, moshi, mpnet, mpt, mra, mt5, musicgen, musicgen_melody, mvp, nat, nemotron, nezha, nllb-moe, nougat, nystromformer, olmo, olmo2, olmo3, olmoe, omdet-turbo, oneformer, open-llama, openai-gpt, opt, ovis2, owlv2, owlvit, paligemma, parakeet_ctc, parakeet_encoder, patchtsmixer, patchtst, pegasus, pegasus_x, perceiver, perception_encoder, perception_lm, persimmon, phi, phi3, phi4_multimodal, phimoe, pix2struct, pixtral, plbart, poolformer, pop2piano, prompt_depth_anything, prophetnet, pvt, pvt_v2, qdqbert, qwen2, qwen2_5_omni, qwen2_5_vl, qwen2_5_vl_text, qwen2_audio, qwen2_audio_encoder, qwen2_moe, qwen2_vl, qwen2_vl_text, qwen3, qwen3_moe, qwen3_next, qwen3_omni_moe, qwen3_vl, qwen3_vl_moe, qwen3_vl_moe_text, qwen3_vl_text, rag, realm, recurrent_gemma, reformer, regnet, rembert, resnet, retribert, roberta, roberta-prelayernorm, roc_bert, roformer, rt_detr, rt_detr_resnet, rt_detr_v2, rwkv, sam, sam2, sam2_hiera_det_model, sam2_video, sam2_vision_model, sam_hq, sam_hq_vision_model, sam_vision_model, seamless_m4t, seamless_m4t_v2, seed_oss, segformer, seggpt, sew, sew-d, shieldgemma2, siglip, siglip2, siglip2_vision_model, siglip_vision_model, smollm3, smolvlm, smolvlm_vision, speech-encoder-decoder, speech_to_text, speech_to_text_2, speecht5, splinter, squeezebert, stablelm, starcoder2, superglue, superpoint, swiftformer, swin, swin2sr, swinv2, switch_transformers, t5, t5gemma, table-transformer, tapas, textnet, time_series_transformer, timesfm, timesformer, timm_backbone, timm_wrapper, trajectory_transformer, transfo-xl, trocr, tvlt, tvp, udop, umt5, unispeech, unispeech-sat, univnet, upernet, van, vaultgemma, video_llava, videomae, vilt, vipllava, vision-encoder-decoder, vision-text-dual-encoder, visual_bert, vit, vit_hybrid, vit_mae, vit_msn, vitdet, vitmatte, vitpose, vitpose_backbone, vits, vivit, vjepa2, voxtral, voxtral_encoder, wav2vec2, wav2vec2-bert, wav2vec2-conformer, wavlm, whisper, xclip, xcodec, xglm, xlm, xlm-prophetnet, xlm-roberta, xlm-roberta-xl, xlnet, xlstm, xmod, yolos, yoso, zamba, zamba2, zoedepth\n",
      "WARNING:hf-to-gguf:Trying to load config.json instead\n",
      "Traceback (most recent call last):\n",
      "File \"C:\\Users\\ruben\\Documents\\TrainingAI\\llama.cpp\\convert_hf_to_gguf.py\", line 688, in load_hparams\n",
      "config = AutoConfig.from_pretrained(dir_model, trust_remote_code=False).to_dict()\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "File \"c:\\Users\\ruben\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py\", line 1380, in from_pretrained\n",
      "raise ValueError(\n",
      "ValueError: Unrecognized model in R:\\Files Ruben\\GitRepos\\DeepDiveV2AI\\artifacts\\merged_models\\Meta-Llama-3-8B-Instruct-ProjectDeepDive-Lora-v1. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: aimv2, aimv2_vision_model, albert, align, altclip, apertus, arcee, aria, aria_text, audio-spectrogram-transformer, autoformer, aya_vision, bamba, bark, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, biogpt, bit, bitnet, blenderbot, blenderbot-small, blip, blip-2, blip_2_qformer, bloom, blt, bridgetower, bros, camembert, canine, chameleon, chinese_clip, chinese_clip_vision_model, clap, clip, clip_text_model, clip_vision_model, clipseg, clvp, code_llama, codegen, cohere, cohere2, cohere2_vision, colpali, colqwen2, conditional_detr, convbert, convnext, convnextv2, cpmant, csm, ctrl, cvt, d_fine, dab-detr, dac, data2vec-audio, data2vec-text, data2vec-vision, dbrx, deberta, deberta-v2, decision_transformer, deepseek_v2, deepseek_v3, deepseek_vl, deepseek_vl_hybrid, deformable_detr, deit, depth_anything, depth_pro, deta, detr, dia, diffllama, dinat, dinov2, dinov2_with_registers, dinov3_convnext, dinov3_vit, distilbert, doge, donut-swin, dots1, dpr, dpt, edgetam, edgetam_video, edgetam_vision_model, efficientformer, efficientloftr, efficientnet, electra, emu3, encodec, encoder-decoder, eomt, ernie, ernie4_5, ernie4_5_moe, ernie_m, esm, evolla, exaone4, falcon, falcon_h1, falcon_mamba, fastspeech2_conformer, fastspeech2_conformer_with_hifigan, flaubert, flava, flex_olmo, florence2, fnet, focalnet, fsmt, funnel, fuyu, gemma, gemma2, gemma3, gemma3_text, gemma3n, gemma3n_audio, gemma3n_text, gemma3n_vision, git, glm, glm4, glm4_moe, glm4v, glm4v_moe, glm4v_moe_text, glm4v_text, glpn, got_ocr2, gpt-sw3, gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gpt_oss, gptj, gptsan-japanese, granite, granite_speech, granitemoe, granitemoehybrid, granitemoeshared, granitevision, graphormer, grounding-dino, groupvit, helium, hgnet_v2, hiera, hubert, hunyuan_v1_dense, hunyuan_v1_moe, ibert, idefics, idefics2, idefics3, idefics3_vision, ijepa, imagegpt, informer, instructblip, instructblipvideo, internvl, internvl_vision, jamba, janus, jetmoe, jukebox, kosmos-2, kosmos-2.5, kyutai_speech_to_text, layoutlm, layoutlmv2, layoutlmv3, led, levit, lfm2, lfm2_vl, lightglue, lilt, llama, llama4, llama4_text, llava, llava_next, llava_next_video, llava_onevision, longcat_flash, longformer, longt5, luke, lxmert, m2m_100, mamba, mamba2, marian, markuplm, mask2former, maskformer, maskformer-swin, mbart, mctct, mega, megatron-bert, metaclip_2, mgp-str, mimi, minimax, ministral, mistral, mistral3, mixtral, mlcd, mllama, mm-grounding-dino, mobilebert, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, modernbert, modernbert-decoder, moonshine, moshi, mpnet, mpt, mra, mt5, musicgen, musicgen_melody, mvp, nat, nemotron, nezha, nllb-moe, nougat, nystromformer, olmo, olmo2, olmo3, olmoe, omdet-turbo, oneformer, open-llama, openai-gpt, opt, ovis2, owlv2, owlvit, paligemma, parakeet_ctc, parakeet_encoder, patchtsmixer, patchtst, pegasus, pegasus_x, perceiver, perception_encoder, perception_lm, persimmon, phi, phi3, phi4_multimodal, phimoe, pix2struct, pixtral, plbart, poolformer, pop2piano, prompt_depth_anything, prophetnet, pvt, pvt_v2, qdqbert, qwen2, qwen2_5_omni, qwen2_5_vl, qwen2_5_vl_text, qwen2_audio, qwen2_audio_encoder, qwen2_moe, qwen2_vl, qwen2_vl_text, qwen3, qwen3_moe, qwen3_next, qwen3_omni_moe, qwen3_vl, qwen3_vl_moe, qwen3_vl_moe_text, qwen3_vl_text, rag, realm, recurrent_gemma, reformer, regnet, rembert, resnet, retribert, roberta, roberta-prelayernorm, roc_bert, roformer, rt_detr, rt_detr_resnet, rt_detr_v2, rwkv, sam, sam2, sam2_hiera_det_model, sam2_video, sam2_vision_model, sam_hq, sam_hq_vision_model, sam_vision_model, seamless_m4t, seamless_m4t_v2, seed_oss, segformer, seggpt, sew, sew-d, shieldgemma2, siglip, siglip2, siglip2_vision_model, siglip_vision_model, smollm3, smolvlm, smolvlm_vision, speech-encoder-decoder, speech_to_text, speech_to_text_2, speecht5, splinter, squeezebert, stablelm, starcoder2, superglue, superpoint, swiftformer, swin, swin2sr, swinv2, switch_transformers, t5, t5gemma, table-transformer, tapas, textnet, time_series_transformer, timesfm, timesformer, timm_backbone, timm_wrapper, trajectory_transformer, transfo-xl, trocr, tvlt, tvp, udop, umt5, unispeech, unispeech-sat, univnet, upernet, van, vaultgemma, video_llava, videomae, vilt, vipllava, vision-encoder-decoder, vision-text-dual-encoder, visual_bert, vit, vit_hybrid, vit_mae, vit_msn, vitdet, vitmatte, vitpose, vitpose_backbone, vits, vivit, vjepa2, voxtral, voxtral_encoder, wav2vec2, wav2vec2-bert, wav2vec2-conformer, wavlm, whisper, xclip, xcodec, xglm, xlm, xlm-prophetnet, xlm-roberta, xlm-roberta-xl, xlnet, xlstm, xmod, yolos, yoso, zamba, zamba2, zoedepth\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "File \"C:\\Users\\ruben\\Documents\\TrainingAI\\llama.cpp\\convert_hf_to_gguf.py\", line 10392, in <module>\n",
      "main()\n",
      "File \"C:\\Users\\ruben\\Documents\\TrainingAI\\llama.cpp\\convert_hf_to_gguf.py\", line 10354, in main\n",
      "hparams = ModelBase.load_hparams(dir_model, is_mistral_format)\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "File \"C:\\Users\\ruben\\Documents\\TrainingAI\\llama.cpp\\convert_hf_to_gguf.py\", line 692, in load_hparams\n",
      "with open(dir_model / \"config.json\", \"r\", encoding=\"utf-8\") as f:\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'R:\\\\Files Ruben\\\\GitRepos\\\\DeepDiveV2AI\\\\artifacts\\\\merged_models\\\\Meta-Llama-3-8B-Instruct-ProjectDeepDive-Lora-v1\\\\config.json'\n",
      "\n",
      "‚ùå Conversion failed with exit code 1.\n",
      "\n",
      "üí° You can now delete the large merged model folder to save space:\n",
      "   R:\\Files Ruben\\GitRepos\\DeepDiveV2AI\\artifacts\\merged_models\\Meta-Llama-3-8B-Instruct-ProjectDeepDive-Lora-v1\n"
     ]
    }
   ],
   "source": [
    "# --- QUANTIZE / CONVERT THE MERGED MODEL TO GGUF ---\n",
    "print(\"\\nüöÄ Starting quantization to GGUF format...\")\n",
    "\n",
    "if not LLAMA_CPP_PATH.exists():\n",
    "    raise NotADirectoryError(f\"CRITICAL: llama.cpp directory not found at '{LLAMA_CPP_PATH}'. Update LLAMA_CPP_PATH in Cell 1.\")\n",
    "\n",
    "quant_type_lower = QUANT_TYPE.lower()\n",
    "needs_two_step = quant_type_lower not in CONVERTER_SUPPORTED_OUTTYPES\n",
    "convert_outtype = QUANT_TYPE if not needs_two_step else \"f16\"\n",
    "intermediate_fp16 = FINAL_GGUF_DIR / f\"{model_folder_name}-{LORA_OUTPUT_NAME}-F16.gguf\"\n",
    "convert_target = FINAL_GGUF_FILE if not needs_two_step else intermediate_fp16\n",
    "\n",
    "convert_command = [\n",
    "    PYTHON_EXECUTABLE,\n",
    "    str(CONVERT_SCRIPT),\n",
    "    str(MERGED_MODEL_DIR),\n",
    "    \"--outfile\", str(convert_target),\n",
    "    \"--outtype\", convert_outtype\n",
    "]\n",
    "\n",
    "print(\"--- Convert Command ---\")\n",
    "print(subprocess.list2cmdline(convert_command))\n",
    "print(\"-----------------------\")\n",
    "\n",
    "process = subprocess.Popen(convert_command, cwd=str(LLAMA_CPP_PATH), stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, encoding='utf-8', bufsize=1)\n",
    "while True:\n",
    "    output = process.stdout.readline()\n",
    "    if output == '' and process.poll() is not None:\n",
    "        break\n",
    "    if output:\n",
    "        print(output.strip())\n",
    "\n",
    "if process.returncode != 0:\n",
    "    print(f\"\\n‚ùå Conversion failed with exit code {process.returncode}.\")\n",
    "else:\n",
    "    if not needs_two_step:\n",
    "        print(f\"\\nüéâ GGUF export successful! File saved at:\\n{FINAL_GGUF_FILE}\")\n",
    "    else:\n",
    "        if QUANTIZE_BINARY is None:\n",
    "            raise FileNotFoundError(\n",
    "                \"Requested quantization type requires the llama.cpp 'quantize' binary. \"\n",
    "                \"Build llama.cpp (cmake -S . -B build && cmake --build build --config Release) \"\n",
    "                \"or switch QUANT_TYPE to one of the converter-supported values.\"\n",
    "            )\n",
    "\n",
    "        quantize_command = [\n",
    "            str(QUANTIZE_BINARY),\n",
    "            str(convert_target),\n",
    "            str(FINAL_GGUF_FILE),\n",
    "            QUANT_TYPE.upper()\n",
    "        ]\n",
    "\n",
    "        print(\"\\n--- Quantize Command ---\")\n",
    "        print(subprocess.list2cmdline(quantize_command))\n",
    "        print(\"------------------------\")\n",
    "\n",
    "        process = subprocess.Popen(quantize_command, cwd=str(QUANTIZE_BINARY.parent), stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, encoding='utf-8', bufsize=1)\n",
    "        while True:\n",
    "            output = process.stdout.readline()\n",
    "            if output == '' and process.poll() is not None:\n",
    "                break\n",
    "            if output:\n",
    "                print(output.strip())\n",
    "\n",
    "        if process.returncode == 0:\n",
    "            print(f\"\\nüéâ Quantization successful! Your game-ready model is located at:\\n{FINAL_GGUF_FILE}\")\n",
    "            if not KEEP_FP16_INTERMEDIATE and convert_target.exists():\n",
    "                try:\n",
    "                    convert_target.unlink()\n",
    "                    print(f\"   (Removed intermediate file: {convert_target})\")\n",
    "                except OSError as cleanup_error:\n",
    "                    print(f\"   ‚ö†Ô∏è Could not delete intermediate file: {cleanup_error}\")\n",
    "        else:\n",
    "            print(f\"\\n‚ùå Quantization failed with exit code {process.returncode}.\")\n",
    "\n",
    "print(\"\\nüí° You can now delete the large merged model folder to save space:\")\n",
    "print(f\"   {MERGED_MODEL_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "69f50046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Final GGUF file not found yet. Run the quantization cell first.\n"
     ]
    }
   ],
   "source": [
    "# --- OPTIONAL: Copy the GGUF into LM Studio's local models folder ---\n",
    "if FINAL_GGUF_FILE.exists():\n",
    "    LM_STUDIO_MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    destination = LM_STUDIO_MODELS_DIR / FINAL_GGUF_FILE.name\n",
    "    try:\n",
    "        shutil.copy2(FINAL_GGUF_FILE, destination)\n",
    "        print(\"‚úÖ Copied GGUF to LM Studio models directory.\")\n",
    "        print(f\"   Location: {destination}\")\n",
    "    except Exception as exc:\n",
    "        print(\"‚ö†Ô∏è Unable to copy GGUF into LM Studio's directory.\")\n",
    "        print(f\"   Details: {exc}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Final GGUF file not found yet. Run the quantization cell first.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aedb3d9",
   "metadata": {},
   "source": [
    "### Workflow Complete!\n",
    "\n",
    "1.  **Locate Your Final Model:**\n",
    "    *   Your `final_gguf_models` folder now lives alongside this notebook (inside the repo on `R:`).\n",
    "    *   Inside you'll find the `...-Q4_K_M.gguf` file ready for inference.\n",
    "\n",
    "2.  **Load in LM Studio:**\n",
    "    *   Either drag-and-drop the GGUF file into LM Studio or use the optional copy cell to push it into `LM_STUDIO_MODELS_DIR` automatically.\n",
    "    *   The model will appear under \"My Models\" once the copy finishes.\n",
    "\n",
    "3.  **Activate and Test:**\n",
    "    *   Select the merged model (no extra LoRA adapters needed).\n",
    "    *   Start the local server and try it inside Project Deep Dive!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
